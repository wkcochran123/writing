\chapter{The Algebra of Events}
\label{chap:algebra}
The first chapter outlined the informational mechanisms of recording 
measurements of physical phenomena:
each measurement produces a finite, distinguishable event, and the
absence of additional events is itself a binding constraint.
Measurement is not a sampling of an underlying continuum; it is the
creation of distinguishability.  From a mathematical standpoint, 
measurement itself is therefore a growing record of discrete selections, ordered
by refinement. Each measured event appends an inassailable fact to the experimental 
record.

This chapter formalizes that structure.  We build the algebra of
measurement itself.  Rather than treating observations as values of
continuous functions, we adopt the combinatorial viewpoint forced by
the Axioms of Cantor~\ref{ax:cantor} and Planck~\ref{ax:planck}\footnote{Henceforth, these axioms will be referenced numerically.}: every admissible 
experimental record
is finite or countable, and every refinement is a restriction of
admissible outcomes.  The central object of this chapter---the
\emph{Causal Universe Tensor}---expresses the fact that history is
built multiplicatively: each new event contracts the set of admissible
continuations of the record.  The universe does not evolve additively
in time; it accumulates consistency through left products of restricted
increments.

\section{Time and Events}

Before introducing any notion of temporal ordering, it is necessary to
separate the concept of an \emph{event} from the concept of \emph{time}.
In this framework, neither is assumed \emph{a priori}. Both arise as consequences
of how distinguishable records can be extended without contradiction.

An \emph{event} is defined as a minimal, distinguishable unit of record (Definition~\ref{def:event}).
It is not something that happens in time. It is the act of recording a
distinction. An event exists precisely when a measurement refines the
experimental ledger. If nothing is distinguished, no event exists.

Thus, the set of events $E$ is the set of all finite acts of distinction.
This set carries structure not from motion or duration but from refinement.

Time, therefore, does not exist as a background coordinate but as a way to 
index events relative to each other. It is an ordinal indexing of
refinement. Given two events $e_i, e_j \in E$, we say that $e_i \prec e_j$
if and only if the record of $e_j$ is a refinement of the record of $e_i$.

Time is therefore not measured. It is counted. It is the order type of the
chain of events:
\[
e_1 \prec e_2 \prec e_3 \prec \cdots
\]
Proper time (see Definition~\ref{def:proper-time} is the cardinality of 
this chain. There is no continuum of time
between irreducible refinements. There is only succession.

\begin{phenomenon}[The Event Effect~\cite{kant1781}]
\NB{Kant identified time not as an object, but as a necessary ordering
condition for experience.  The present construction reverses this dependency:
the ordering of irreducible events generates time rather than presupposing it~\cite{kant1781}.}

An event is the minimal irreducible act of distinguishability.  It arises
when and only when a measurement refines the admissible history.  It is not
located in time; it generates time by its order.
\end{phenomenon}

It is necessary to clarify what it means for a
measurement to exist at all.  In this framework, measurement is not a passive
act and not an inquiry about a pre--existing quantity.  It is the creation of a
distinction that did not previously appear in the record.  A measurement is an
operation that enlarges the causal ledger---it describes an observation to the
exclusion of all others.

A record that does not grow is not a record that was measured.  Silence cannot
be distinguished from absence, and absence cannot participate in causal
structure.  For this reason, the null act cannot be admitted as a measurement.

This has an important structural consequence.  Measurement is not reversible.
Later observations may refine, reinterpret, or contextualize earlier ones, but
they cannot erase the fact that a distinction was recorded.  The ledger may be
extended, but it cannot be undone.  This irreversibility is not a postulate of
physics; it is a logical consequence of what it means to record anything at
all.

\begin{phenomenon}[The Positive Measurement Effect~\cite{plato1996}]
All admissible measurements produce a strictly positive refinement.  A zero
measurement is not a measurement but a null act and is therefore excluded.
Every event increases the informational record by a positive integer amount.
Observations cannot be replaced or canceled by future observations.

This positivity does not preclude geometric notions such as direction; such
structures do not arise from the sign of a measurement but from relations
among distinct positive refinements.
\end{phenomenon}

Although no intermediate event exists between successive refinements, the
dense limit of refinement forces the appearance of a smooth interval as an
approximation. This interval is not fundamental. It is the shadow of coarse
observation.

Let $\tau : E \to \mathbb{N}$ be the ordinal timing map induced by
refinement order. For two successive events $e_i \prec e_{i+1}$, the open set
\begin{equation}
(\tau(e_i), \tau(e_{i+1}))
\end{equation}
contains no elements of $E$.

However, in the smooth completion forced later by the Axiom of Cantor, this
empty discrete interval is represented as a continuous open interval. This
representation gives rise to the notion of a \emph{present moment}.

\begin{definition}[The Moment]
\label{def:moment}
The \emph{Moment} is the smooth shadow of the interim between successive
admissible events.  It is not a primitive atom of time, but the continuous
domain on which the analytic completion of the record is defined when no new
distinguishable refinements occur.  Concretely, the Moment corresponds to the
open interval
\[
(i,i+1) \subset \mathbb{R},
\]
together with the unique analytic function determined by the refinement data
at its endpoints.  It represents the smooth surrogate of informational
silence: the continuous interpolation of the ledger's discrete gaps.
\end{definition}

\subsection{The Causal Universe Tensor}

Having established that time is not a background parameter but an ordinal
structure induced by refinement, we now consider the object that records the
global consequences of this ordering.  The axioms do not allow a universe to
be postulated as a stage on which events occur.  Instead, what is called the
“universe” must be constructed from observation as the unique coherent extension of the finite
record itself.  The following tensor object is not dynamical and not
geometric.  It is a bookkeeping device forced by the requirement that local
distinctions admit a single, globally consistent smooth shadow.

Before introducing the update rule \eqref{eq:Uk-update}, it is essential to
distinguish the role of time in the discrete ledger from its role in the
continuous shadow.  In the discrete domain $E$, time is nothing more than the
order in which distinguishable events are appended to the record; it is an
ordinal index that counts refinements.  No structure beyond this ordering is
available or assumed.  By contrast, the continuous shadow $U$ carries no
notion of temporal flow.  Its appearance of continuity does not arise from
propagation in a background medium, but from the analytic completion that
represents the cumulative effect of past refinements.  Each ``moment'' in the
shadow is simply the smooth image of the current admissible record; it does
not evolve on its own.

This distinction clarifies the meaning of the update:
the tensor $\U_{k+1}$ is not obtained by transporting $\U_k$ forward in an
independent time variable, but by applying the continuous image of the
restriction induced by the latest event.  The continuous universe is
therefore not postulated as a field living on a manifold; it is the coherent
bookkeeping of discrete consistency.

In the discrete domain $E$,
time is ordinal: an index into the growing chain of distinguishable
selections.  In the continuous shadow $U$, time does not flow at all.
The continuous tensor $\mathbf{U}_k$ at step $k$ is not the result of
propagation, but the image of a restriction map:
\begin{equation}
\label{eq:Uk-update}
\U_{k+1} \;=\; \Psi\!\big(e_{k+1} \cap \hat{R}(e_k)\big)\,\U_k,
\end{equation}
where $\hat{R}$ is the discrete restriction induced by the most recent event
$e_k$, and $\Psi$ is its continuous representation.  Thus the
continuous universe is not postulated as a field living on a manifold:
it is the coherent bookkeeping of discrete consistency.

The ordered product makes this explicit.  If $\hat{R}(e_j)$ denotes
the admissible refinement of the $j$-th event, then the causal universe
as seen by a single inertial observer is
\begin{equation}
\mathbf{U}_k \;=\; \prod_{j=1}^{k-1} \Psi\!\big(e_{j+1} \cap \hat{R}(e_j)\big).
\end{equation}

In this way, Chapter~\ref{chap:algebra} performs the key transition:
from sets of distinguishable events to an algebra of restricted,
multiplicative updates. 
The remainder of the chapter introduces the axioms, operators, and
tensor structures that make this viewpoint precise, culminating in the
formal definition of the causal universe tensor.

\begin{example}[Statistical Process Control~\cite{shewhart1931}]
\NB{Observational records have been used to understand and control complex
processes to remarkable success.  Statistical process control
demonstrates that measurement does not estimate a continuous parameter
directly; it eliminates process states that are incompatible with the
record.  The state of the system is therefore not an average, but the
set of configurations that have survived all admissible checks.}
For a formal treatment, see Phenomenon~\ref{ph:spc} later in this chapter.

Imagine a factory that manufactures a precision component.  The process
is controlled by a set of adjustable parameters: temperature, pressure,
feed rate, alignment, and so on.  At startup, all parameter settings
that satisfy the design tolerances are admissible; the process could be
in any one of many configurations.  A single measurement does not
determine the underlying state.  It merely rules out those
configurations that would have produced a conflicting outcome.

This is the essential structure of statistical process control.  Each
inspection, probe, gauge reading, or quality check eliminates a subset
of incompatible configurations.  After $k$ measurements, the surviving
parameter settings are precisely those that are consistent with all $k$
observations.

Let $e_k$ denote the $k$-th inspection result, and let $\hat{R}(e_k)$ be
the discrete restriction that removes every process state incompatible
with $e_k$.  If $\Psi$ embeds these restrictions into the continuous
tensor domain, the recorded state of the process after $k$ inspections
is
\begin{equation}
\mathbf{U}_k \;=\; \prod_{j=1}^{k-1} \Psi\!\big(e_{j+1} \cap \hat{R}(e_j)\big).
\end{equation}
The process does not ``evolve'' in time in the usual dynamical sense;
it accumulates admissibility.  Each new inspection refines the record
by discarding alternatives, giving the stepwise update
\begin{equation}
\mathbf{U}_{k+1} = \Psi\!\big(e_{k+1}\cap\hat{R}(e_{k})\big)\,\mathbf{U}_k.
\end{equation}

Two independent inspectors, perhaps located at different stations on
the production line, can refine their records without communication.
If their measurements are mutually consistent, the products merge
without conflict.  If not, no admissible configuration survives the
combined restrictions, and the process is flagged as out of control.
In this sense the system is independent, yet globally constrained
by the requirement of the coherent environment.

This ordinary industrial setting exhibits the same structure developed
in this chapter.  Measurement eliminates incompatible alternatives,
time indexes the number of admissible refinements, and the continuous
representation $\mathbf{U}_k$ is nothing more than the shadow of a
discrete product of selections.
\end{example}

We begin by enumerating the seven Axioms of Measurement, which formalize the
structure of admissible records and the refinement of observational
history.


\section{The Axioms of Mathematics}
\label{se:mathaxiom}

All mathematics in this work is carried out within the framework of
Zermelo–Fraenkel set theory with the Axiom of Choice (ZFC)~\cite{jech2003,kunen1980}.
Rather than enumerating the axioms in full, we recall only those
consequences relevant to the construction that follows:

\begin{itemize}
  \item \textbf{Extensionality} ensures that distinguishability has formal
  meaning: two sets differ if and only if their elements differ.
  \item \textbf{Replacement} and \textbf{Separation} guarantee that
  recursively generated collections such as the causal chain of events
  remain sets.
  \item \textbf{Choice} permits well–ordering, allowing every countable
  causal domain to admit an ordinal index.
  \item \textbf{The Successor Function} of the Peano axioms provides the
  mechanism by which distinguishable outcomes may be counted.
\end{itemize}

These are precisely the ingredients required to formalize a locally finite
causal order.
All further constructions---relations, tensors, and operators---are definable
within standard ZFC mathematics; see Kunen~\cite{kunen1980} and Jech~\cite{jech2003}
for set-theoretic foundations, and Halmos~\cite{halmos1958,halmos1974naive} for the
induced tensor and operator structures on finite-dimensional vector spaces.

\subsection{Measurements are Mathematical in Nature}
The starting point of this framework is methodological rather than
ontological.  We do not assume anything about the substance of physical
reality.  We assume only that the outcomes of measurement are finite or
countable collections of distinguishable results recorded in time.
This is standard across probability theory and information theory:
Shannon formalized information as distinguishable symbols drawn from a
finite or countable alphabet \cite{shannon1948}, and Kolmogorov showed
that empirical outcomes can be represented as elements of measurable
sets within standard set theory \cite{kolmogorov1933}.  In this view,
measurement produces data, and data are mathematical objects.
Everything that follows concerns the admissible transformations among
such records.

\begin{definition}[Admissible Structure]
Let $E$ denote the set of admissible objects.

Every element of $E$ is a set.  Two types of elements are distinguished
internally:

\begin{itemize}
  \item An \emph{event} (see Defintion~\ref{def:event}) is a set $e$ such that $e \in E$.
  \item A \emph{causal network} (see Definition~\ref{def:causal-network}) is a set $S$ such that
\[
S \in E \quad \text{and} \quad \forall e \,(e \in S \Rightarrow e \in E).
\]
\end{itemize}

Thus the admissibility relations are stratified as
\[
e \in E, \qquad S \in E, \qquad e \in S.
\]
\end{definition}

Before we may speak of time, refinement, or consistency, we must specify the
kind of object a finite observer actually possesses.  An observer does not
hold a field, a manifold, or a continuum; they hold a list of outcomes---each a
distinguishable refinement that survived an attempted measurement. 

Such a list forms the only durable evidence available to the observer.  It is
the structure from which ordinal time emerges, the substrate on which
refinement acts, and the boundary condition for every continuous shadow.  To
develop the theory, we therefore need a precise object that captures this
accumulating, non-erasable, finitely generated sequence of distinctions.

We call this object the \emph{admissible record}.

\begin{definition}[Admissible Record]
\label{def:admissible-record}
An \emph{admissible record} is a finite, totally ordered sequence of
distinguishable events
\[
e_1 \prec e_2 \prec \cdots \prec e_k,
\]
generated by successive applications of the admissible structure.  Each
successor event $e_{i+1}$ must refine the preceding record through the
restriction map $\hat{R}$, producing a strictly positive informational
increment.  No event may contradict any element of the record, and no event
may erase or replace a previous refinement.  

An admissible record therefore captures exactly the information a finite
observer can justify: each entry is a persistent, irreversible refinement,
and the ordering $\prec$ is the ordinal index of the refinement process
itself.
\end{definition}

We now build this record mathematically.

\begin{axiom}[The Axiom of Kolmogorov~\cite{kolmogorov1933}]
\label{ax:kolmogorov}
\emph{[Measurement as a Formal Record.]}
Formally, there exists a set $E$ such that $E$ is an admissible record and
\begin{equation}
\left|\{\, e \mid e \in E \,\}\right| > 3 .
\end{equation}
\NB{The choice of $3$ is not arbitrary, but serves to eliminate degenerate
configurations and simplify the foundational proofs. There are posedness
concerns for universes with less than 4 events and the analysis of the
degenerate cases is omitted.}
\end{axiom}

The record of measurement---defined as the finite or countable set of
observed, distinguishable events and worldlines---is taken to be a mathematical object
representable within ZFC. No
ontological claim is made about physical reality. The axiom asserts only
that observable data can be formalized as sets and relations.

This standpoint is consistent with Kolmogorov's construction of probability
spaces, in which empirical outcomes are represented as measurable sets
\cite{kolmogorov1965}. Accordingly, a record of finite observations is a
mathematical object whose structure is defined entirely within ZFC. Throughout
this work, the word ``information'' refers exclusively to these representable
distinctions; nothing is asserted about any underlying physical substrate
that might produce them.

\begin{phenomenon}[The Box Effect~\cite{box1976}]
Because refinements are irreversible (Axiom~\ref{ax:kolmogorov}), any admissible
ledger forms a time series: a strictly increasing sequence of recorded
events.  Each new entry extends the record and none may be removed, replaced,
or rewritten.

A finite observer writes down the outcomes of distinguishable operations.
Once recorded, these outcomes persist; later observations cannot erase or
contradict them without destroying coherence.  The notebook therefore grows
as an ordered list of refinements:
\[
e_1 \prec e_2 \prec \cdots \prec e_k,
\]
where the ordering reflects not metric time but informational succession.
This monotonicity is the origin of temporal order in the discrete domain.
What the observer experiences as “time” is the ordinal index of accumulating
distinctions. This is a time series as formalized by Box and collaborators~\cite{box1976}.

In the continuous shadow, this growth appears as a smooth trajectory, but no
flow is taking place; the shadow merely represents the coherent completion of
the discrete time series.  Temporal structure is thus not a primitive
background but a consequence of the irreversibility of refinement.
\end{phenomenon}


\subsection{Mathematics is the Language of Measurement}

Mathematics enters this framework not as an external interpretive layer
but as the minimal language in which measurement can be expressed. A
record of observation is a finite collection of distinguishable outcomes,
and the relations among those outcomes—order, refinement, exclusion,
and compatibility—require a precise symbolic setting. The purpose of this
subsection is therefore methodological: to state explicitly the mathematical
rules under which every subsequent construction is carried out.

No structure beyond ordinary set theory is needed. The axioms of ZFC
provide the machinery for forming sets of events, for defining relations
among them, and for building the tensor algebra in which their continuous
shadows will later appear. Within this system, counting becomes the first
and most fundamental operation: to measure is to distinguish, and to
distinguish is to enumerate the admissible outcomes. Peano’s contribution
is thus not philosophical but operational. The natural numbers supply the
ordinal scaffold upon which every causal record is indexed.

With this in mind, we begin by stating the formal principle that makes
counting available as a tool of measurement.

\begin{axiom}[The Axiom of Peano~\cite{fraenkel1922,martin1970,zermelo1908}]
\label{ax:peano}
\emph{[Counting as the Tool of Information]}
All reasoning in this work is confined to the framework of ZFC.
Every object---sets, relations, functions, and tensors---is
constructible within that system, and every statement is interpretable
as a theorem or definition of ZFC.  No additional logical principles
are assumed beyond those required for standard analysis and algebra.

Formally,
\[
\mathrm{Measurement} \;\subseteq\; \mathrm{Mathematics} \;\subseteq\; \mathrm{ZFC} \;\subseteq\; \mathrm{Counting}.
\]
Thus, the language of mathematics is taken to be the entire ontology of
the theory: the physical statements that follow are expressions of
relationships among countable sets of distinguishable events, each
derivable within ordinary mathematical logic.
\end{axiom}

The Axiom of Peano supplies the successor structure that every admissible
record inherits: refinements arrive one at a time, each indexed by the next
natural number.  A speedometer is therefore not a device that measures a
continuous quantity called “speed,” but a mechanism that compares successive
entries in a Peano-ordered ledger.  It records position at step $k$ and at
step $k+1$, and reports the distinguishable change between these two
successors divided by the clock’s own successor count.  Its reading is a
finite-difference ratio computed over the Peano structure of the record, not
a primitive geometric derivative.  In this framework, the speedometer is the
operational realization of the successor axiom: it produces a quantity only
because the ledger grows in discrete, ordered steps.


\begin{example}[The Speedometer~\cite{warner1902,yoshida1980}]
\label{te:speedometer}
\NB{The mechanical implementation of measuring devices often are protected
by explicit descriptions of how they work. The patents cited here explicitly
describe how they turn counting into data.}

Consider an ordinary automobile speedometer.  The dial appears to report a
continuous real number at each instant, but the device does not have access
to the real numbers.  A mechanical speedometer counts wheel rotations through
a gear train and maps those counts to pointer positions.  A digital
speedometer counts the same rotations and displays a numeral drawn from a
finite alphabet.

Each time the counter increments and the displayed symbol or pointer position
changes, a new distinguishable event is recorded.  Between two successive
display states there is no way, from the informational record alone, to assert
that any additional state occurred.  The apparent continuity of ``speed'' is a
visual interpolation of a finite counting process.

Thus the speedometer does not output a real number.  It outputs a countable
sequence of distinguishable states derived from integral counts of wheel
rotations.  The act of measuring speed reduces to counting transitions of a
finite-state device.  All physical inference based on such data can be
expressed within ordinary arithmetic and set theory.

This illustrates Axiom~\ref{ax:peano}: measurement generates only countable,
finitely coded distinctions, and every mathematical object used to interpret
those distinctions---numbers, functions, tensors---is a construct of ZFC.
No structure beyond counting is assumed at the fundamental informational
level.
\end{example}


\section{The Axioms of Informational Structure}

The previous section established that a physical record is a set of
distinguishable observations, representable within ZFC, and partially ordered
by causal precedence. Nothing further was assumed about geometry, dynamics, or
the continuum, even thous it has been shown that these concepts can be derived from ZFC. 
In this section, we introduce two informational axioms that
restrict how such a record may be interpreted independent of a predictive law. These axioms express constraints
on admissible descriptions of the world, independent of any particular model
of physical phenomena. Measurements are bound by what came before.

Axiom~\ref{ax:ockham} formalizes the principle that a physical history may not
contain unobserved structure. Among all symbolic descriptions that reproduce
the recorded events, any admissible one implies no missing events. We demonstrate that 
this is the information--theoretic form of Ockham's principle: no plurality of assumptions
without necessity.

Axiom~\ref{ax:causal} asserts that the record of events is not merely ordered but
forms a locally finite causal set. Local finiteness ensures that causal cardinality 
is discrete, while the partial order encodes temporal precedence. Continuum
spacetime, or any other set of mathematical descriptions, is therefore understood 
as an approximation that faithfully embeds this discrete informational structure.

Together, these axioms define the informational content of the physical world:
a causal set with no unrecorded structure and no additional assumptions beyond
the observational record itself.

\subsection{Information Minimality}

The observational record $E$ is defined only by the distinguishable events it
contains. Between two recorded events $e_i$ and $e_{i+1}$, no additional
structure is present in the data: no new marks in the notebook, no threshold
crossings, and no observable distinctions. Set theory alone does not forbid a
hypothetical refinement that inserts additional structure between $e_i$ and
$e_{i+1}$, but any such refinement asserts observations that did not occur.
To prevent unrecorded structure from being introduced by assumption, we impose
an informational constraint.

Among all symbolic descriptions that reproduce the recorded events, the
admissible one is the shortest. In modern information theory, this statement
is formalized by Kolmogorov complexity ~\cite{kolmogorov1965,li2008}:
a description is preferred if it
introduces no additional information beyond the events in $E$. This embodies
the classical principle that no plurality of assumptions should be posited
without necessity. It is not derived from the set-theoretic framework; it is
an axiom about how physical theories must interpret finite empirical records.

\begin{axiom}[The Axiom of Ockham~\cite{ockham1323})]
\label{ax:ockham}
\emph{[Order Coherence]}
Let $E=\{e_0 \prec e_1 \prec \cdots \prec e_n\}$ be a finite or countable
partially ordered set of recorded events.  The admissible histories are
order--respecting in the following sense: for any two events $e_1,e_2 \in E$,
\[
e_1 \prec e_2 \;\;\Longrightarrow\;\; \forall S \in E \, (\, e_1 \in S \Rightarrow e_2 \in S \,).
\]

That is, no admissible history may contain an earlier event without also
containing all later events forced by the causal order.
\end{axiom}

We have seen this principle in action already.  Refer to Thought 
Experiment~\ref{te:invisible-curve} and the use of Simpson's rule to
compute the path of a spaceship with minimal measurement information.
Further, we will later show, using Law~\ref{law:discrete-spline}, that the
Axiom~\ref{ax:ockham} imposes an informational minimality constraint on the
evolution of the Causal Universe Tensor, and that this constraint
mathematically characterizes the sharpness of the razor.


\subsection{Causal Set Theory}
The previous axiom imposed an informational constraint on admissible
descriptions of the record of measurement. We now introduce a structural
constraint. The empirical record is a set of distinguishable events with a
causal precedence relation $\prec$, but this alone does not restrict the size
of causal intervals. In a general partially ordered set, the number of events
between $a$ and $b$ may be infinite. Physical measurements, however, produce
finite data. To represent this empirically grounded discreteness, we assume
that the causal order is locally finite: every causal interval contains only
finitely many recorded events.

This postulate places the present construction within the causal set program
of Sorkin and collaborators, where spacetime is modeled as a locally finite
partial order and continuum geometry, when it appears, is a derived
approximation. Order encodes temporal precedence, and local finiteness
encodes discrete causal volume. No metric, field, or manifold structure is
assumed at the fundamental level; these arise only if the causal set admits a
faithful embedding into a Lorentzian manifold.

\begin{axiom}[The Axiom of Causal Sets~\cite{bombelli1987}]
\label{ax:causal}
\emph{[Events are Discrete]}

The distinguishability relations among recorded events admit a representation
as a locally finite partially ordered set $(E,\prec)$, where
\begin{enumerate}
\item $e\prec f$ means that the record of $e$ is incorporated before the record of $f$,
\item $(E,\prec)$ is acyclic and transitive,
\item and for any two events $a\prec b$, the interval
$\{\,e\in E : a\prec e\prec b\,\}$ is finite.
\end{enumerate}
Local finiteness ensures that the recorded causal cardinality is discrete, and the
order relation encodes temporal precedence within the record.  Any Lorentzian
manifold, when it exists, is merely a physical model in which this discrete
causal structure may be faithfully approximated.
\end{axiom}

Axiom~{ax:causal} Sets describes the abstract structure that any admissible
record must obey: events appear discretely, in a definite order, and only
finitely many distinctions can occur between any two recorded observations.
To make this concrete, we consider how an actual laboratory procedure
generates such a structure.  A finite observer interacts with instruments,
performs distinguishable operations, and records outcomes one at a time.
Each entry in the notebook is appended irreversibly, and no later act can
erase or reorder what has already been written.  The laboratory therefore
implements the axioms directly: it produces a locally finite, acyclic,
transitive ordering of events whose causal precedence is nothing more than
the order in which the experimenter can justify each entry.  This operational
picture is the simplest physical realization of the causal set structure.


\begin{example}[The Laboratory Procedure~\cite{ockham1323,wheeler1983}]
\label{ex:psi-lab}
\NB{The following example collects ideas from several well–established
perspectives in measurement theory.  Bohr and Wheeler emphasize that a
physical experiment records only distinguishable outcomes; no other
structure is operationally meaningful~\cite{bohr1928,wheeler1983}.  In
information theory, such records are represented as finite or countable
strings of distinguishable symbols~\cite{cover2006,shannon1948}.  In
ergodic theory and causal set theory, successive measurements refine a
partition of the observational domain into finer distinguishable
elements~\cite{ornstein1991,rohlin1967,sorkin2005}.  Finally,
computational mechanics and operator–theoretic dynamics treat the
“evolution” of a system as the repeated update of its information
state~\cite{birkhoff1931,crutchfield1989,koopman1931}.  Taken together,
these perspectives justify modeling a laboratory procedure as a refinement
operator acting on a finite measurement record.  The experiment does not
solve differential equations; it follows the laboratory procedure $\Psi$.}

Consider a laboratory notebook in which each threshold crossing of a detector
is recorded as a mark in ink. The notebook contains a finite sequence of
distinguishable entries
\[
e_0 \prec e_1 \prec \cdots \prec e_n,
\]
each representing an irreversible update of the experimental record. The
notebook is not a model of reality; it is the empirical record. No
claim is made about any mechanism behind it.

Now suppose one attempts to describe what ``really'' happened between two
successive entries $e_i$ and $e_{i+1}$. If additional curvature, oscillation,
turning points, or discontinuities had occurred, then the detector would have
crossed a threshold and a new entry would appear. Because no such entry is
present, the observational record forbids any refinement that predicts one.

Thus the notebook determines a finite set $E=\{e_0,\dots,e_n\}$ of recorded
events. Every admissible history must be a completion that introduces no new
distinguishable events beyond $E$. Any hypothetical refinement with additional
structure is rejected as inadmissible, since it asserts observations that did
not occur.
\end{example}

\section{The Axioms of Observation}
\label{se:observationaxioms}

A common criticism of mathematical physics is the extent to which mathematics can 
be tuned to fit observation~\cite{boltzmann1896,planck1914} and, conversely, 
manipulated to yield nonphysical results~\cite{hossenfelder2018}.
Lord Berkeley's critique of Newton’s fluxions~\cite{berkeley1734} could only be answered by centuries of successful 
prediction with only intuition as justification. 
Today, calculus feels like a natural extension of the real world---so much so that 
Hilbert, in posing his famous list of open problems, explicitly formalized the lack 
of a rigorous foundation for physics as his Sixth Problem~\cite{hilbert1902,weyl1949}.

We aim to show that the mathematical language used to describe observation gives 
rise to a system expressible entirely as a discrete set of events ordered in 
time. Moreover, this ordered set possesses a mathematical structure that 
naturally yields the appearance of continuous physical laws and the conservation of quantities.
To understand how this works, we first clarify what we mean by measurement.

\subsection{The Countable Nature of Events}

Physical laws predict change.  Before change can be predicted it must be understand.
For instance, any expression involving a time derivative---such as
Newton’s relation between force and momentum---implicitly assumes the
existence of at least two distinguishable states of the world, one preceding
the other.  Without a countable sequence of admissible events, no notion of
variation or update is meaningful.  The following example illustrates how
even a familiar law such as momentum change depends fundamentally on the
existence of a discrete, ordered record of measurements.

\begin{example}[Momentum~\cite{newton1687}]
\NB{For a rigorous treatment of momentum see Phenomena~\cite{ph:momentum} and~\cite{ph:ang-momentum}}
\label{sse:measurement}
Physical laws relate measurements. For example, Newton’s second law~\cite{newton1687}
\begin{equation}
\label{eq:newton2}
F=\frac{dp}{dt}
\end{equation}
states that force relates to the \emph{change} in momentum over time. To speak of change you must have at least
two momentum values, one that \emph{comes before} the other; otherwise there is nothing to distinguish.
In set-theoretic terms, by the Axiom of Extensionality (assumed in Axiom~\ref{ax:peano}), different states must differ in their
contents, so ``change'' presupposes the distinguishability of two states.
\end{example}

In this framing, measurement values are \emph{counts} (cardinalities) of elementary occurrences: the number of
hyperfine transitions during a gate, the tick marks traversed on a meter stick, the revolutions of a wheel.
The \emph{event} is the action that makes previously indistinguishable outcomes distinguishable; the
\emph{measurement} is the observed differentiation (the count) between two anchor events.  This is not the
absolute measure of the event, but just relative difference of the two.  We count the events as time passes (See Thought Experiment~\ref{te:speedometer}.

A measurement device such as a speedometer does not report a universal time;
it compares successive entries in an observer’s record.  Its output reflects
the ordering of refinements, not an underlying temporal parameter.  Because
every operational notion of “duration’’ arises from counting successor steps
in a Peano-ordered ledger, no observer ever gains access to a global scalar
that represents time for all processes at once.  Different instruments,
different observers, and different experimental contexts may record their
successor chains at different rates, but all of them agree on the order in
which distinguishable events occur.  What is physically meaningful, and what
is operationally recoverable, is this ordered list of refinements.  It is
this list—not any global numerical clock---that we elevate to the first
physical principle.

\begin{axiom}[The Axiom of Cantor~\cite{cantor1895,earman1974}]
\label{ax:cantor}
\emph{[Time is an Ordinal Labeling]}

For every admissible record $(E,\prec)$ satisfying the Axiom~\ref{ax:causal},
there exists an injective, order-preserving map
\[
\tau : E \longrightarrow \omega
\]
into the von Neumann naturals $\mathbb{N}$ such that
\[
e \prec f \;\Longleftrightarrow\; \tau(e) < \tau(f).
\]
In particular, every finite segment of the record is order-isomorphic to an
initial segment $\{0,1,\dots,n-1\}$ of~$\omega$, and the ordinal labels
$\tau(e)$ provide a canonical indexing of events by their place in the
refinement sequence.
\end{axiom}

Once temporal duration is understood as the ordinal count of refinements
between events, there is no mechanism by which two spatially separated
observers can enforce a global notion of “now.”  Their clocks are simply
records of how many successor steps have occurred locally; different
instruments refine their ledgers at different rates depending on their
motion, causal environment, and measurement activity.  Because no observer
has direct access to the refinements of another, there is no operational
procedure that can align their ordinal labels into a single universal time
coordinate.

Attempts to synchronize distant clocks inevitably rely on signals---light
pulses, exchanged measurements, or other physical carriers of information.
But signals themselves are events in each observer’s ledger, and their
records of reception and transmission occupy different ordinal positions.
Thus “simultaneity’’ becomes frame-dependent: it is a relation defined by the
rules each observer uses to assign labels to their own causal interval, not a
global partition of the universe.

Relativistic simultaneity is therefore not a geometric postulate but a
consequence of the informational structure.  With time reduced to ordinal
successor count, two observers moving differently will, in general, generate
non-isomorphic refinements of their ledgers.  What one observer calls
simultaneous corresponds to different ordinal positions in another’s record.
The relativity of simultaneity follows from the impossibility of sharing a
single refinement sequence across distinct causal paths.

\begin{example}[Relativistic Simultaneity~\cite{einstein1905}.]
\NB{See Phenomenon~ref{ph:rel-sim} for a rigorous treatment.}
Two laboratories, $A$ and $B$, perform independent procedures, each producing
a finite measurement record.  Because the experiments are independent, their
events commute: no record in $A$ constrains the order of any record in $B$.
Both notebooks are internally consistent, but their events are mutually
unordered.

Now two observers, $C$ and $D$, travel past the laboratories on different
trajectories, each at a velocity close to the speed of light.  Their
instruments register signals from $A$ and $B$ in different sequences.  Since
the events commute, both observers are free to assemble the two notebooks
into different global orders.  Observer $C$ concludes that certain events in
$A$ precede those in $B$, while observer $D$ concludes the opposite.  Each
construction is internally consistent, because commutativity permits the
reordering.

The discrepancy is not a contradiction, but the finite analogue of
relativistic simultaneity: different trajectories generate different
admissible orderings of commuting events.  The events themselves may be
reordered independently of each other, yet the invariants are preserved.
\end{example}


\subsection{Observations are Fixed and Combinatorial}
\label{sse:finite}

A finite observer records events one at a time.  Each record refines the
set of admissible histories, and every refinement depends only on the
records accumulated so far.  Physical description is therefore necessarily
recursive: the $(k+1)$st step is constructed from the $k$ steps that
precede it.

The recursive description of physical reality is meaningful only within the
finite causal domain of an observer. Each step in such a description corre-
sponds to a distinct measurement or recorded event. Observation is therefore
bounded not by the universe itself, but by the observer’s own proper time and
capacity to distinguish events within it.

\begin{axiom}[The Axiom of Planck~\cite{planck1901}]
\label{ax:planck}
\emph[Observations are Finite and Immutable]
For any observer, the set of observable events within their causal domain
is finite.  The chain of measurable distinctions terminates at the limit of the
observer’s proper time or causal reach. These observations do not change over time.

More formally, there exists a finite precision scale $\mathcal{E}$ with
$0 < \mathcal{E} < \infty$ such that for every $e \in E$,
\begin{equation}
0 < |e| \le \mathcal{E},
\end{equation}
where $|e|$ denotes the magnitude of the distinguishable change associated
with $e$.
\end{axiom}

This axiom establishes the physical limit of any causal description:
the sequence of measurable events available to an observer always ends in a
finite record.  Beyond this frontier---beyond the end of the observer’s time---no
additional distinctions can be drawn.  The \emph{last event} of an observer
thus coincides with the top of their causal set: the boundary of all that can be
measured or known.

\subsection{Measurements Must Extend Without Contradiction}
The preceding axioms restrict the informational content of the record and the
structure of causal precedence.  We now introduce an axiom governing how
events may be selected in a consistent physical history.  A partial history is
a finite sequence of recorded distinctions that respects the causal order.  In
a locally finite causal set, many partial histories may be extended, but not
all extensions are admissible: each new event must be consistent with the existing record and may not
contradict any previously recorded distinction.

Axiom~\ref{ax:boltzmann} asserts that whenever we impose countably many
local admissible requirements---each representing a physically permitted
constraint---there exists at least one consistent history that satisfies
all of them\footnote{In the continuum limit, when observables range over a complete
set of measurable values, the admissible history is unique up to sets of
measure zero: there is exactly one continuous completion consistent with
all recorded refinements.}.
  Mathematically, this parallels the role of Martin's Axiom
in set theory, where dense sets encode constraints and a filter selects
a coherent global object \cite{jech2003,kunen1980,martin1970,todorcevic2010}.
Physically, it echoes Boltzmann's principle that every admissible
microstate selection must preserve distinguishability \cite{boltzmann1896},
and follows the causal-set program in which a spacetime history is
constructed one event at a time under admissible refinement
\cite{bombelli1987,finkelstein1996}.  Hilbert's call to axiomatize the
foundations of physics \cite{hilbert1902} is realized here as a minimal
requirement: if each local constraint is permissible, then some coherent
global history must also be permissible.

At the heart of the Axiom of Boltzmann is the concept of a partially ordered set.
\begin{definition}[Partially Ordered Set~\cite{davey2002}]\label{def:poset}
A poset is a pair $(E,\leq)$ where $\leq$ is a binary relation on $E$ satisfying:
\begin{enumerate}
  \item \textbf{Reflexivity:} $e \leq e$ for all $e \in E$
  \item \textbf{Antisymmetry:} if $e \leq f$ and $f \leq e$, then $e = f$
  \item \textbf{Transitivity:} if $e \leq f$ and $f \leq g$, then $e \leq g$
\end{enumerate}
\end{definition}

Such an ordering always admits at least one maximal element~\cite{bombelli1987}
\begin{definition}[Top of a Poset~\cite{davey2002}]
\label{def:top}
Let $(E,\leq)$ be a partially ordered set.  The \emph{top} of $E$, denoted
$\mathrm{Top}(E)$, is the set of maximal elements of $E$:
\begin{equation}
\label{eq:top}
\mathrm{Top}(E) = \{\, e \in E \mid \nexists f \in E \text{ with } e < f \,\}.
\end{equation}
That is, $\mathrm{Top}(E)$ contains those events in $E$ for which no strictly
greater event exists.
\end{definition}

The elements of \(\mathrm{Top}(E)\) represent the current causal frontier---the 
most recent events that have occurred but have no successors~\cite{sorkin2005}.  
Although \(\mathrm{Top}(E)\) may contain several incomparable (spacelike) 
elements, it is never empty and therefore provides a well-defined notion of a 
``last event'' from the observer’s perspective. 

\begin{axiom}[The Axiom of Boltzmann~\cite{boltzmann1877,martin1970}]
\label{ax:boltzmann}
\emph{[Events are Selected to be Coherent.]}
An experiment may impose many local causal requirements: detector
constraints, boundary conditions, conservation rules, and so on.
As long as each requirement can be satisfied on its own, the Axiom of
Boltzmann asserts that there always exists at least one, globally coherent
history satisfying \emph{all} of them simultaneously.  No matter how many
local constraints we specify, they can be assembled into one consistent
record.

Formally, let $(\mathsf{P},\leq)$ be the partially ordered set (Definition~\ref{def:poset}) of
finite, order-consistent partial histories in a locally finite causal
domain, ordered by extension.  For every countable family
$\{D_n\}_{n\in\mathbb{N}}$ of dense subsets of $\mathsf{P}$ (local causal
constraints), there exists a filter $G\subseteq\mathsf{P}$ such that
$G\cap D_n\neq\varnothing$ for all $n$.
\end{axiom}


\section{The Causal Universe Tensor}
The axioms above determine the structure of the physical record: events form a
locally finite causal set, extensions of partial histories preserve causal
consistency, and informational minimality forbids unrecorded structure.  What
remains is to represent this record in a mathematical form that allows the
accumulation of distinctions.  We now construct such a representation.

\subsection{Sets of Events}
\label{sse:eventsets}

Let the set of all events accessible to an observer be denoted $E$\footnote{
The symbol $E$ here denotes the \emph{set of distinguishable events}---it is
not the energy operator or expectation value familiar from mechanics.
Throughout this work, $E$ indexes discrete occurrences in the causal order,
while quantities such as energy, momentum, or stress appear only later as
\emph{derived measures} on this set.
}, ordered by causal precedence $(\prec)$.  
Because any physically realizable region is finite, this order forms a locally finite partially ordered set (poset)~\cite{finkelstein1988causal}.

\begin{definition}[Causal Precedence~\cite{bombelli1987}]
\label{def:causalprecedence}
Let $E$ be the set of distinguishable events accessible to an observer.
For $e_i,e_j \in E$, we say that $e_i$ \emph{causally precedes} $e_j$,
written $e_i \prec e_j$, if the record of $e_j$ cannot be formed without
already having distinguished $e_i$.  Equivalently, $e_j$ refines the
admissible outcomes of $e_i$.  The relation $\prec$ is a strict partial
order: it is irreflexive ($e \not\prec e$), antisymmetric, and transitive.

\NB{The term ``causal'' is used only in the sense of ordering:
$e_i \prec e_j$ asserts that $e_j$ depends on the distinctions recorded
in $e_i$.  No geometric notion of signal propagation or physical
influence is assumed.}
\end{definition}


Each admissible set of events may be represented as a locally finite
partially ordered structure~\cite{bombelli1987,sorkin1991},
whose links record only those relations that are causally admissible.
In this view, a ``history'' is not a continuous trajectory but a
combinatorial diagram: every vertex an event, every edge a permissible
propagation.

This discrete formulation generalizes the intuition behind
Feynman's space--time approach to quantum mechanics, in which the
amplitude of a process is obtained by summing over all consistent
histories~\cite{feynman1948,feynman1965}.
The Feynman diagram thus motivates a special case of the causal
network itself---a pictorial reduction of the full tensor of event
relations---and the path integral becomes a statement of global
consistency across all measurable causal connections.

\begin{example}[Feynman Diagrams (classical)~\cite{feynman1965}]
\NB{This is a classical simplification of the highly specialized notation of
the Feynman diagram.  See Thought Experiment~\ref{te:feynman-full} for a
more rigorous treatment.}

In conventional quantum field theory, a Feynman diagram depicts a sum
over interaction histories connecting initial and final particle states.
Each vertex represents an elementary event---an interaction that renders
previously indistinguishable outcomes distinct---and each propagator
represents the possibility of causal influence between events.

In the present formulation, such a diagram is naturally interpreted as a
finite \emph{causal network}.  The set of vertices corresponds to the event
set $E$, and the directed edges encode the order relation $\prec$ defined
by Axiom~\ref{ax:cantor}.  To each event $e_k$ we associate a representation
$\mathbf{E}_k$ that records the admissible refinement induced by that
event, and the directed structure describes which refinements must
precede others.  The composition of these event tensors gives the Causal
Universe Tensor of the inertial frame:
\begin{equation}
\mathbf{U}_n = \prod_{k=1}^{n} \mathbf{E}_k.
\end{equation}

At this stage, $\mathbf{U}_n$ is a classical accumulator: it records the
count and structure of distinguishable events without assigning amplitudes
or phases.  This is deliberate.  The present framework concerns only the
logical bookkeeping of distinctions.  The full quantum structure---including
complex amplitudes, superposition, and interference---appears only after
the informational gauge is introduced.  In that setting, the classical
accumulator becomes the coarse projection of a richer amplitude algebra,
much as a Feynman diagram may be viewed as the combinatorial skeleton of
a path integral.  That generalization is deferred until
Chapter~\ref{chap:mass}, where the amplitude-bearing form of $\mathbf{U}$
is constructed.

Summing over all consistent diagrams is therefore equivalent to enumerating
all admissible orderings of distinguishable events.  The path integral
itself becomes a statement of \emph{global consistency} across the entire
causal network: every measurable amplitude corresponds to a possible
embedding of finite causal order into the continuous limit.  In this sense,
a Feynman diagram is not merely a pictorial tool, but a discrete
representation of the causal tensor algebra from which continuum physics
emerges.
\end{example}


This identification is pedagogically useful.  From this point onward, every
construction may be viewed as an algebraic generalization of the familiar
Feynman diagram:  the event tensors are its vertices, the causal relations
its edges, and the Cauasl Universe Tensor the cumulative sum over all consistent
orderings.  The remainder of the monograph simply formalizes this graphical
intuition in set--theoretic and tensorial language, rather than using calculus.

Every event $e \in E$ corresponds to an irreducible distinction in the
experimental record.  Under the measurable embedding
$\Psi : E \rightarrow \mathcal{T}$ introduced in Thought
Experiment~\ref{ex:psi-lab}, each logical event is mapped to an algebraic
object $\mathbf{E}_e$ in the tensor algebra.  These objects compose
whenever their corresponding events are compatible in the causal order,
so the accumulation of observed events yields a record that reflects the
ordered refinement of the causal set.

The goal of this section is to define a cumulative object $\mathbf{U}_n$
---the \emph{Causal Universe Tensor}---that embodies the total
informational content of all events observed up to step $n$ in the
current inertial reference frame.  This tensor is not a dynamical
evolution. 
It is the bookkeeping device that records how refinements have survived
admissibility by accumulating exactly those features that remain invariant
under all allowed extensions of the record.

It is crucial to emphasize that no background time parameter is introduced.
There is no external clock and no continuous variable $t$ against which
events are measured.  Instead, Axiom~\ref{ax:cantor} guarantees that the
causal set admits a linear extension: the events can be listed in a
sequence that respects causal precedence.  In this framework, \emph{time}
is merely the ordinal index of an event in such a sequence.  It is not a
physical field or metric quantity, but a bookkeeping device that labels
the relative order of observations.

With this viewpoint, accumulating the event tensors in order is not
evaluating a function of time.  It is forming the ordered product of
distinctions that have occurred.  The resulting object, the Causal
Universe Tensor, represents the total recorded history up to any chosen
ordinal position in the list of events.

\subsection{Refinement}

This observation motivates the first physical axiom: that time is not an
independent scalar field but an ordinal index on the partially ordered set of
distinguishable events.  Each admissible refinement increments this ordinal
by one count, and an observer’s “clock’’ is simply a local parametrization of
that count within their own causal domain.  When two observers’ causal
domains overlap, their records admit a common refinement: the locally finite
structure ensures that their rank assignments agree up to order-isomorphism
on the shared events.  What differs is only the density with which each
observer samples the causal order.  The apparent continuity of time is thus
the smooth shadow of many closely spaced refinements, not an underlying
continuum of duration.

\begin{definition}[Rank time~\cite{bombelli1987,davey2002}]\label{def:rank-time}
Let $(E,\prec)$ be a locally finite \emph{partially ordered set of events}. A rank time is an order-embedding
\[
\tau : E \to \mathrm{Ord}
\]
satisfying $e \prec f \implies \tau(e) < \tau(f)$. Local finiteness implies that for any observer's causal domain $D \subseteq E$, $\tau(D)$ is order-isomorphic to an initial segment of $\mathbb{N}$. We therefore define the \emph{duration}, $|\delta t|$, between anchors $a \prec b$ by
\[
|\delta t|(a,b) \;=\; \#\{\, e \in E \mid a \prec e \prec b \,\}\in \mathbb{N}.
\]
Two rank functions $\tau,\tau'$ are \emph{equivalent} if there exists an order-isomorphism $\phi$ with $\tau'=\phi\circ\tau$; equivalent ranks yield identical durations.
\end{definition}

A finite observer never encounters the world as a continuum.  What is
available are discrete, distinguishable outcomes recorded one at a time.
The informational content of the record grows only when a new measurement
produces a distinction that was not previously present.  Such an addition is
a \emph{refinement}: an admissible strengthening of the observer’s causal
ledger that preserves all earlier distinctions while adding a new one.  

Refinements are the fundamental units of temporal structure.  The ordinal
indexing of time (Definition~\ref{def:rank-time}) arises because each
refinement appends a successor in the causal order.  When two observers’
causal domains overlap, their records admit a common refinement: any
discrepancy in their descriptions can be resolved by adding further
distinctions until both records agree on all shared events.  Refinement
therefore functions as the basic consistency operation—the procedure that
allows independent descriptions of the world to be compared, merged, and
extended without contradiction.

Before introducing the Causal Universe Tensor, we require a precise
definition of this operation.

\begin{definition}[Refinement~\cite{dirichlet1850}]
\label{def:refinement}
Let $(E,\prec)$ be an admissible record and let $e$ be a distinguishable
event not already in $E$.  A \emph{refinement} of $E$ by $e$ is the formation
of a new record
\[
E' = E \cup \{e\},
\]
equipped with the smallest partial order extending $\prec$ such that every
causal relation already present in $E$ is preserved and $e$ is placed in a
position consistent with all admissible observations.  A refinement must
respect:
\begin{enumerate}
\item \textbf{Irreversibility}: no event in $E$ is removed or weakened;
\item \textbf{Distinguishability}: the increment $|e|$ is strictly positive and finite;
\item \textbf{Order-consistency}: the updated poset remains locally finite and acyclic.
\end{enumerate}
If $E_1$ and $E_2$ are admissible records, a record $F$ is a
\emph{common refinement} if $F$ refines both $E_1$ and $E_2$.
\end{definition}


\subsection{On the Structure of Measurement}

In this formulation, a measurement is not the evaluation of a continuous
quantity against an external time parameter.  No clock, ruler, or metric is
assumed.  Instead, the Axioms of Planck and Cantor assert that an observer's
record is a locally finite, causally ordered set of distinguishable events.
To extract a numerical value from such a record, one must identify which
events satisfy a specified property and count how many of them occur between
two anchors in the causal order.

This viewpoint treats measurement as a purely combinatorial act: the
\emph{value} of a measurement is the number of admissible distinctions
satisfying a predicate inside a finite causal interval.  The result is always
an integer, and continuity---when it appears---arises only as the smooth
limit of increasingly refined counts.  We formalize this as follows.


\begin{definition}[Measurement~\cite{wheeler1990}]
\label{def:measurement}
Let $(E,\prec)$ be a locally finite partially ordered set of events, and let
$P:E\to\{0,1\}$ be a predicate designating which events satisfy a specified
property.  For two anchor events $a,b\in E$ with $a\prec b$, the
\emph{measurement of $P$ between $a$ and $b$} is the finite integer
\[
M_P[a,b]
\;:=\;
\bigl|\{\,e\in E:\; a\prec e\prec b \text{ and } P(e)=1\,\}\bigr|
\;\in\;\mathbb{N}.
\]
That is, a measurement is a count of distinguished events satisfying $P$
within the causal interval $(a,b)$.
\end{definition}

A measurement in this setting is therefore nothing more than a count of
distinguished events between anchors.  Numerical values arise only when such
counts are compared against a conventional scale.  No continuous quantity is
assumed \emph{a priori}; continuity is inferred from the refinement of a
finite causal record.  In practice, every physical “number” depends on a
calibration that relates discrete counts to a chosen system of units.

The analysis concerns only the \emph{structure of measurement itself}:
the mathematical relations among counts of distinguishable events that
underlie all physical observations.  In this framing, physics is viewed
as a grammar of distinctions.  The familiar constants and fields---mass,
charge, curvature, temperature---arise as \emph{derived measures} within a
finite causal order, not as independent entities.

\begin{phenomenon}[The Chomsky Effect~\cite{chomsky1956,chomsky1957}]
\label{ex:bnf-measurement}
\NB{Measurement is a formal writing system. Each observation selects a
symbol from a finite alphabet, and a record is the word formed by these
selections. No physical semantics are assumed; the structure is purely
syntactic in the Backus--Naur sense~\cite{backus1959,naur1963}. In the
spirit of Wheeler's dictum that information is fundamental~\cite{wheeler1990},
the act of measurement is treated here as the creation of symbolic
distinctions, nothing more.}

Because measurement produces distinguishable outcomes, each observation
selects a symbol from a finite or countable alphabet
\[
\Sigma = \{\sigma_1, \sigma_2, \ldots \}.
\]
A record of $n$ measurements is therefore a word $w \in \Sigma^n$.  When
an instrument is refined—by increasing precision or reducing noise—any
coarse symbol $\sigma_k$ may be replaced by a finite set of more precise
symbols,
\[
\sigma_k \;\Rightarrow\; \sigma_{k,1} \;\big|\; \sigma_{k,2} \;\big|\;
\cdots \;\big|\; \sigma_{k,r},
\]
just as in a Backus--Naur Form (BNF) production rule
\cite{backus1959,naur1963}.  Not all replacements are admissible: they
must remain compatible with every other measurement that overlaps in
time or causal order.  Two refined histories that disagree on an
overlapping interval cannot both represent valid records.

Thus admissible measurement histories form a formal language generated by
the allowed refinement rules.  The ``law'' governing measurement is the
constraint that only globally consistent extensions of a record may be
generated.  This is not an analogy: it is the standard formal structure
of symbol sequences in coding and information theory \cite{sipser1997}.
\end{phenomenon}

Measurements do not reveal an underlying continuum; they create
distinctions.  Each admissible event is a refinement that separates two
previously indistinguishable possibilities and appends a new token to the
observer's record.  As these refinements accumulate, they form a chain of
distinguishable outcomes, each one justified by an operation whose effects
leave a finite trace.  This chain is not optional: without it there is no
basis on which an observer can assert difference, change, or causality.

Because refinements are irreversible (Axiom~\ref{Kolmogorov}), and because each
refinement must be consistent with all earlier ones (Axiom~\ref{ax:boltzmann}), the
record grows in a definite order.  The resulting sequence of distinguishable
events is therefore well-founded and locally finite.  It is the only
structure every observer can agree upon: not a metric, not a geometry, but a
chain of distinctions that survived admissibility.

This chain is the backbone of the causal ledger.  All temporal notions,
all refinements, and all subsequent tensor representations derive from the
ordering and accumulation of these distinguishable events.

\begin{definition}[Distinguishability Chain~\cite{kolmogorov1933}]
Let $\Omega$ be a nonempty set.
A distinguishability chain on $\Omega$ is a sequence
$\mathcal{P}=\{P_n\}_{n\in\mathbb{Z}}$ of partitions $P_n\in\Part(\Omega)$ such that
$P_{n+1}$ \emph{refines} $P_n$ for all $n$ (every block of $P_{n+1}$ is contained in a block of $P_n$).
Write $\Blocks{P}$ for the set of blocks of a partition $P$
Each refinement step produces zero or more event.
\end{definition}

A finite observer cannot access the world continuously; they access it only
through operations that produce finite, irreversible traces.  Each such trace
marks a distinction that was not present before the operation was performed.
These distinctions are the primitive units of information: without them there
is no basis for asserting difference, change, or causality.  

What survives in the observer’s notebook is not the underlying process but
the residue of those operations that produced a new, admissible refinement.
This residue must be discrete (Axiom~\ref{ax:planck}), persistent (Axiom~\ref{ax:kolmogorov}), 
and compatible with all earlier residues (Axiom of~\ref{ax:boltzmann}).
It is therefore not a “state” of the world but the smallest unit of
distinguishability that can be justified by operational means.

We call such a justified, persistent, distinguishable token an \emph{event}.

\begin{definition}[Event~\cite{kolmogorov1933,sorkin2005}]\label{def:event}
Fix a distinguishability chain $\mathcal{P}=\{P_n\}$.
An event at index $n$ is a minimal refinement step:
a pair
\begin{equation}
\label{eq:eventdef}
e=(B,\{B_i\}_{i\in I},n)
\end{equation}
such that:
\begin{enumerate}
  \item $B\in\Blocks{P_n}$;
  \item $\{B_i\}_{i\in I}\subseteq \Blocks{P_{n+1}}$ is the family of all blocks of $P_{n+1}$ contained in $B$,
        with $|I|\ge2$ (a nontrivial split);
  \item (\emph{minimality}) there is no proper subblock $C\subsetneq B$ with $C\in\Blocks{P_n}$ for which
        the family $\Blocks{P_{n+1}}\cap\mathcal{P}(C)$ is nontrivial.
\end{enumerate}
Let $E$ denote the set of all such events.
We define a strict order on events by
$e\prec f \iff n_e<n_f$, where $n_e$ denotes the index of $e$
\end{definition}

All temporal structure in this framework arises from refinement.  An
observer’s clock does not measure a flowing background parameter; it counts
the distinguishable refinements that occur along the observer’s own causal
path.  This count is intrinsic: no other observer can directly access or
modify the sequence of refinements recorded within a given worldline, and no
external synchronization procedure can force two observers to share the same
refinement density.

The ordinal rank provided by Definition~\ref{def:rank-time} therefore acquires
a special status when restricted to a single causal thread.  Along such a
thread, refinements occur in a fixed order, with no ambiguity or
branching.  The resulting sequence forms the unique, locally defined measure
of temporal progression available to the observer.  It is immune to
coordinate choices, independent of any geometric embedding, and invariant
under all admissible reparametrizations of the global causal set.

This observer-specific refinement count is what we call \emph{proper time}.
It is the intrinsic temporal measure of a causal path: the duration encoded
by the observer’s own chain of distinguishable events, not the duration
assigned by any external chart or coordinate system.

\begin{definition}[Proper Time~\cite{misner1973}]
\label{def:proper-time}
Let $E$ be the set of events generated by a distinguishability chain $P=\{P_n\}$. 
For any two events $a,b\in E$ with $a\prec b$, the \emph{proper time} between them is
\[
\tau(a,b)
=
\max\Bigl\{
\,|C|:\; C=\{c_0,\dots,c_k\}\subseteq E,\;
a=c_0\prec c_1\prec\cdots\prec c_k=b
\Bigr\}.
\]
That is, $\tau(a,b)$ is the cardinality of a maximal chain of strictly refinable
events between $a$ and $b$. Local finiteness of the distinguishability chain
guarantees $\tau(a,b)\in\mathbb{N}$.
\end{definition}

Once proper time is understood as the intrinsic count of refinements along a
causal thread, it follows that an observer cannot refine all aspects of a
measurement record arbitrarily.  Each admissible event consumes part of the
finite informational budget supplied by the axioms: every refinement increases
distinguishability in one direction while limiting the refinement capacity
available to its conjugate descriptions.  In the smooth shadow, these
dual directions appear as position and momentum, slope and curvature, or more
generally, a variable and its rate of change.  The constraint is purely
combinatorial: a ledger with finite precision cannot allocate unlimited
distinguishability to both simultaneously.  This is the informational origin
of the Heisenberg effect.

\begin{phenomenon}[The Heisenberg Effect~\cite{heisenberg1927}]
\label{ph:heisenberg}
A refinement ledger with finite precision cannot simultaneously resolve both
a quantity and the variations of that quantity with arbitrarily high
accuracy.  Increasing the precision of a measurement consumes refinement
capacity that would otherwise distinguish how that measurement changes across
successive refinements.  Perfect specification of a value therefore requires
an unbounded refinement cost in its variation.

Every admissible refinement encodes a finite, irreversible distinction.  
To sharpen the measured value of a
quantity, the ledger must allocate refinements to its instantaneous
distinguishability.  To resolve how that value changes---its rate, slope, or
local variation---the ledger must allocate refinements to successive
differences in the same causal neighborhood.  These two informational tasks
draw from the same finite refinement budget.  Allocating refinements to fix a
value exhausts the capacity needed to record its variability, and allocating
refinements to variability reduces the capacity available to specify the

The Heisenberg Effect expresses the structural tradeoff between measuring a
quantity and measuring how it changes.  The familiar uncertainty relations of
continuum physics arise as the smooth shadow of this discrete bookkeeping
constraint: a finite ledger cannot support unbounded precision in both value
and variation at once.
\end{phenomenon}

It is obvious that related measurements must constrain each other.  We now turn
our attention to unreleasted measurements.
The notion of \emph{uncorrelant events} formalizes the idea that two recorded
distinctions may be independent of one another.  In causal set theory,
incomparability under the causal order corresponds to physical independence
of events \cite{bombelli1987}.  The same conceptual separation appears in
quantum theory, where observables acting on independent subsystems commute
and their measurement outcomes do not influence each other
\cite{dirac1958,peres1995}.  Classical discussions of separated systems, from
Einstein--Podolsky--Rosen and Schr\"odinger to Wheeler's formulation of
complementarity \cite{einstein1935,schrodinger1935,wheeler1983}, frame the
same idea operationally: when no physical procedure can distinguish the
relative order of two events, their ordering has no empirical content.  The
definitions below captures this in the minimal set-theoretic language of the
causal poset.


\begin{definition}[Uncorrelant~\cite{bombelli1987,sorkin1991}]
\label{def:uncorrelant}
Let $(E,\prec)$ be a locally finite partially ordered set of events. Two
events $e,f\in E$ are said to be \emph{uncorrelant} if they are incomparable
under the causal order; that is,
\[
\neg(e\prec f)\quad\text{and}\quad \neg(f\prec e).
\]
The uncorrelant relation partitions $E$ into equivalence classes of events
whose relative order carries no operational consequence for any admissible
measurement or refinement.  In particular, no experimentally distinguishable
difference follows from interchanging the positions of uncorrelant events
in any linear extension of $(E,\prec)$.
\end{definition}

A single observer’s ledger records only the refinements that occur along one
causal path.  But the physical world is not built from one thread of
refinement; it is a tapestry of many locally generated records, each
produced by a finite observer interacting with its own environment.  Whenever
two observers can exchange signals or compare outcomes, the distinctions they
record must cohere: refinements in one ledger must not contradict refinements
in another.  The structure that collects these many partial records into a
globally consistent object is the causal network.

A causal network arises from stitching together locally finite chains of
distinguishable events—each chain representing the refinement history along a
particular worldline—and enforcing the rule that shared events must appear in
the same order in every ledger that records them.  This requirement of
overlap consistency ensures that independently produced descriptions of the
world can be merged into a single, coherent partial order.  The resulting
network is not a manifold or a geometry but a combinatorial object: a web of
refinement relations encoding which events can influence which others.

Before introducing continuous shadows or dynamical laws, we must give a
precise definition of this network, for it is the primitive structure from
which all temporal, kinematic, and geometric notions will eventually emerge.


\begin{definition}[Causal Network~\cite{bombelli1987}]
\label{def:causal-network}
Let $E$ be a finite set of admissible events and let $\triangleright$ denote
the \emph{immediate causal cover}: $e \triangleright f$ if and only if $e < f$
and there exists no $g \in E$ such that $e < g < f$.  The \emph{causal network}
is the directed graph $(E, \triangleright)$ whose vertices are the events in
$E$ and whose directed edges record the immediate causal relations.
\end{definition}

This network is the combinatorial diagram of the event record: each vertex is
a distinguishable event, and each directed edge $e \triangleright f$ certifies
that $f$ cannot be observed without first observing $e$.  Its transitive
closure recovers the full causal order $<$ of Definition~\ref{def:causal-order}.
See Phenomenon~{ph:feynman-diagram} for a rigorous treatment.

Each observer’s ledger records a locally generated sequence of refinements:
a chain of distinguishable events ordered by the succession in which they were
justified.  But physical claims cannot depend on a single observer’s record.
Whenever two observers interact, exchange signals, or jointly participate in
an experiment, their ledgers must agree wherever their domains overlap.  This
overlap consistency requires that any event witnessed by both observers appear
in the same relative order in both records.

The only structure capable of enforcing such universal compatibility is a
global causal order: a partial order that extends every observer’s local
refinement chain while preserving all shared precedence relations.  Local
threads become linearly ordered segments of a single, globally coherent
network; disagreements in refinement density are permitted, but disagreements
in causal order are not.  The global order contains exactly those precedence
relations that survive all admissible mergers of observational records.

Before we can speak of continuous shadows, tensor embeddings, or dynamical
laws, we must formalize this universal ordering relation.  It is the minimal
structure that any coherent universe must admit.


\begin{definition}[Causal Order~\cite{bombelli1987}]
\label{def:causal-order}
Let $P=\{P_n\}_{n\in\mathbb{Z}}$ be a distinguishability chain of partitions, and let an
event be $e=(B,\{B_i\}_{i\in I},n)$ as in Definition~\ref{def:event}, where $B\in\mathrm{Bl}(P_n)$ splits
nontrivially into child blocks $\{B_i\}\subset\mathrm{Bl}(P_{n+1})$.

For $m>n$ and $C\in\mathrm{Bl}(P_m)$, let $\pi_{m\to n}(C)\in\mathrm{Bl}(P_n)$ denote the unique
ancestor block in $P_n$ containing $C$ (well-defined because $P_{n+1}$ refines $P_n$).
Define the \emph{immediate causal cover} relation $e\triangleright f$ between events
$e=(B,\{B_i\},n)$ and $f=(C,\{C_j\},m)$ by
\[
n<m
\quad\text{and}\quad
\pi_{m\to n+1}(C)\subseteq B_i\ \text{for some child}\ B_i\ \text{created by } e.
\]
The \emph{causal order} $\prec$ on the event set $E$ is the transitive closure of
$\triangleright$:
\[
e\prec f \iff \text{there exist events } e=e_0,e_1,\dots,e_k=f \text{ with } e_i\triangleright e_{i+1}\ \text{for all }i.
\]
Then $(E,\prec)$ is a locally finite partially ordered set (reflexivity suppressed for strictness),
where incomparability is allowed: it may happen that neither $e\prec f$ nor $f\prec e$.
\end{definition}

As an illustration, recall the twin paradox of the previous chapter\footnote{See Coda: The Twin Paradox, Chapter 1.}.
In the informational gauge, proper time is not a geometric interval but the
work of reconciling distinguishable events.  The traveling twin accrues a
denser log of refinements---engine burns, course corrections, telemetry---while
the stay-at-home twin records a coarser sequence.  When their notebooks are
merged into a single coherent history, the richer record requires strictly
greater informational effort to reconcile.  Equivalently, the proper time of
the unaccelerated twin is necessarily longer, because her history contains
fewer distinctions and therefore a larger merge is required to absorb those
recorded by her sibling.  In the smooth limit this appears as a shorter
proper time along the curved worldline, but the effect is not mysterious:
it is the discrete fact that one history contains more recorded distinctions
than the other.  Geometry only codifies what measurement already certified.


\subsection{Accumulation of Measurement}

Operationally, every observation can be decomposed into three layers:
\begin{enumerate}
  \item the \textbf{logical} layer---which events are distinguishable;
  \item the \textbf{mathematical} layer---how those distinctions are counted;
  \item the \textbf{physical} layer---how the resulting counts are named and
        parameterized as energy, momentum, or time.
\end{enumerate}
By isolating the first two layers, we obtain a calculus of variations that is universal
to any admissible physics: a closed system of relations that expresses how
order itself becomes measurable.

\begin{phenomenon}[The Bacon Effect~\cite{bacon1620}]
All admissible physical knowledge is restricted to the experimental record:
the finite, irreversible sequence of distinguishable events that an observer
can justify by operational means.  No physical claim may outrun this record,
and no structure may be admitted that cannot, in principle, leave a finite
trace within it.

A measurement produces a refinement---a new distinction appended to the causal
ledger.  This refinement cannot be erased, must be consistent with all earlier 
refinements, and must have finite resolution.  Independent observers who interact must
agree on all shared refinements, and their ledgers must admit a common
extension.  The union of all such ledgers, stitched together through overlap
consistency, forms the global experimental record.  It is the only invariant
structure that survives every admissible merger of observational histories.

The Experimental Record asserts that physics is an empirical
discipline in a precise, combinatorial sense: the universe is known only
through the distinctions that have actually survived admissibility.  Smooth
fields, geometric intervals, dynamical laws, and tensor representations
appear only as shadows of this discrete record.  The experimental record is
therefore the primitive data structure of the theory—the source of all
invariants and the boundary of all admissible description.
\end{phenomenon}

We now define the experimental record mathematically as a time series of events.

\begin{definition}[Time Series~\cite{box1976}]
\label{def:time-series}
Let $(E,\prec)$ be a locally finite partially ordered set of admissible
events.  A \emph{time series} is a finite or countably infinite sequence
\[
e_1 \prec e_2 \prec e_3 \prec \cdots
\]
such that each $e_{k+1}$ is a refinement of the record containing
$\{e_1,\dots,e_k\}$ and no two distinct events share the same position in the
sequence.  The ordering reflects the succession in which distinguishable
refinements were justified by an observer.
\end{definition}


\begin{definition}[Experimental Record]
\label{def:experimental-record}
An \emph{experimental record} is a time series
\[
R = \langle e_1 \prec e_2 \prec \cdots \prec e_n \rangle
\]
consisting of all admissible refinements produced by a finite observer during
interaction with the world.  Each $e_{k+1}$ records a distinguishable,
irreversible refinement consistent with all earlier entries, and each entry
survives admissibility under the axioms of Planck, Kolmogorov, Boltzmann, and
Peano.

If two observers have overlapping causal domains, their experimental records
must agree on the order of all shared events.  The union of all mutually
consistent experimental records forms a globally defined partial order, the
\emph{global experimental record}, which serves as the unique causal backbone
of the theory.
\end{definition}

\begin{proposition}[The Experimental Record Is a Hilbert Vector]
\label{prop:record-hilbert}

Let $R=\langle e_1 \prec e_2 \prec \cdots \prec e_n\rangle$ be an experimental
record (Definition~\ref{def:experimental-record}), and let
$\Psi : E \to \mathcal{H}$ be the continuous representation map into a real,
separable Hilbert space $\mathcal{H}$ obtained by taking the Cauchy
completion of the refinement increments (Axiom of Cantor).  Then:
\end{proposition}

\begin{proofsketch}{record-hilbert}
\begin{enumerate}
\item \textbf{(Vector assignment)}  
      Each refinement $e_k$ determines a vector
      $v_k := \Psi(e_k) \in \mathcal{H}$ of finite norm, because
      refinements have finite informational magnitude (Axiom~\ref{ax:planck}).

\item \textbf{(Vector additivity)}  
      The cumulative record
      \[
      V_R \;:=\; v_1 + v_2 + \cdots + v_n
      \]
      is a well-defined element of $\mathcal{H}$, and the construction is
      compatible with the vector-space axioms:
      \[
      (V_R + V_{R'}) = (v_1+\cdots+v_n) + (v_1'+\cdots+v_m'),
      \]
      addition is associative and commutative, and for all $\alpha \in
      \mathbb{R}$,
      \[
      \alpha V_R = \alpha v_1 + \cdots + \alpha v_n.
      \]
      Thus the experimental record combines linearly.

\item \textbf{(Hilbert-space norm)}  
      The Hilbert norm induced by the inner product satisfies
      \[
      \| V_R \| = \sqrt{\langle V_R, V_R\rangle},
      \qquad
      \| V_R + V_{R'} \| \le \|V_R\| + \|V_{R'}\|,
      \]
      so record vectors obey the triangle inequality.  Informationally,
      the distinguishability budget of a combined record cannot exceed
      the sum of the budgets of its parts.

\item \textbf{(Observer invariance)}  
      If two observers possess experimental records $R$ and $R'$ with
      overlapping causal domains, the overlap consistency requirement
      (Axiom of Boltzmann) implies that $V_R$ and $V_{R'}$ agree on all
      shared refinement vectors.  Thus the vector assigned to a record is
      invariant under all admissible mergers of observational histories.

\item \textbf{(Density)}  
      Since $\mathcal{H}$ is the Cauchy completion of the refinement
      increments, the span of all experimental record vectors is dense in
      $\mathcal{H}$.  Every element of the Hilbert space can be
      approximated arbitrarily well by finite linear combinations of
      record vectors.

\end{enumerate}

Every experimental record determines a unique vector $V_R$ in the Hilbert
space $\mathcal{H}$.  The experimental record is therefore a Hilbert vector:
the continuous, linear shadow of a discrete sequence of admissible
refinements.
\end{proofsketch}

Having established that every experimental record determines a unique vector
in a separable Hilbert space, we may now admit Hilbert spaces as legitimate
computational shadows of the discrete ledger.  This is the first point in
the development at which such continuous structures become operationally
warranted: the linearity and Cauchy completeness of the Hilbert space arise
directly from the refinement structure of the record, not from geometric or
quantum assumptions.  Subsequent sections will introduce additional Hilbert
spaces—each justified in the same manner---as analytic environments in which
operations on the experimental record can be approximated, compared, and
extended without exceeding the informational content of the underlying
discrete events.

\begin{definition}[Event Tensor~\cite{golub2013}]
\label{def:eventtensor}
Let $V$ be a finite-dimensional real vector space of measurable quantities.
An event tensor $\mathbf{E}_k \in \mathcal{T}(V)$ encodes the
distinguishable contribution of the $k$th event $e_k \in E$ to the
cumulative record.  It is related to the logical event by a measurable
embedding
\begin{equation}
\Psi : E \to \mathcal{T}(V), \qquad \mathbf{E}_k = \Psi(e_k).
\end{equation}
No algebraic relations are assumed beyond those required by linearity:
$\mathbf{E}_k$ is simply the algebraic image of the $k$th logical
distinction.
\end{definition}

An individual event tensor records a single admissible refinement of the
measurement record.  To represent the cumulative effect of many events, we
must specify how these algebraic objects combine.  Because the causal set is
ordered only up to informational precedence, the combination rule must
respect a chosen linear extension of the partial order and must make no
assumptions of commutativity.  This leads naturally to a left--multiplicative
update: each new event contracts the admissible record of all that precede
it, and the cumulative history is represented by the product of these
restricted increments along any finite prefix of the causal chain.

The combination rule corresponds directly to the set--theoretic refinement
of admissible outcomes.  At each step, the new logical event is not taken in
isolation, but restricted against all prior observations:
\[
e_{k+1}' := e_{k+1} \cap \bigcap_{j=1}^{k} \hat{R}(e_j),
\]
where $\hat{R}$ is the operator that removes outcomes incompatible with the
existing record.  In this framework, physical laws appear nowhere
else: they are encoded entirely in the restriction operator.  What survives
admissibility is physical; what is removed was never a possible history.

In the
algebraic domain this restriction is represented by
\[
\mathbf{U}_{k+1}
:= \Psi(e_{k+1}')\,\mathbf{U}_k
= \Psi\!\bigl(e_{k+1} \cap \hat{R}(e_k)\bigr)\,\mathbf{U}_k,
\]
where $\Psi$ embeds the surviving distinctions into the tensor algebra.
Each new event therefore contracts the admissible history by left
multiplication.  The cumulative record is the product of these restricted
increments along any finite prefix of the causal chain.

Formally, the measurable embedding $\Psi$ sends the set--theoretic
restriction to a multiplicative update in the tensor algebra.  Instead of
embedding the raw event $e_{k+1}$, we embed only the portion that survives
all prior admissibility constraints:
\[
\mathbf{E}_{k+1}
=
\Psi\!\Bigl(
e_{k+1} \cap \bigcap_{j=1}^{k} \hat{R}(e_j)
\Bigr).
\]
Writing $\mathbf{R}(e) := \Psi(\hat{R}(e))$, the cumulative record evolves by
left multiplication:
\[
\mathbf{U}_{k+1}
=
\mathbf{R}(e_{k+1})\,\mathbf{U}_k
=
\Psi\!\bigl(e_{k+1} \cap \hat{R}(e_k)\bigr)\,\mathbf{U}_k,
\qquad 0 \le k < n.
\]
Thus the tensor update is the algebraic realization of the same logical
operation performed in $E$: a new event is applied only after its outcomes
have been restricted by all earlier observations.  The universe accumulates
consistency through products of restricted increments, not by additive
evolution.

\begin{definition}[Partition of the Event Set~\cite{halmos1974naive}]
\label{def:partition}
Let $(E,\prec)$ be a locally finite partially ordered set of distinguishable
events.  A \emph{partition} of $E$ is a collection of disjoint subsets
$\{E_\alpha\}_{\alpha\in A}$ such that
\[
E = \bigcup_{\alpha\in A} E_\alpha,
\qquad
E_\alpha \cap E_\beta = \varnothing
\quad\text{for}\;\alpha\neq\beta.
\]
Each $E_\alpha$ is an informationally independent component: no event in
$E_\alpha$ refines or is refined by an event in $E_\beta$.  Correlant
events therefore lie within the same partition element, while uncorrelants
lie in distinct elements of the partition.
\end{definition}

\begin{definition}[Restriction Operator]
\label{def:restriction}
Let $(E,\prec)$ be a partially ordered set of events, and let
$e \in E$ be a newly recorded event.  The \emph{restriction operator}
\[
\hat{R}(e) : E \to E
\]
acts on the event record by removing any outcomes that are incompatible
with $e$.  For $f \in E$,
\[
\hat{R}(e)(f) =
\begin{cases}
f, & \text{if $f$ is admissible given $e$,} \\
\varnothing, & \text{otherwise.}
\end{cases}
\]
Equivalently, if $E_\alpha$ is the partition element containing $e$,
then
\[
\hat{R}(e)
\;:\; E_\alpha \mapsto E_\alpha',
\qquad
E_\alpha' = \{\,f \in E_\alpha \mid f \text{ is compatible with } e\,\}.
\]
Thus $\hat{R}(e)$ contracts the event domain by discarding outcomes that
contradict the new distinction.
\end{definition}

We now present the \emph{Causal Universe Tensor}.

\begin{proposition}[The Existence of a Causal Universe Tensor]
\label{prop:universe-tensor}
Let $(E,\prec)$ be a locally finite partially ordered set of events, and let
$\Psi : E \to \mathcal{T}(V)$ be the measurable embedding.  For each event
$e\in E$, define its admissible factor by
\[
\mathbf{F}(e) \;:=\; \Psi\!\big(\hat{R}(e)\big).
\]
Fix a finite linear extension $e_1 \prec \cdots \prec e_n$ of $(E,\prec)$ and
set $\mathbf{U}_0 := \mathbf{I}$ (the multiplicative identity in
$\mathcal{T}(V)$).  Define the left recursion
\begin{equation}
\label{eq:left-update}
\mathbf{U}_{k+1}
:= \mathbf{E}_{k+1} \,\mathbf{U}_k
= \Psi\!\big(e_{k+1} \cap \hat{R}(e_k)\big)\,\mathbf{U}_k,
\qquad 0 \le k < n,
\end{equation}
Then:
\begin{enumerate}
\item \emph{(Naturality of restriction)}
Writing
\[
  R(e) \;:=\; \Psi\!\big(\hat{R}(e)\big),
\]
the recursion \eqref{eq:left-update} can be written purely in terms of the
restriction operator as
\[
  \mathbf{U}_{k+1}
  \;=\; R(e_{k+1})\,\mathbf{U}_k.
\]
In other words, the tensor update is exactly the image under $\Psi$ of the
same discrete restriction that acts on the event record.  On ${\rm im}\,\Psi$
this is expressed by the commuting relation
\[
  R \circ \Psi \;=\; \Psi \circ \hat{R},
\]
which is the naturality of restriction.

Moreover, this restriction is the informational inverse of merging along
uncorrelant events, up to the permutation of uncorrelant factors: uncorrelant
segments commute, so the order in which they are removed or reintroduced
does not affect the admissible tensor.  Thus the relation
$R\circ\Psi=\Psi\circ\hat R$ holds modulo the natural reordering of
uncorrelant components.


\item \emph{(Causal uniqueness)} The recursion \eqref{eq:left-update} is
uniquely determined by the chosen linear extension.  Any two linear
extensions differ only by permutations of informationally independent
events (partition elements of $E$), so once the order is fixed the product
is mechanically well-defined.

\item \emph{(Independence under commuting factors)} If a subset
$S\subset\{1,\dots,n\}$ indexes events whose admissible factors pairwise
commute, $\mathbf{F}(e_i)\mathbf{F}(e_j)=\mathbf{F}(e_j)\mathbf{F}(e_i)$ for
$i,j\in S$, then any permutation of $\{\mathbf{F}(e_i)\}_{i\in S}$ leaves
$\mathbf{U}_n$ invariant under all cyclic scalar functionals (e.g., traces
of contractions).

\item \emph{(Fully commutative case)} If all admissible factors commute, then
\[
\mathbf{U}_n \;=\; \prod_{k=1}^{n} \mathbf{F}(e_k)
\]
is independent of the linear extension; the product reduces to the
order-insensitive accumulation of factors.
\end{enumerate}
\end{proposition}

Categorically, the structure underlying this result is the
naturality of a monoidal functor in the sense of
Mac~Lane~\cite{maclane1971}, with further development in
Kelly~\cite{kelly1982} and Leinster~\cite{leinster2014}.  The proof sketch
below follows this diagrammatic perspective; the fully explicit ZFC
realization appears in Appendix~\ref{app:proofs}.

\begin{proofsketch}{universe-tensor}
Let $\mathcal{E}$ be the refinement category of admissible event records,
with objects the event sequences and morphisms the refinement maps
$\widehat{R} : \mathbf{i} \to \widehat{R}(\mathbf{i})$.
Let $T(V)$ be the tensor algebra regarded as a \emph{symmetric monoidal
category} under the tensor product.

The embedding $\Phi : E \to T(V)$ extends uniquely to a monoidal functor
\[
\Phi^{(\bullet)} : \mathcal{E} \longrightarrow T(V)^{(\bullet)},
\qquad
\mathbf{i} = (i_1,\dots,i_n) \longmapsto
\bigl(\Phi(e_{i_1}),\dots,\Phi(e_{i_n})\bigr),
\]
sending refinement maps to componentwise restriction on the image.

A refinement $\widehat{R} : \mathbf{i} \to \mathbf{j}$ in $\mathcal{E}$ is a
morphism expressing that $\mathbf{j}$ is the universal solution to a finite
cone of compatibility conditions.  Under the monoidal functor
$\Phi^{(\bullet)}$, this induces a canonical morphism
\[
  \Phi^{(\bullet)}(\widehat{R}) :
  \Phi^{(\bullet)}(\mathbf{i})
  \longrightarrow
  \Phi^{(\bullet)}(\mathbf{j}).
\]
By functoriality of $\Phi^{(\bullet)}$, the diagram
\[
\begin{tikzcd}
  \mathbf{i} \ar[r, "\widehat{R}"] \ar[d, "\Phi^{(\bullet)}"'] &
  \mathbf{j} \ar[d, "\Phi^{(\bullet)}"] \\
  \Phi^{(\bullet)}(\mathbf{i})
    \ar[r, "\Phi^{(\bullet)}(\widehat{R})"'] &
  \Phi^{(\bullet)}(\mathbf{j})
\end{tikzcd}
\]
commutes.  This is the naturality condition expressing that refinement and
embedding commute.


To obtain the Causal Universe Tensor, form the \emph{monoidal accumulation}
of the embedded sequence:
\[
U(\mathbf{i})
   := \Phi(e_{i_1}) \otimes \cdots \otimes \Phi(e_{i_n}).
\]
Since $T(V)$ is symmetric monoidal, any two linear extensions of a finite
event poset differ by braidings of incomparable events, and such braidings
commute with the tensor structure.  Hence $U(\mathbf{i})$ is well defined up
to the canonical symmetry of the monoidal category.

Thus the Causal Universe Tensor is the monoidal image of a refinement diagram under a
functor preserving both the tensor product and the naturality of
refinement. 
\end{proofsketch}

The existence of the Causal Universe Tensor gives rise to the appearance of
stability in long sequences of refinement.  Because each admissible update is
not free to evolve arbitrarily, but must remain compatible with the unique
globally coherent extension of the record, deviations cannot accumulate
without bound.  Local inconsistencies are absorbed through restriction and
embedding, producing the observable effect of bounded variation in the
measurement ledger.  This structural stability is not enforced by physical
feedback or control, but by the logical necessity of coherent refinement
itself.  This gives rise to the following informational phenomenon.

\begin{phenomenon}[The Statistical Process Effect~\cite{shewhart1931}]
\label{ph:spc}
A sequence of measurements refined under admissible updates exhibits
structural stability.  Local deviations are smoothed by the unique coherent
extension enforced by restriction and embedding.  The resulting record
remains bounded, not by physical forces, but by the logical requirement of
global consistency.  This informational stability is the phenomenon known in
classical practice as statistical process control.
\end{phenomenon}




With the ordinal structure of events established, we now formalize how 
these measurements combine algebraically within a finite vector space.


\subsection{Formal Structure of Event and Universe Tensors}
\label{se:formaluniverse}

We now specify the algebraic structure of the quantities introduced above.
Let $\V$ denote a finite--dimensional real vector space representing
the independent channels of measurable quantities (e.g.\ energy, momentum,
charge).  Define the tensor algebra~\cite{halmos1958,lang2002}
\begin{equation}
\label{eq:tensoralg}
\Talg = \bigoplus_{r=0}^\infty \V^{\otimes r},
\end{equation}
whose elements are finite sums of $r$--fold tensor products over $\mathbb{R}$.
Each \emph{event tensor} $E_k$ is a member of $\Talg$
encoding the distinguishable contribution of the $k$--th event to the global
state.  We write
\begin{equation}
\label{eq:eventalgebra}
\E_k \in \Talg, \qquad
\U_n = \prod_{k=1}^{n} \E_k \in \Talg).
\end{equation}
Addition is understood componentwise in the direct sum and preserves the
ordering of indices guaranteed by the Axiom of~Order~\cite{bombelli1987,halmos1958}.  In this setting the
``universe tensor'' $\U_n$ is the cumulative history of all event tensors up to
ordinal~$n$.

\begin{definition}[Tensor Algebra~\cite{golub2013}]\label{def:tensoralgebra}
The tensor algebra on a vector space $\V$ is
\[
\Talg=\bigoplus_{r=0}^{\infty}\V^{\otimes r}
\]
with componentwise addition and associative tensor product
\end{definition}


\begin{remark}
\label{rem:posetfrontier}
Each logical event $e_k$ in the partially ordered set $(E,\prec)$
induces a tensor $\E_k = \Psi(e_k)$ in $\Talg$.
The mapping $\Psi$ translates causal structure into algebraic contribution,
ensuring that causal precedence corresponds to index ordering in $\U_n$.
\end{remark}

Because $\Talg$ is a free associative algebra, all
operations on $\U_n$ are well defined using the standard linear maps,
contractions, and bilinear forms of~$\V$.  The subsequent analysis
of variation and measurement therefore proceeds entirely within conventional
linear--operator theory.

From the definition of the Universe Tensor
\begin{equation}
U_n = \prod_{k=1}^{n} E_k,
\end{equation}
we may regard an \emph{uncorrelant} as any subset of events whose local order can be permuted without altering the global scalar invariants of \(U_n\). 
Formally, a subset \(S \subseteq \{E_1, \ldots, E_n\}\) is uncorrelant if, for every permutation \(\pi\) of \(S\),
\begin{equation}
\prod_{E_i \in S} E_i = \prod_{E_i \in S} E_{\pi(i)}.
\end{equation}
In this case, all contractions or scalar traces derived from \(U_n\) remain unchanged by reordering the elements of \(S\), even though the operator sequence itself may differ.

\begin{definition}[Commutator and Commutator Ideal~\cite{dummit2004}]
Let $\mathcal{A}$ be an algebra over a field $\mathbb{F}$ with bilinear
multiplication $(x,y)\mapsto xy$.  For $x,y\in\mathcal{A}$, the
\emph{commutator} of $x$ and $y$ is the element
\[
[x,y] \;:=\; xy - yx \;\in\; \mathcal{A}.
\]
The set of all finite $\mathbb{F}$-linear combinations of commutators,
\[
[\mathcal{A},\mathcal{A}]
\;:=\;
\left\{\,\sum_{i=1}^{m}\alpha_i [x_i,y_i]
:\alpha_i\in\mathbb{F},\;x_i,y_i\in\mathcal{A}\,\right\},
\]
is called the \emph{commutator ideal}.  It is the smallest two-sided ideal
of $\mathcal{A}$ that contains every element $xy-yx$; equivalently, it is
the smallest linear subspace of $\mathcal{A}$ closed under left and right
multiplication by arbitrary elements of $\mathcal{A}$.
\end{definition}



\begin{remark}[Algebraic Characterization of Informational Independence]
\label{rem:uncorrelant-algebraic}
Let $\Psi : E \to \mathcal{T}(V)$ be the event embedding and
$\mathbf{E}_e := \Psi(e)$.  If $S \subseteq E$ lies in distinct elements of
the partition of $E$ (Definition~\ref{def:partition}), then the admissible
increments $\{\mathbf{E}_e\}_{e \in S}$ pairwise commute.  Consequently,
any reordering of these factors within a linear extension of $(E,\prec)$
produces the same value of $\mathbf{U}_n$ under all cyclic scalar
functionals (e.g., traces of contractions).  In this algebraic sense,
informational independence corresponds exactly to order-insensitive
contribution to the invariants derived from $\mathbf{U}$.
\end{remark}

\begin{phenomenon}[Non-commutative Event Pair~\cite{hawking1973}]
\NB{Non-commutative event tensors often signal a \emph{dependency}:
one update must precede the other for the restricted outcome set to
remain consistent.  Reversing such events changes the operator state,
even though measurable scalar invariants remain the same.}

Let $V=\mathbb{R}^2$ and let event tensors act as $2\times 2$ matrices
under the usual (non-commutative) multiplication.  Define
\[
E_A=\begin{pmatrix}1 & 1\\[4pt] 0 & 1\end{pmatrix},
\qquad
E_B=\begin{pmatrix}1 & 0\\[4pt] 1 & 1\end{pmatrix}.
\]
A direct computation gives
\[
E_AE_B=\begin{pmatrix}2 & 1\\[4pt] 1 & 1\end{pmatrix}
\;\neq\;
\begin{pmatrix}1 & 1\\[4pt] 1 & 2\end{pmatrix}=E_BE_A,
\quad\text{so } [E_A,E_B]\neq 0.
\]

Thus, applying the updates in different orders leads to different operator
states.  However, cyclic scalar invariants agree:
\[
\mathrm{tr}(E_AE_B)=\mathrm{tr}(E_BE_A)=3,
\qquad
\det(E_AE_B)=\det(E_A)\det(E_B)=1.
\]
In this sense, noncommutativity affects the internal operator record but
not the measurable quantities obtained by cyclic scalar functionals.
\end{phenomenon}


\begin{phenomenon}[Independent Event Chains~\cite{langevin1911}]
\NB{This is analogous to the inertial segment of the twin paradox.  During
coasting, neither twin exchanges signals with the other, so no event on one
worldline refines or restricts events on the other.  The two chains are
informationally independent until a causal interaction occurs.}

Consider two finite event chains
\[
A_1 \prec A_2,
\qquad
B_1 \prec B_2,
\]
with no causal relation between any $A_i$ and any $B_j$.  Let their event
tensors act on $V=\mathbb{R}^2$ as
\[
E_{A1}=\begin{pmatrix}1&0\\0&0\end{pmatrix},
\quad
E_{A2}=\begin{pmatrix}0&1\\0&0\end{pmatrix},
\qquad
E_{B1}=\begin{pmatrix}0&0\\1&0\end{pmatrix},
\quad
E_{B2}=\begin{pmatrix}0&0\\0&1\end{pmatrix}.
\]

Because the $A$-events refine only the $A$-chain and the $B$-events refine
only the $B$-chain, their admissible factors commute:
\[
E_{A2}E_{B2} = E_{B2}E_{A2}.
\]
Thus, any linear extension of the partial order may place the $A$- and $B$-
events in either interleaving without changing cyclic scalar invariants.  For
example, applying the four events in the order
\[
A_1,\, A_2,\, B_1,\, B_2
\quad\text{or}\quad
A_1,\, B_1,\, A_2,\, B_2
\]
gives operator states that differ, but
\[
\mathrm{tr}(E_{A2}E_{B2}) = \mathrm{tr}(E_{B2}E_{A2}) = 1,
\qquad
\det(E_{A2}E_{B2}) = \det(E_{B2}E_{A2}) = 0.
\]

This illustrates the algebraic meaning of independence: when two event chains
are partitioned into disjoint informational domains, their admissible
increments commute.  Order affects the internal operator record but leaves
measurable cyclic scalars unchanged, exactly as in the coasting phase of the
twin paradox.
\end{phenomenon}


\begin{coda}{Achilles and the Tortoise}
\NB{For a rich treatment of this paradox, see Hofstadter~\cite{hofstadter1979}.}
Zeno’s paradox of Achilles and the tortoise~\cite{plato1996} is one of the oldest arguments
against the possibility of motion. Achilles, swift of foot, gives a tortoise
a small head start. Because the tortoise begins ahead, Achilles must first
reach the tortoise’s initial position. By that time, the tortoise has advanced
a little farther; Achilles must then reach that new position, and by the time
he arrives, the tortoise has advanced again, and so on without end. Zeno’s
conclusion is that Achilles can never overtake the tortoise, for he must
complete an infinite sequence of tasks to do so.

Formally, one can express the argument in familiar modern notation. Suppose
the tortoise begins one unit ahead. Achilles covers half the remaining
distance on his first stride, then half of what remains on the next stride,
then half again, producing the well-known geometric series
\[
1 = \frac{1}{2} + \frac{1}{4} + \frac{1}{8} + \frac{1}{16} + \cdots .
\]
More generally, one may express the same identity as
\[
1 = \sum_{n=1}^{\infty} \frac{1}{2^{\,n}}.
\]
Zeno’s reasoning is now captured in a single line: if Achilles must perform
an infinite number of sub-journeys to reach the tortoise, and if completing
infinitely many tasks requires infinite time, then Achilles never arrives.

The mathematics appears to sharpen the paradox. The right-hand side contains
infinitely many terms, and yet their sum is finite. An infinite decomposition
and a finite limit uneasily coexist. From a purely symbolic viewpoint, Zeno
is correct: the path to the finish line can be written as a countable infinity
of smaller and smaller segments. Nothing in the algebra forbids infinitely
many subdivisions of the interval.

The difficulty lies not in the mathematics, but in the hidden assumption that
every subdivision corresponds to a physically meaningful event. Zeno imagines
that the runner physically performs each of these infinitesimal subpaths, as
though each term in the series corresponds to an actual step. In reality, the
decomposition exists only on paper. It is an artifact of representation, not
an element of the physical world.

In the information gauge, motion is not defined by a continuous geometric
parameter, but by the accumulation of admissible distinctions---measurable,
irreversible updates of state. A notebook of observations does not record
symbolic halvings of distance; it records physical events that are detectable
by an instrument. Proper time is not the integral of infinitesimal steps, but
the count of such admissible distinctions.

Viewed in this light, the identity
\[
1 = \sum_{n=1}^{\infty} \frac{1}{2^{\,n}}
\]
does not imply that Achilles performs infinitely many physical actions. It
states only that a continuous model permits infinitely many subdivisions,
should one choose to write them down. The infinite chain is a mathematical
convenience, not a physical ledger.

The resolution is found in precision. Achilles does not detect every possible
subinterval of his path; no instrument possesses infinite resolving power. His
step length, his stride cadence, and the sensor that records his position
determine a finite resolution. If the act of stepping advances him by
$10^{-2}$ units, there are at most $100$ admissible distinctions in a one-unit
race. Even if the instrumentation resolves position to $10^{-6}$ units, the
notebook contains no more than one million recorded distinctions. Once this
finite notebook is reconciled, Achilles is at the finish line. The race
consumes a finite count of admissible distinctions because the physical
process does not instantiate an actual infinity of subevents.

Zeno’s paradox relies on treating every symbolic refinement of the interval
as physically real. The information gauge rejects that assumption. A
measurement records only what can be stably distinguished. Achilles’s
``infinite'' steps are not steps at all; they are possible refinements of a
mathematical model. Precision is the gatekeeper. The paradox dissolves when
we recall that Achilles’s motion is measured, not imagined, and that every
measurement has finite resolution. Refinement does not create motion; it
reveals it.

\end{coda}

