\chapter{The Calculus of Dynamics}
\label{chap:dynamics}

In the previous chapter, motion was described entirely as a sequence of
admissible distinctions---a finite notebook of observable updates. No
geometry, metric, or continuum was assumed. Refinement revealed additional
events, but the history of any physical process remained a countable record
that could be reconciled into a globally coherent ledger.

This chapter introduces dynamics in the same spirit. By ``dynamics'' we do
not mean a force law or a geometric trajectory. We mean the rule that
selects, from all admissible histories, those that are physically possible.
The key observation is that a physical history cannot contain unexplained
motion. Any segment of a worldline must be consistent with the measurements
that precede and follow it. When a history can be refined without altering
its predictions at the recorded events, the refined history contains no
additional information. In this sense, the physically admissible refinement
is the one that introduces no new distinctions beyond those required by the
data.

This principle has a classical name. In the continuum limit, the requirement
that refinements add no “hidden motion” is precisely the Euler–Lagrange
condition: an admissible trajectory introduces no superfluous curvature beyond
that certified by observed events \cite{ciarlet1978,courant1953,lanczos1970}.
A trajectory of least informational content is a trajectory of least action,
in the classical sense of Maupertuis, Euler, Lagrange, Hamilton, and their
modern successors \cite{demaupertuis1744,euler1744,goldstein2002,hamilton1834,lagrange1788}.
In the calculus of dynamics, smooth solutions arise not from
geometry but from the demand that no further admissible distinctions can be
discovered between measurements. The spline that leaves nothing to correct is
the one nature selects.


The remainder of this chapter develops this idea formally. Starting from a
finite set of measurements, we construct the weak form of the problem and
show that the unique refinement consistent with all observed distinctions is
the cubic spline. Its extremality in the continuum reproduces the
Euler--Lagrange equations familiar from classical mechanics and field theory.
Dynamics are not imposed at the outset; they emerge as the limit in which
refinement ceases to yield new information.

\begin{example}[Minimizing Variations~\cite{courant1953}]
\NB{For a comprehensive treatment of the calculus of variations, see Brenner and Scott~\cite{brenner2008}
and Courant and Hilbert~\cite{courant1953}.}

We consider the functional
\[
J[x] = \int_a^b f\bigl(t,x(t),\dot{x}(t)\bigr)\, dt,
\]
where $x$ is a twice continuously differentiable function with fixed
endpoints $x(a)=x_a$ and $x(b)=x_b$. Let $\eta(t)$ be an admissible
perturbation with $\eta(a)=\eta(b)=0$, and define the variation
\[
x_\varepsilon(t) = x(t) + \varepsilon\,\eta(t), \qquad \varepsilon\in\mathbb{R}.
\]
The directional derivative of $J$ at $x$ in the direction $\eta$ is
\[
\delta J[x;\eta]
  = \left.\frac{d}{d\varepsilon} J[x_\varepsilon]\right|_{\varepsilon=0}
  = \left.\frac{d}{d\varepsilon}
       \int_a^b f\bigl(t, x_\varepsilon(t), \dot{x}_\varepsilon(t)\bigr)\, dt
    \right|_{\varepsilon=0}.
\]
Since the integration limits do not depend on $\varepsilon$, the
derivative may be moved inside:
\[
\delta J[x;\eta]
  = \int_a^b
      \left.\frac{\partial}{\partial\varepsilon}
      f\bigl(t, x_\varepsilon(t), \dot{x}_\varepsilon(t)\bigr)
      \right|_{\varepsilon=0} dt.
\]
By the chain rule,
\[
\frac{\partial}{\partial\varepsilon}
 f\bigl(t, x_\varepsilon(t), \dot{x}_\varepsilon(t)\bigr)
 = f_x(t,x(t),\dot{x}(t))\,\eta(t)
 + f_{\dot{x}}(t,x(t),\dot{x}(t))\,\dot{\eta}(t).
\]
Thus
\[
\delta J[x;\eta]
  = \int_a^b \Bigl(
      f_x(t,x,\dot{x})\,\eta(t)
      + f_{\dot{x}}(t,x,\dot{x})\,\dot{\eta}(t)
    \Bigr)\, dt.
\]
Integrate the second term by parts:
\[
\int_a^b f_{\dot{x}}\,\dot{\eta}\, dt
  = \bigl[f_{\dot{x}}\eta\bigr]_{a}^{b}
    - \int_a^b \frac{d}{dt}\bigl(f_{\dot{x}}\bigr)\,\eta(t)\, dt.
\]
Because $\eta(a)=\eta(b)=0$, the boundary term vanishes. Therefore
\[
\delta J[x;\eta]
  = \int_a^b
       \left(
         f_x - \frac{d}{dt} f_{\dot{x}}
       \right)\eta(t)\, dt.
\]
If $x$ is a stationary point of $J$, then $\delta J[x;\eta]=0$ for all
admissible $\eta$. The fundamental lemma of the calculus of variations
implies
\[
f_x(t,x,\dot{x}) - \frac{d}{dt}f_{\dot{x}}(t,x,\dot{x}) = 0,
\]
for all $t\in(a,b)$. This is the Euler--Lagrange equation, more commonly
represented as
\begin{equation}
\frac{\partial f}{\partial x} = \frac{d}{dt}\frac{\partial f}{\partial\dot{x}}.
\end{equation}

This derivation demonstrates that the Euler--Lagrange equation selects the
trajectory with no first-order change under admissible perturbations. No
hidden motion can be inserted without altering the notebook. The path is
stationary in its informational curvature.
\end{example}

\section{Emergent Dynamics}
\label{sec:emergent-dynamics}

In the discrete setting, the Causal Universe Tensor assigns a finite
informational weight to every admissible history.  Refinement increases
this weight only when new distinctions are recorded.  Any replacement of
an admissible history by one containing additional, unobserved structure
violates Axiom~\ref{ax:boltzmann}.  Consequently, dynamics is not an
independent physical postulate.  It is the unique continuous shadow of
informational extremality: the smooth curve is simply the history for
which no further admissible distinctions can be revealed.

In the discrete domain, \emph{anchor points} are the only places where the universe has
committed to a specific value.  Between anchors the record is silent: the
data permit many possible continuations, but most would introduce
unobserved structure.  Any admissible configuration must therefore agree
at the anchor points and remain free of additional distinguishable
features in between.  The role of the anchors is not geometric; it is
informational.  They fix the admissible boundary data against which all
variations are tested.  A candidate variation that disagrees at an anchor
is rejected immediately, because it contradicts an established event.  A
variation that agrees at the anchors but inserts additional oscillation,
curvature, or ``hidden motion'' is rejected by
Axiom~\ref{ax:boltzmann}, because those features would have produced
additional anchors that do not appear in the record.

\begin{definition}[Anchor Points~\cite{deboor1978}]
\label{def:anchors}
A finite set of \emph{anchor points} is the collection of measured
events at which admissible configurations must agree.  Two candidate
histories $\psi$ and $\phi$ are said to share the same anchors if they
record identical distinguishable values at those events. Axiom~\ref{ax:ockham}
requires that any refinement of a history preserve agreement
on the anchors: no admissible configuration may contradict an observed
event.
\end{definition}

In the discrete setting, reciprocity arises from a simple counting fact.
A refinement of $\psi$ by a test configuration $\phi$ is admissible only
when the resulting history contains no additional distinguishable events.
If $\phi$ were to introduce extra curvature, oscillation, or ``hidden
motion,'' the refinement would increase the causal count and violate
Axiom~\ref{ax:boltzmann}.  The reciprocity pairing
$\psi^{\!*}\mathcal{L}\phi$ measures this change: it evaluates whether
$\phi$ is informationally neutral relative to $\psi$.

Crucially, the dual $\psi^{\!*}$ is not a geometric adjoint; it is the
reflection of $\psi$ in the informational algebra.  It answers the
question: \emph{If $\psi$ is perturbed by $\phi$, does the universe record
new distinguishable structure?}  If the reciprocity pairing vanishes for
all admissible $\phi$ that share the anchors, then $\psi$ is extremal.
Any remaining variation would imply new recorded events, and therefore be
inadmissible.

\begin{definition}[Reciprocity Map]
\label{def:reciprocity}
\NB{In geometric settings equipped with a metric or inner product, the
reciprocity map reduces to the familiar adjoint or complex conjugate, and
the operation $\psi\mapsto\psi^{\!*}$ is often interpreted as a covariant or
contravariant dual.  No such geometric structure is assumed here.  The
reciprocity dual is defined purely informationally, as the configuration that
symmetrizes the causal pairing.  It should not be confused with metric
adjoints that appear in geometric representation theory, such as the Dirac
adjoint of a spinor or the dual of a Weyl field.  Those constructions depend on
Lorentz symmetry, Clifford algebras, and an invariant bilinear form; none of
these structures are present at the informational level.}


Let $\psi$ be an admissible configuration and let $\phi$ be a test variation
that agrees with $\psi$ at the anchor points.  The \emph{reciprocity map} is
the linear evaluation
\[
\langle \psi,\phi\rangle_{\mathcal{L}}
  := \psi^{\!*}\,\mathcal{L}\,\phi,
\]
where $\mathcal{L}$ counts distinguishable causal increments.  A configuration
$\chi$ is called a \emph{reciprocity dual} of $\psi$ if it satisfies
\[
\langle \psi,\phi\rangle_{\mathcal{L}}
  = \langle \phi,\chi\rangle_{\mathcal{L}}
  \quad\text{for all test variations }\phi.
\]
When it exists, such a $\chi$ is denoted by $\psi^{\!*}$.
The reciprocity dual encodes the informational response of $\psi$ to an
infinitesimal variation $\phi$ without assuming any differential structure.
\end{definition}

\begin{proposition}[The Uniqueness of the Reciprocity Dual]
\label{prop:reciprocity-unique}
Assume the causal pairing $\langle \cdot,\cdot\rangle_{\mathcal{L}}$ is
nondegenerate in the second slot: if
\[
\langle \phi,\chi\rangle_{\mathcal{L}} = 0
\quad\text{for all test variations }\phi,
\]
then $\chi$ is the trivial (null) configuration.  If $\chi_1$ and $\chi_2$
are both reciprocity duals of the same configuration $\psi$, then
$\chi_1 = \chi_2$.  In particular, whenever a reciprocity dual exists, it is
unique.
\end{proposition}

\begin{proofsketch}{reciprocity-unique}
Let $\chi_1$ and $\chi_2$ be reciprocity duals of $\psi$.  By definition,
\[
\langle \psi,\phi\rangle_{\mathcal{L}}
  = \langle \phi,\chi_1\rangle_{\mathcal{L}}
  = \langle \phi,\chi_2\rangle_{\mathcal{L}}
\quad\text{for all test variations }\phi.
\]
Subtracting the two expressions gives
\[
\langle \phi,\chi_1 - \chi_2\rangle_{\mathcal{L}} = 0
\quad\text{for all }\phi.
\]
By nondegeneracy in the second slot, this implies $\chi_1 - \chi_2$ is the
null configuration, hence $\chi_1 = \chi_2$.  Thus any reciprocity dual, if
it exists, is unique.
\end{proofsketch}


In the continuum shadow, the reciprocity pairing becomes the usual weak
inner product of variational calculus~\cite{brenner2008,evans2010}.  Integration by parts moves the
variation from $\psi$ onto the test functions, producing natural boundary
terms determined by the anchors.  The condition
\[
\langle \psi,\phi\rangle_{\mathcal{L}}
  = \langle \phi,\psi\rangle_{\mathcal{L}}
\]
is then the classical reciprocity of the Euler--Lagrange operator: the
dynamics are self-adjoint under the informational measure.  This equality
holds not because symmetry is assumed, but because any antisymmetric
contribution would encode unrecorded distinctions and be eliminated by
Axiom~\ref{ax:boltzmann}.



\subsection{Weak Formulation on Space--Time}
\label{sec:weak-formulation}

Let $\psi$ be an admissible configuration consistent with a fixed set of
event anchors, and let $\phi$ be any test configuration that agrees with
$\psi$ at those anchors.  Replacing $\psi$ by $\phi$ is permissible only if
it does not reduce causal consistency.  In the discrete algebra this means
that $\psi$ introduces no superfluous refinements relative to $\phi$; any
additional curvature, oscillation, or ``hidden motion’’ would imply
unrecorded events and thus be inadmissible.

In the dense limit of refinement, this constraint appears as a weak
relation
\begin{equation}
  \psi^{\!*}\,\mathcal{L}\,\psi
  \;\leq\;
  \psi^{\!*}\,\mathcal{L}\,\phi,
  \label{eq:weak-form}
\end{equation}
where $\mathcal{L}$ is the informational count of distinguishable
increments, and $\psi^{\!*}$ denotes its reciprocity dual.  The weak
inequality asserts that $\psi$ is extremal among all admissible
perturbations $\phi$.  No differential operators are assumed: the weak
form arises because refinement limits the class of permissible discrete
variations.

Completing this refinement yields the continuous counterpart of
\eqref{eq:weak-form}.  Integration by parts shifts variations from $\psi$
onto the test functions, producing natural boundary conditions and a weak
Euler--Lagrange statement.  The continuum calculus therefore does not
describe an independently assumed physical law; it is the smooth completion
of informational minimality on the discrete domain.

\subsection{Reciprocity and the Adjoint Map}
\label{sec:reciprocity-adjoint}

The weak extremality relation~\eqref{eq:weak-form} compares an admissible
configuration $\psi$ against a test configuration $\phi$ that shares the
same event anchors.  In the discrete domain, replacing $\psi$ by $\phi$
means refining the event record: only those local changes that introduce
new, distinguishable curvature would alter the admissible history.  Any
such change must correspond to additional recorded events; if none are
present, the refinement is informationally neutral.  Thus $\phi$ is an
admissible variation of $\psi$ precisely when it agrees at the anchors and
introduces no distinctions beyond those already encoded in $\psi$.  The
weak extremality condition~\eqref{eq:weak-form} is the continuous shadow of
this discrete refinement rule.


The weak comparison between $\psi$ and $\phi$ admits a natural dual
representation.  For any admissible configuration $\psi$, there exists a
\emph{reciprocity map} $\psi^{\!*}$ such that the informational pairing
\begin{equation}
\psi^{\!*}\,\mathcal{L}\,\phi
\end{equation}
measures the change in distinguishability that would result from locally
replacing $\psi$ by $\phi$ between the anchors.  Intuitively, $\psi^{\!*}$
captures the ``shadow'' of $\psi$ when viewed from the perspective of
informational minimality: components of $\phi$ that would introduce new,
unrecorded distinctions are suppressed by the adjoint action, while
components that are informationally neutral remain.  In the dense limit,
this pairing becomes the standard weak inner product of variational
calculus.

Because admissible configurations cannot contain hidden structure, the
reciprocity map annihilates variations that are invisible at the event
anchors.  If $\phi$ and $\psi$ agree at the anchors and differ only by an
undetectable perturbation, then refining the event record yields no new
distinctions, and the informational pairing remains unchanged:
\[
\psi^{\!*}\,\mathcal{L}\,\phi
\;=\;
\psi^{\!*}\,\mathcal{L}\,\psi.
\]
This equality is precisely the weak relation~\eqref{eq:weak-form}.  In this
sense, $\psi^{\!*}$ enforces closure: the extremal configuration carries no
latent curvature that would be revealed by further refinement.


The ``variation'' of $\psi$ is therefore not a differential operation but a
refinement of the causal record consistent with the event anchors.  The
reciprocity map acts as the dual constraint, suppressing any component of
that refinement which would introduce unrecorded distinctions.  Taken
together, admissible refinements and their reciprocity dual generate the weak
Euler--Lagrange structure entirely within the discrete domain, without
assuming differentiability or a continuum of states.


In this way, the reciprocity map ensures that any admissible refinement of
$\psi$ corresponds to an interpolant $f(\psi)$ that introduces no new
distinguishable structure.  As refinement becomes dense, all such
interpolants converge to the same smooth closure $\Psi$.  Since the event
record defines a finite labeled partition of the causal domain, $\Psi$
preserves anchor order and is injective on each partition element.  Its
inverse $\Psi^{-1}$ therefore recovers exactly the original discrete record:
\begin{equation}
  f(\psi) \;\longrightarrow\; \Psi^{-1}.
\end{equation}
Thus the interpolant and its smooth limit are informationally equivalent
representations of the same causal structure.


\subsection{Dense Limit and Euler--Lagrange Closure}
\label{sec:dense-limit}

In the present framework no differentiability is assumed.  The weak
extremality relation~\eqref{eq:weak-form} is defined entirely in the discrete
domain, where each term counts distinguishable causal increments.  A
``variation'' of $\psi$ is therefore not a differential operator but a
refinement of the event record that leaves the anchors unchanged.

In the discrete domain, such refinements appear as finite differences: each
admissible update replaces a segment of the causal history by one with
strictly greater resolution.  Because informational minimality forbids
unobserved curvature, every admissible refinement corresponds to a
piecewise-linear or piecewise-polynomial interpolant that agrees with $\psi$
on the anchors and introduces no new distinguishable structure.  As
refinements become arbitrarily dense, the finite differences form a Cauchy
sequence in the space of admissible interpolants, and their limit is the
unique smooth closure $\Psi$ established in the previous subsection.

Applying the reciprocity pairing to successive refinements yields the
discrete extremality condition: no admissible finite difference can reduce
the informational measure $\mathcal{L}$.  In the dense limit, the weak
relation~\eqref{eq:weak-form} becomes the standard variational identity of
Euler--Lagrange calculus, obtained entirely from finite differences.  The
weak derivative enters only as the completion of refinement; it is not
assumed \emph{a priori}.

When the causal grid is refined, informational minimality forces cubic
continuity at each event anchor: jumps in slope or curvature would constitute
new observable events and are therefore inadmissible.  In the dense limit,
the discrete extremal coincides with the classical Euler--Lagrange closure.
This structure is summarized in the following proposition.

\begin{proposition}[The Spline Condition of Information]
\label{prop:fourth-derivative}
Let $\psi$ be an admissible configuration with smooth closure $\Psi$.  If no
admissible refinement reduces the informational measure $\mathcal{L}$, then
$\Psi$ is $C^{2}$ and satisfies
\begin{equation}
\Psi^{(4)} = 0.
\end{equation}
\end{proposition}

\begin{proofsketch}{fourth-derivative}
Between anchors, $\Psi$ must be polynomial, since any additional inflection
would imply unrecorded structure.  Polynomials of degree greater than three
contain latent turning points and are therefore excluded.  Hence each segment
is cubic.  At the anchors, the interpolants must glue with $C^2$ continuity:
jumps in slope or curvature would constitute new observable events.  As the
grid of anchors is refined, the third derivative $\Psi'''$ must be constant
on every shrinking interval.  In the dense limit that interval has zero
measure, so $\Psi'''$ is constant everywhere.  A constant third derivative
implies $\Psi^{(4)} = 0$.  Thus the smooth closure of any informationally
extremal configuration satisfies the Euler--Lagrange condition.
\end{proofsketch}

Proposition~\ref{prop:fourth-derivative} shows that the Euler--Lagrange
equation is not postulated.  It is the continuous shadow of discrete
informational extremality.  Finite differences do not approximate the
differential equation; they \emph{generate} it.  The unique admissible smooth
representative is cubic on each partition element, $C^2$ at the event
anchors, and satisfies $\Psi^{(4)}=0$ everywhere.  Smooth calculus appears
solely as the completion of refinement in the discrete causal record.


\begin{phenomenon}[Repeatability of Invisible Motion~\cite{bacon1620}]
\label{ex:repeatability}
Consider two independent observers, $A$ and $B$, who record the motion of a
particle between the same event anchors $x_i \prec x_{i+1}$.  Each observer
has finite resolution: any acceleration or inflection large enough to be
distinguishable produces a new event.  Both refine their instruments until
no further events are detected on the interval.

If hidden curvature existed between the anchors, further refinement would
create additional distinguishable records.  The absence of such records
forces each observer to recover the same polynomial of minimal degree.  Thus
both obtain a cubic patch on the interval.

Now let $A$ and $B$ exchange data and perform a joint refinement on a finer
grid.  Any disagreement in value, slope, or bending moment at a shared
anchor would itself generate an observable event.  To avoid contradiction,
the cubic patches must glue together with continuous $U$, $U'$, and $U''$.
In the dense refinement limit, the piecewise constant third derivative
converges to a continuous function whose integral vanishes on every
shrinking interval, yielding
\[
U^{(4)} = 0.
\]

Thus repeatability demands the Euler--Lagrange closure: if two observers can
refine their measurements indefinitely without producing new events, their
reconstructions must converge to the same cubic extremal.  Smooth dynamics
are therefore the unique histories that leave no trace.
\end{phenomenon}



\section{The Law of Spline Sufficiency}
\label{sec:spline-sufficiency}

The preceding analysis shows that every admissible refinement of the event
record corresponds to a piecewise-cubic interpolant that preserves the event
anchors and introduces no new distinguishable structure.  In the dense
limit, these interpolants converge to a unique smooth closure $\Psi$ that is
$\mathcal{C}^{2}$ and satisfies $\Psi^{(4)} = 0$.  The discrete causal record
and its smooth completion are therefore informationally equivalent
representations of the same history.

\begin{law}[The Law of Spline Sufficiency]
\label{law:spline-sufficiency}
Let $\psi$ be any finite, non-contradictory record of admissible events.
There exists a unique continuous completion $\Psi$ such that:
\begin{enumerate}
\item $\Psi$ agrees with $\psi$ at every event anchor,
\item $\Psi$ is piecewise cubic and $\mathcal{C}^{2}$ on its domain,
\item $\Psi$ introduces no new distinguishable structure beyond $\psi$, and
\item $\Psi$ satisfies the Euler--Lagrange closure $\Psi^{(4)} = 0$.
\end{enumerate}
The cubic spline is therefore \emph{sufficient} to represent all admissible
distinctions in the data: no higher-order model encodes additional
information available to measurement.
\end{law}

The Law of Spline Sufficiency justifies the use of Galerkin methods~\cite{galerkin1915} in this
framework.  By choosing the functional 
\begin{equation}
\mathcal{J}[\Psi] = \int (\Psi'')^2
\,dx
\end{equation} as a measure of curvature, the Galerkin extremal selects the simplest
admissible interpolant consistent with the event anchors.  Because every
sequence of admissible refinements converges to the unique $\mathcal{C}^{2}$
cubic closure, the Galerkin solution coincides with the informationally
extremal configuration.  No additional degrees of freedom are required.

In this sense, spline sufficiency provides the logical bridge between
discrete measurement and continuous dynamics:
\[
\text{discrete measurement}
\;\xrightarrow{\;\text{spline sufficiency}\;}\;
\Psi
\;\xrightarrow{\;\text{closure}\;}\;
\Psi^{(4)} = 0.
\]
Finite differences do not approximate the Euler--Lagrange equation; they
\emph{generate} it.  Smooth calculus enters only as the completion of
refinement in the causal record, not as an assumed geometric primitive.

\subsection{Minimality}

The Law of Spline Sufficiency guarantees that admissible histories are those
that interpolate between refinements with minimal curvature.  This law does
not merely describe smoothness; it constrains how influence can propagate
through the ledger.  A localized refinement cannot remain localized under
extension.  Its effect must be distributed across every admissible
continuation consistent with global coherence.

As the causal record is extended outward from an event, the frontier of
admissible continuations grows combinatorially.  The same finite refinement
budget must be shared across an increasing number of admissible degrees of
freedom.  The effect of any single refinement therefore weakens with
informational separation, not because of dissipation, but because of
accounting.

While any minimality can be enforced, measurement tends to prefer minimization
in the $\mathcal{L}^2$-norm.

\begin{phenomenon}[The Inverse Square Effect]
\label{ph:inverse-square}

\textbf{Statement.}
The influence of a refinement event decreases as the inverse square of the
informational separation.  This scaling is not postulated; it is forced by
the geometry of admissible splines.

\textbf{Mechanism.}
By the Law of Spline Sufficiency, admissible continuations of the causal
ledger are the minimal curvature interpolants consistent with boundary
anchors.  A single refinement event acts as a localized constraint on the
spline.  As the distance from that constraint increases, the number of
distinct admissible continuations grows with the surface measure of the
surrounding causal sphere.

In three admissible dimensions, this measure scales as $4\pi r^2$.  The
influence of a fixed refinement budget must therefore be distributed across a
quadratically growing frontier.  The admissible effect per refinement falls
as
\[
I(r) \propto \frac{1}{r^2}.
\]

\textbf{Interpretation.}
This is not a force law.  It is a bookkeeping law.  The ledger cannot assign a
fixed refinement cost to an expanding set of admissible continuations without
diluting its effect.

The inverse-square behavior of gravitation, radiation, and flux is therefore
the smooth shadow of the combinatorial growth of admissible splines.

\end{phenomenon}


\section{Galerkin Methods}
\label{sec:galerkin}
\NB{This argument applies the Law of Spline Sufficiency.  We do not assume
that Euler--Lagrange dynamics exist \emph{a priori}.  Rather, we show that
if the data admit a smooth completion, then a cubic spline exists which
reproduces the Euler--Lagrange solution to arbitrary accuracy.  In this
sense, observing a spline is sufficient to infer Euler--Lagrange dynamics:
the differential equation models the behavior only insofar as the data
allow it, and no additional geometric or differentiable structure is
assumed.}


The Law of Spline Sufficiency establishes that cubic splines contain all
admissible distinguishable structure.  In this section we assume the
existence of a smooth Euler--Lagrange solution and show that a Galerkin
projection onto a spline basis produces a sequence of spline functions that
converges to it.  This suffices to justify the use of splines as the
representatives of continuous dynamics: if Euler--Lagrange motion exists,
Galerkin refinement will recover it to arbitrary accuracy.

\subsection{Galerkin Projection onto a Spline Basis}

Let $\Psi$ be the smooth solution to an Euler--Lagrange boundary value
problem.  Choose a finite spline basis $\{\varphi_k\}$ that satisfies the
boundary constraints and let
\[
  \Psi_n(x) = \sum_{k=1}^{n} a_k\,\varphi_k(x)
\]
be the Galerkin projection of $\Psi$ onto this space.  The coefficients
$a_k$ are chosen so that the residual of the Euler--Lagrange equation is
orthogonal to the spline basis:
\begin{equation}
  \int \Psi_n''(x)\,\varphi_k''(x)\,dx = \int \Psi''(x)\,\varphi_k''(x)\,dx,
  \qquad k = 1,\dots,n.
  \label{eq:galerkin-projection}
\end{equation}
This is the standard spline Galerkin formulation \cite{ciarlet1978,brenner2008}:
the weak form enforces the Euler--Lagrange condition in the finite
dimensional subspace spanned by the splines.

Solving \eqref{eq:galerkin-projection} yields a unique spline $\Psi_n$ that
agrees with the smooth solution at all knot points and is $\mathcal{C}^2$ on
the domain.  No higher-order degrees of freedom are necessary; the curvature
functional ensures that splines are the minimal weak extremals.

\subsection{Convergence of the Galerkin Sequence}

By the Weierstrass Approximation Theorem, cubic splines form a dense
subspace of continuous functions on a compact interval.  As the mesh is
refined and more basis functions are added, the sequence $\{\Psi_n\}$
converges uniformly to $\Psi$:
\[
  \Psi_n \;\xrightarrow[n\to\infty]{}\; \Psi.
\]
Because the Euler--Lagrange operator is continuous in the weak topology,
convergence of $\Psi_n$ implies convergence of all weak derivatives:
\[
  \Psi_n'' \;\xrightarrow[n\to\infty]{}\; \Psi''.
\]
Thus the Galerkin sequence yields arbitrarily good spline approximations of
the Euler--Lagrange solution.  In particular, $\Psi_n$ satisfies
\[
  \Psi_n^{(4)} = 0
\]
on each spline element, up to a boundary residual that vanishes as the mesh
is refined.

\begin{corollary}
If a smooth Euler--Lagrange solution $\Psi$ exists, a sequence of cubic
splines $\{\Psi_n\}$ constructed by Galerkin projection converges uniformly
to $\Psi$.  Since cubic splines represent all admissible distinguishable
structure, observing a spline solution is sufficient to infer the underlying
Euler--Lagrange dynamics.
\end{corollary}

In summary:
\[
  \Psi
  \xrightarrow{\text{Galerkin projection}}
  \Psi_n
  \xrightarrow[n\to\infty]{\text{Weierstrass}}
  \Psi,
\]
so splines not only represent all admissible distinctions, but converge to
the unique extremal of the Euler--Lagrange equation whenever one exists.  The
Galerkin method therefore completes the argument of spline sufficiency in the
continuum: if continuous dynamics exist, spline solutions will recover them
to arbitrary accuracy.

The Galerkin refinement therefore recovers smooth calculus without assuming
infinitesimal increments or geometric primitives.  The classical paradox of
the fluxion may now be revisited in this light.


\begin{phenomenon}[Fluxions~\cite{berkeley1734,newton1687}]
\label{te:newtonian-ghost}

\NB{The classical paradox of the fluxion treats an infinitesimal $\mathrm{d}t$
as a quantity that is neither zero nor nonzero.  In the present framework,
the limit is defined without invoking infinitesimals: smooth structure appears
only as the unique completion of finite distinctions.}

In the 18th century, Bishop Berkeley criticized Newton's calculus of
\emph{fluxions} ($\dot{x},\dot{y}$) for relying on quantities that vanish in
one step of a proof and are treated as nonzero in the preceding step.  If
$\dot{x}$ and $\dot{y}$ are the ghost-like ``increments'' of position, the
question arises: \emph{How can a finite, observable change emerge from the
vanishing difference of infinitesimal quantities?}

In the causal accounting used here, this is not a paradox of quantity but a
limitation of informational resolution.  The fluxion
\[
  \dot{x} = \frac{\Delta x}{\Delta t}
  \]
  is a ratio of two sequentially recorded distinctions: the number of spatial
  ticks $\Delta x$ versus the number of temporal ticks $\Delta t$ between two
  anchors.  Both are finite, integer-valued measurements.

  The classical paradox appears only when $\Delta t \to 0$ is interpreted as a
  transition through a nonphysical intermediate state.  In the present
  framework, no such state is required.  The smooth completion $\Psi$
  constructed in the dense limit satisfies $\Psi^{(4)}=0$ and is the unique
  curvature-free extension of the data.  As the anchor spacing shrinks, the
  ratio $\frac{\Delta x}{\Delta t}$ converges to the unique $\mathcal{C}^{2}$
  slope $\Psi'$ of the cubic interpolant determined by the neighboring anchors.

  No ghost-like infinitesimal is invoked.  The derivative is the continuous
  shadow of finite bookkeeping: the single value required to prevent the
  appearance of new, unrecorded events as resolution increases.  Smooth
  calculus arises not by manipulating vanished quantities, but as the unique
  function consistent with every refinement of the observable record.

  \end{phenomenon}

\subsection{The Physical Impossibility of Infinite Refinement}
\label{sec:impossibility-spline-law}

A law of spline necessity \emph{would} describe the continuous limit of an
ideal refinement process much like the law of spline sufficiency. In such a limit, 
where arbitrarily fine
distinguishable refinements are permitted, the unique smooth closure
compatible with informational minimality would necessarily coincide with a
cubic spline satisfying $\Psi^{(4)} = 0$ between all anchors. This behavior
would characterize the limiting object toward which all admissible
refinements converge.

However, this description is inherently conditional. The existence of such a
law requires access to refinements at arbitrarily small scales. In the
informational setting developed here, no such refinement process exists:
every record admits only finitely many distinguishable refinements. As a
consequence, the continuum limit in which an exact spline law \emph{would}
hold is never attainable. The law does not fail; rather, it is not a
law of the finite world.

\NB{The idea of a spline necessity law is meaningful only as a limiting
construct. It does not apply to any finite record because no observational
process can instantiate the infinite refinement depth the law presupposes
(Axiom~\ref{ax:planck}).}

This observation motivates an approximate interpretation. Although an exact
law cannot hold, the Galerkin convergence results of
Section~\ref{subsec:approx-spline-necessity} imply that finite-dimensional
closures can be made arbitrarily close to the ideal spline closure. Thus,
while a spline necessity law describes an unattainable limit, its behavior
is still relevant: finite informational models approach that limit as their
resolution increases. The continuum spline is therefore best understood as
the \emph{attractor} of refinement-compatible approximations, not as a law
governing finite observational structure.

\begin{definition}[Attractor~\cite{mandelbrot1982}]
An attractor is a set of configurations toward which the admissible
states of a system asymptotically converge under iteration of the update
rule.  Once the refinement enters the neighborhood of the attractor,
subsequent refinements remain confined to it.  The attractor represents the
stable informational pattern that balances the system's internal stress and
the constraints of the refinement process.  
\end{definition}


\subsection{Indistinguishability of Approximate and Ideal Spline Closures}
\label{subsec:indistinguishability-spline}

A law of spline necessity would characterize the exact continuous limit of an
ideal refinement process. In practice, however, only approximate spline closures
exist, obtained through refinement-compatible approximations such as Galerkin
methods. This raises a natural question: could any measurement distinguish between
an approximate closure and the ideal spline attractor it converges toward?

The answer is no. Under the axioms of event selection, refinement compatibility,
and informational minimality, no admissible measurement can separate the two. Any
measurement capable of distinguishing an approximate spline from the ideal one
would require detecting differences at scales finer than the minimum resolvable
distinction allowed by the record. Such a measurement would necessarily violate
the axioms by introducing new refinements below the Planck scale.

\NB{There exists no admissible observational procedure, consistent with the axioms
of measurement, that can differentiate between the approximate spline obtained at a
finite refinement scale and the ideal spline that would appear in the continuum
limit. Any attempt to do so requires forbidden refinements and is therefore
inadmissible.}

Let $(\Psi_N)$ be a sequence of refinement-compatible approximations converging
toward an ideal spline $\Psi$ in the sense of
Section~\ref{subsec:approx-spline-necessity}. For any fixed resolution scale
permitted by the record, there exists $N$ such that
\[
\|\Psi_N - \Psi\| < \delta,
\]
where $\delta$ is the smallest distinguishable refinement allowed by the axioms.
Because no measurement can detect variation smaller than $\delta$, the outputs of
$\Psi_N$ and $\Psi$ are observationally identical. To distinguish them would
require a measurement refining the domain below $\delta$, which the axioms forbid.

\begin{definition}[Observational Indistinguishability~\cite{nyquist1928}]
\label{def:indistinguishability}
A finite-dimensional closure $\Psi_N$ is observationally indistinguishable from the
ideal spline closure $\Psi$ if, for the minimum refinement scale $\delta$ of the
record,
\[
|\Psi_N(x) - \Psi(x)| < \delta
\quad\text{for all admissible measurement points } x.
\]
No admissible measurement can detect any discrepancy of magnitude less than
$\delta$.
\end{definition}

\subsection{Indistinguishability of Infinite Refinement}
\label{subsec:pigeonhole-indistinguishability}

Axiom~\ref{ax:kolmogorov} states that every measurement produces a symbol from a finite or
countable alphabet and that all refinements are bounded below by a minimum
distinguishable scale~$\delta > 0$. A measurement record is therefore a finite
string over an alphabet whose effective base is determined by the refinement
scale. In this setting, the pigeonhole principle implies that only finitely many
distinct measurement outcomes are possible at resolution~$\delta$.

Let $\Psi$ be an ideal closure that would be obtained in an infinite-refinement
limit, and let $\Psi_\delta$ be any finite-resolution approximation consistent
with the refinement scale~$\delta$. If $\Psi$ and $\Psi_\delta$ differ only on
sub-\,$\delta$ scales, then no admissible measurement can distinguish them.
Their projections into the measurement alphabet coincide, and therefore they
produce the same finite string of observations.

\begin{proposition}[Pigeonhole Indistinguishability of Infinite Refinement~\cite{dirichlet1834,hardy1938}]
\label{prop:pigeonhole}
Let $\Sigma_\delta$ be the finite set of symbols distinguishable at refinement
scale~$\delta$, and let $\mathcal{M}$ denote the measurement map
\[
\mathcal{M} : \{\text{closures}\} \to \Sigma_\delta^*.
\]
If two closures $\Psi$ and $\Phi$ differ only at scales smaller than~$\delta$,
then
\[
\mathcal{M}(\Psi) = \mathcal{M}(\Phi).
\]
In particular, any infinitely refined closure is observationally
indistinguishable from a sufficiently refined finite approximation.
\end{proposition}

\begin{proofsketch}{pigeonhole}
This result follows directly from the pigeonhole principle. A finite
measurement alphabet cannot encode distinctions below the minimal refinement
scale~$\delta$. Once two closures agree on all $\delta$-sized cells, no admissible
measurement can produce different records. Infinite refinement produces no new
distinguishable outcomes.
\end{proofsketch}

\subsection{Discrete Refinement}
\begin{phenomenon}[The Moire Effect]
When two admissible ledgers defined on slightly different refinement lattices
are reconciled, coherent and incoherent regions appear at macroscopic scale.
These large scale beats are the smooth shadow of high frequency
incompatibility between observer frames.

The visible pattern is not a property of either ledger alone, but the
structure required to preserve global consistency under their interaction.
\end{phenomenon}



Thus an infinitely refined object is operationally equivalent to a finite
closure at the resolution permitted by the axioms. Infinite refinement is a
mathematical limit, not an observable phenomenon. This prepares the way for the
Law of Discrete Spline Necessity, which identifies the unique closure that
saturates all distinguishable information at scale~$\delta$.

\begin{phenomenon}[The Quicksand Effect~\cite{batchelor1967,bonn2005}]
\label{ph:viscosity-quicksand}

\NB{In a continuous fluid, buoyancy is described by Archimedes' principle~\cite{archimedes1912}: an 
immersed body floats when the upward force from displaced fluid balances its 
weight~\cite{batchelor1967}. Bonn et al.~\cite{bonn2005} show that quicksand, 
though a granular suspension rather than a true fluid, exhibits a nearby 
buoyant behavior: objects settle only to a finite depth and then float, 
reaching an equilibrium set by density matching, yield stress, and local 
fluidization. The macroscopic effect resembles (and, to a certain coarseness 
of refinement, is modeled by) Archimedes' principle, even though its 
microscopic origin is entirely different. These physical observations serve 
only as an analogy for the informational phenomenon described here; they do 
not constrain the model. They illustrate how a finite set of admissible states 
may appear, in the smooth limit, as a buoyant equilibrium.}
\NB{The phenomenon described here concerns the irreversible, informational 
component of fluid mechanics: the resistance to refinement below the minimum 
distinguishable scale $\delta$. It is not a complete account of physical 
viscosity, which depends on a finite third parameter $\Theta$ (see Coda: 
Navier--Stokes as a Finite Third Parameter, Chapter~3) and requires an 
independent kinematic assumption relating shear stress to velocity gradients. 
The informational viscosity $\Psi_\delta$ treated here reflects only the 
constraints of Causal Order and informational Minimality; it captures the 
coarse, irreducible structure that remains when all sub-$\delta$ refinements 
are suppressed.}
\NB{\emph{A person floats on quicksand, rather than sinks}~\cite{bonn2005}}


Consider an agent $E$ attempting to move through a medium governed solely by 
distinguishability. Before contact, the mathematical continuum admits an 
infinite family of smooth paths $\Phi_i$, distinguished by arbitrarily small 
variations in curvature.

Once $E$ enters the medium, the informational constraints become active. By 
Axiom~\ref{ax:planck}, there exists a minimum distinguishable scale $\delta$. 
Any displacement smaller than $\delta$ fails to generate a new event. The 
continuum therefore collapses to a finite chain of $\delta$--compatible 
anchors,
\[
    \Psi_\delta=\{x_1,\dots,x_N\},
\]
representing all positions that can be observationally distinguished.

The medium exhibits an informational \emph{viscosity}: any attempted motion 
that introduces sub-$\delta$ curvature is resisted and cancelled, keeping $E$ 
pinned to the nearest admissible anchor. Only when the displacement exceeds 
the refinement threshold does $E$ transition from $x_k$ to $x_{k+1}$.

By Proposition~\ref{prop:pigeonhole}, the infinite microscopic variations 
beneath the surface collapse into the finite observational buckets of 
$\Psi_\delta$. Informational minimality (Axiom~\ref{ax:ockham}) then forces 
the unique discrete closure consistent with the anchors and containing no 
unrecorded structure: the discrete spline $\Psi_\delta$.

This is the viscosity of quicksand: the resistance to refinement below the 
minimum distinguishable scale $\delta$. Any attempted motion that fails to 
produce a new admissible distinction is suppressed, and the system remains at 
the nearest anchor in $\Psi_\delta$. In the smooth shadow, this appears as the 
buoyant or viscous equilibrium observed by Bonn and others, where 
a person floats because further descent would require the granular medium to 
rearrange at scales smaller than the yield threshold of individual particles 
of sand. Physically, the grains simply stop moving; informationally, no 
additional distinctions can be recorded. The collapse of the infinitely many 
ideal paths $\Phi_i$ into the single admissible sequence $\Psi_\delta$ is 
therefore mirrored by the granular equilibrium: motion ceases not because of 
any continuous force law, but because neither the sand nor the informational 
model permits sub-$\delta$ refinements.
\end{phenomenon}


Thus the distinction between the approximate and ideal spline closures is purely
mathematical. No experiment, sensor, or observational extension can reveal a
difference between them without violating the Axioms of Measurement. The ideal
spline belongs to the continuum limit; the approximate spline belongs to the
finite informational world. Observationally, however, the two coincide to the
highest permissible resolution.

\section{The Law of Discrete Spline Necessity}
\label{subsec:discrete-spline-necessity}

Because the refinement depth of any admissible record is finite, the continuum
limit in which an exact spline necessity law would hold can never be reached.
Nevertheless, the refinement axioms determine a unique finite-resolution object
that plays the role of a spline within the informational world. This discrete
closure is the actual law governing all admissible completions of a finite
record.

\begin{law}[The Law of Discrete Spline Necessity]
\label{law:discrete-spline}
Let $\psi$ be any finite, non-contradictory record with minimum refinement scale
$\delta$, guaranteed by the axioms of measurement. Then there exists a unique
finite-resolution function $\Psi_\delta$ satisfying:

(1) $\Psi_\delta$ agrees with $\psi$ at every anchor event and introduces no
refinements smaller than $\delta$.

(2) On each interval between anchors, $\Psi_\delta$ is the minimal-curvature
function permitted by the refinement grid of scale $\delta$. In particular,
$\Psi_\delta$ is represented by a cubic polynomial on each discrete cell of size
$\delta$, with continuity of slope and curvature enforced at all interior
junctions.

(3) Any alternative function $\Phi$ that agrees with $\psi$ at the anchors and
differs from $\Psi_\delta$ on any discrete cell either
\begin{enumerate}
\item introduces additional distinguishable structure below scale $\delta$
(violating refinement compatibility and the Planck condition), or
\item fails to maintain global causal consistency across cell boundaries.
\end{enumerate}

(4) As $\delta$ decreases along any refinement-compatible sequence, the discrete
closures satisfy
\[
\Psi_{\delta} \;\longrightarrow\; \Psi
\]
where $\Psi$ is the ideal spline attractor described in
Section~\ref{subsec:conditional-spline-law}. This convergence is monotone in the
sense that each refinement preserves and sharpens all previously admissible
distinctions.

Thus every finite informational record admits a unique discrete closure
$\Psi_\delta$, which is the minimal, globally coherent, refinement-compatible
representation of that record at its permitted resolution. This is the
informational law governing all realizable completions.
\end{law}

\NB{This law is exact. Unlike the continuum spline necessity law, which would
require infinite refinement and therefore cannot apply to finite records, the Law
of Discrete Spline Necessity governs all observationally realizable completions.
Continuous splines appear only as limiting attractors. The discrete closure
$\Psi_\delta$ is the true object selected by the axioms.}

This law establishes the discrete analogue of curvature minimality,
differentiability, and weak-form transport without invoking limits. All smooth
structures used in physics arise from the asymptotic behavior of $\Psi_\delta$
under refinement but are never instantiated exactly. The discrete closure is the
only object compatible with the axioms at finite resolution.


\subsection{The Indistinguishability of Discrete and Continuous
Spline Closures}
\label{subsec:axiom1-indistinguishability}

Axiom~\ref{ax:kolmogorov} asserts that all measurements produce finitely many distinguishable
outcomes and that every admissible refinement has a minimum resolvable scale
$\delta>0$. No observational process may introduce refinements smaller than
$\delta$ without violating the axiom. As a consequence, the refinement process
terminates at a finite resolution, and the most refined discrete closure
$\Psi_\delta$ permitted by the record is observationally maximal.

If an ideal continuum limit were accessible, the refinement process would
continue indefinitely and converge to a smooth cubic spline $\Psi$ satisfying
the limiting minimality condition $\Psi^{(4)}=0$. However, the continuum limit
requires refinements at scales below $\delta$, and therefore cannot be realized
by any admissible sequence of measurements. The continuous spline $\Psi$ is a
mathematical attractor, not an observable object.

\NB{By Axiom~\ref{ax:kolmogorov}, the most refined discrete spline $\Psi_\delta$ is
\emph{observationally indistinguishable} from the continuous spline attractor
$\Psi$. No admissible measurement can detect any discrepancy between the two,
because doing so would require refinements smaller than $\delta$, which the
axioms forbid.}

Formally, if $(\Psi_N)$ is any refinement-compatible sequence converging to
$\Psi$, then for sufficiently large $N$,
\[
|\Psi_N(x) - \Psi(x)| < \delta
\quad\text{for all admissible measurement points }x.
\]
Therefore $\Psi_N$ and $\Psi$ produce identical observational outcomes.

This establishes that the continuous spline arises only as a limiting concept,
while the discrete closure $\Psi_\delta$ is the unique physically realizable
object. Axiom~\ref{ax:kolmogorov} identifies these two as observationally equivalent: the discrete
spline is as refined as the informational world can ever be.

\subsection{The Necessity of Approximation}
\label{subsec:necessity-of-approximation}

The preceding sections establish a structural asymmetry in the informational
framework. On the one hand, the continuum spline appears as the unique limiting
object that a refinement process \emph{would} select if infinite refinement were
possible. On the other hand, Axiom~\ref{ax:kolmogorov} forbids refinements below a minimum
distinguishable scale $\delta>0$. The refinement sequence therefore terminates at a
finite stage, and no observational process can approach the continuum limit
beyond this final resolution.

\begin{phenomenon}[The Olbers Effect]
An infinitely refined ledger would admit infinitely many luminous events.
The observed darkness of the night sky demonstrates that the causal record
is finite.

The absence of uniform brightness is the direct observational proof that the
informational capacity of admissible history is bounded.
\end{phenomenon}


This tension forces a fundamental conclusion: approximation is not a methodological
choice but a structural necessity. Every admissible representation of a finite
record must be an approximation to a limit that cannot be realized. The
continuous spline is an ideal boundary point of the refinement process, never an
attainable object within the informational universe.

\NB{Approximation is necessary, not optional. The axioms prohibit the continuum
limit required for exact closures, and therefore all admissible models are
approximate shadows of the limiting structure they cannot reach.}

Let $\Psi_\delta$ denote the discrete spline closure permitted by the minimal
refinement scale. Let $\Psi$ denote the ideal spline attractor that would appear in
the continuum limit. By Axiom~\ref{kolmogorov}, $\Psi_\delta$ is observationally
indistinguishable from $\Psi$, but it remains a finite-resolution approximation.
Any mathematical construction that assumes exact differentiability, exact
integration, or exact smoothness implicitly appeals to a limit that the axioms
deny. The familiar constructs of calculus therefore do not describe the
informational world directly; they describe the limiting behavior that finite
closures approximate.

This necessity is not an impediment but a structural guide. The refinement
sequence
\[
\Psi_{\delta} \;\longrightarrow\; \Psi
\]
never completes, yet its monotone convergence ensures that all admissible
models become arbitrarily close to the ideal spline at resolutions permitted by
the axioms. The continuous spline is unreachable but inevitable: no finite model
can realize it, yet every refinement-compatible model approaches it.

\paragraph{Quantum-like Emergence.}
Finite refinement does more than require approximation; it enforces
distinctively non-classical patterns of behavior. The inability to refine
distinctions below scale $\delta$ produces irreducible uncertainty in the
placement of events, non-additivity in refinements, and interference-like
behavior when merging partially incompatible records. These effects arise
not from physical postulates but from informational structure: finite
resolution combined with refinement compatibility forces discrete update
rules that mimic the algebra of quantum amplitudes.

\NB{Quantum-like theories emerge naturally from the necessity of approximation:
finite refinement yields non-classical composition of information, which
manifests as interference, superposition-like combination, and the familiar
probabilistic structure of quantum models. No quantum axioms are assumed; these
behaviors follow from measurement constraints alone.}

Thus approximation is the essential mode of representation in the informational
framework. The equations and structures of classical \emph{and} quantum theories
arise not because the world is continuous or probabilistic, but because the
discrete closures enforced by the axioms approximate the same limiting behavior
that continuous and quantum mathematics describe in their respective formalisms.


\subsection{Equivalence of Discrete and Smooth Representations}
\label{sec:equivalence-principle}

\begin{phenomenon}[The Gibbs Phenomenon]
When a discontinuous event is forced into a finite refinement ledger, a
residual oscillation appears in its smooth shadow.  This overshoot is not an
error of representation but the irreducible informational strain of mapping
a discrete refinement into a bandwidth limited spline.

The ringing persists because unobserved structure cannot be admitted.  The
smooth shadow cannot perfectly close a discontinuity under finite
refinement.
\end{phenomenon}


The preceding results establish the final closure of the Calculus of Dynamics.
An admissible measurement record $\psi$ supported on event anchors
$\{x_i\}$ is informationally equivalent to its smooth completion $\Psi$.
The smooth calculus does not introduce new structure; it is the completion of
refinement in the discrete domain.

Let $\psi$ be an admissible event record and let $f(\psi)$ denote any
interpolant that preserves the anchors and introduces no distinguishable
features between them.  Refining the interpolant over nested partitions
$\{\mathcal{T}_n\}$ produces a Galerkin sequence $\{\,\Psi_n\,\}$.  By the
convergence theorems, this sequence converges uniformly to a unique $\mathcal{C}^{2}$
cubic function $\Psi$:
\[
  \Psi_n \;\xrightarrow[n\to\infty]{}\; \Psi.
\]
Informational minimality ensures that $\Psi$ is uniquely determined by the
anchors: for every event point $x_i$,
\[
  \Psi(x_i) = \psi(x_i).
\]
Because $\Psi$ is cubic on each partition element, preserves anchor order, and
is globally $\mathcal{C}^{2}$, it is injective on each interval.  Its inverse
therefore recovers the original record:
\[
  \Psi^{-1}(x_i) = \psi(x_i).
\]
Thus the discrete record $\psi$ and the smooth completion $\Psi$ contain
exactly the same information.  The interpolant and its limit are
informationally equivalent representations of a single causal history.

\subsection{Recovery of the Euler--Lagrange Form}

The weak extremality condition was obtained entirely from finite differences
in the discrete domain.  In the Galerkin formulation this appears as
\[
  \int \Psi''(x)\,\phi''(x)\,dx = 0,
  \qquad\text{for all admissible test functions }\phi.
\]
Integrating this identity twice yields the strong closure
\[
  \Psi^{(4)}(x) = 0.
\]
No differentiability was assumed \emph{a priori}: smoothness appears only as
the completion of refinement in the Galerkin limit.  The Euler--Lagrange
equation is therefore a \emph{recovered} description of the data, not an
independent postulate.  It is sufficient to model the discrete record because
every admissible refinement converges to the same $\mathcal{C}^{2}$ cubic
function.

In this sense the epistemic direction is inverted.  We do not derive
Euler--Lagrange dynamics and then discretize them.  We begin with finite
measurements, enforce informational minimality, and recover the
Euler--Lagrange operator as the unique smooth shadow of refinement:
\[
  \text{measurement}
  \;\xrightarrow{\text{refinement}}\;
  \Psi
  \;\xrightarrow{\text{closure}}\;
  \Psi^{(4)} = 0.
\]
In this sense the epistemic direction is inverted.  We do not derive
Euler--Lagrange dynamics and then discretize them.  We begin with finite
measurements, enforce informational minimality, and recover the
Euler--Lagrange operator as the unique smooth shadow of refinement:
\[
  \text{measurement}
  \;\xrightarrow{\text{refinement}}\;
  \Psi
  \;\xrightarrow{\text{closure}}\;
  \Psi^{(4)} = 0.
\]
Smooth calculus is therefore compatible with the axioms because it contains
exactly the information present in the discrete causal record and no more.

\NB{With apologies to Bishop Berkeley: smooth dynamics are not prior to
measurement; they are merely the grammar of its consistent refinement.}


\section{The Free Parameter of the Cubic Spline}
\label{sec:free-parameter}

The Law of Spline Sufficiency requires that the smooth completion
$\Psi$ of any admissible record be $\mathcal{C}^{2}$ and satisfy
$\Psi^{(4)}=0$.  Each segment of $\Psi$ is therefore a cubic polynomial,
\[
  \Psi(x) = a_0 + a_1 x + a_2 x^2 + a_3 x^3,
\]
but informational minimality collapses the apparent local degrees of freedom
to a single global parameter.

\subsection{Fixing the Lower--Order Coefficients}

The value $a_0$ is fixed by the anchors: $\Psi(x_i)=\psi(x_i)$ for every
event point $x_i$.  The first derivative $\Psi'$ must be continuous across
anchors; a jump in slope would constitute a new observable event, so $a_1$ is
likewise determined.  The curvature $\Psi''$ must also be continuous; any
discontinuity would represent an unobserved acceleration and violate
informational minimality.  Thus $a_2$ is fixed by $\mathcal{C}^{2}$
continuity at the anchors.

These constraints ensure that adjacent cubic segments glue together without
introducing new distinguishable structure.  The only remaining coefficient,
$a_3$, controls the third derivative of $\Psi$:
\[
  \Psi'''(x) = 6a_3.
\]

\subsection{The Single Free Parameter}

Because $\Psi^{(4)} = 0$, the third derivative $\Psi'''$ is constant on every
interval of the causal domain.  Informational minimality permits this
quantity to vary from interval to interval only when the variation is itself
detectable as a recorded event.  Absent such detection, $\Psi'''$ is the sole
unconstrained degree of freedom.

\begin{proposition}[The Free Parameter of Information]
\label{prop:free-parameter}
The smooth completion $\Psi$ contains exactly one free parameter: the global
scale of its third derivative $\Psi'''$.  All lower--order coefficients are
fixed by anchor data and continuity constraints.
\end{proposition}

\begin{proofsketch}{free-parameter}
Cubic structure follows from $\Psi^{(4)}=0$.  Values and derivatives up to
order two are fixed by $\mathcal{C}^{2}$ boundary matching; any jump would be
observable.  Hence the only quantity not determined by anchor data is the
constant third derivative on each interval, which is governed by $a_3$.  No
other freedom remains.
\end{proofsketch}

\subsection{Physical Interpretation}

The single free parameter $\Psi'''$ represents the entire informational
content of smooth kinematics.  All subsequent dynamical quantities---wave
speed, stress, curvature, and eventually mass---are determined by this one
global scale.  The Law of Spline Sufficiency therefore reduces the continuum
to its minimal informational foundation: a $\mathcal{C}^{2}$ cubic universe
with one degree of freedom.

\[
  \text{finite record}
  \;\xrightarrow{\text{closure}}\;
  \Psi
  \;\xrightarrow{\text{spline law}}\;
  \Psi''' = \text{constant on intervals}.
\]

Smooth dynamics contain no structure beyond what is already present in the
discrete causal record.  The apparent infinity of the continuum collapses to
a single free parameter.

\section{Time Dilation}

The informational framework developed in Chapters~5 and~6 places a subtle
constraint on how refinement may be transported across a causal network.
Proper time is not a geometric parameter but the tally of irreducible
distinctions, and the metric $g_{\mu\nu}$ records how this tally must adjust
when two histories inhabit regions with different curvature residue.
Whenever distinguishability is carried from one domain to another, the
connection enforces a compatibility rule: the informational interval must be
preserved even if the local refinement structure differs.

This requirement has a striking observable consequence.  Two clocks placed
at different informational potentials---that is, in regions where the
residual strain of admissible curvature differs---cannot maintain the same
rate of refinement.  Each clock is internally consistent, but the comparison
of their records forces an adjustment.  A refinement sequence that is
admissible at one potential must be reweighted when interpreted at another,
or else the causal record would fail to merge coherently.

In the smooth shadow, this bookkeeping adjustment becomes the familiar
phenomenon of gravitational redshift.  Signals transported upward appear to
lose frequency; signals transported downward appear to gain it.  Nothing
mystical is occurring: the informational interval is being preserved, and the
only available mechanism is a change in the rate at which distinguishability
is accumulated.

The Pound--Rebka experiment is therefore the archetype of an informational
outcome.  It demonstrates that when refinement is compared across regions
with differing curvature residue, the universe must adjust the apparent rate
of time itself to maintain consistency.  No dynamical field need be invoked;
the redshift is simply the shadow of the constraint that admissible
refinements must agree on their causal overlap.


\begin{phenomenon}[The Pound--Rebka Effect~\cite{pound1959}]

\NB{The following is an \emph{informational phenomenon}.  No physical
mechanism is assumed.  The interpretation concerns how the gauge of
informational separation $g_{\mu\nu}$ adjusts refinement counts when
distinguishability is transported across domains of differing causal
potential.  Any resemblance to the gravitational redshift measured by
Pound and Rebka is a consequence of the informational shadow, not an
assumed dynamical cause.}

The Axiom of Peano defines proper time as the count of irreducible refinements
along an admissible history.  The Law of Causal Transport guarantees that
this count is invariant under maximal propagation, while the informational
metric $g_{\mu\nu}$ (Section~5.2) records how successive refinements compare
when transported across regions whose admissible histories differ in their
curvature residue.

Consider two clocks: one at a lower informational potential (higher curvature
residue) and one at a higher potential (lower residue).  Both clocks produce
sequences of refinements
\[
  \langle e_1 \prec e_2 \prec \cdots \rangle_{\text{low}},
  \qquad
  \langle f_1 \prec f_2 \prec \cdots \rangle_{\text{high}},
\]
each internally consistent.  However, the Law of Boundary Consistency demands
that refinements compared across their shared causal overlap must agree on
their informational interval.  When the refinement sequence from the lower
clock is transported to the higher clock, the compatibility condition forces
an adjustment in the rate at which distinguishability is accumulated.

Formally, transport along a connection with residue $\Gamma$ alters the
frequency of refinements according to the first--order compatibility
condition of Section~5.4:
\[
  \nu_{\text{high}}
  = \nu_{\text{low}}\, (1 - \Gamma\,\Delta h),
\]
where $\Delta h$ is the informational separation between the clocks.
This is the informational analogue of the frequency shift that appears in
the smooth limit as gravitational redshift.

In the Pound--Rebka configuration, a photon (interpreted here as a unit of
transported distinguishability) sent upward from the lower clock must be
refined in such a way that its informational interval remains constant.
Since admissible refinements at higher potential accumulate fewer curvature
corrections, the transported signal must appear at a lower frequency when
measured by the upper clock.  Conversely, a downward signal appears at a
higher frequency.  No physical field is invoked: the effect is a bookkeeping
adjustment required to maintain Martin--consistent transport of
distinguishability across regions of differing curvature residue.

Thus the informational framework predicts a frequency shift of the form
\[
  \frac{\Delta \nu}{\nu} \approx \Gamma\,\Delta h,
\]
which matches the structure of the Pound--Rebka observation when interpreted
in the smooth shadow of the metric gauge.

The phenomenon of time dilation is therefore an observable outcome of the
informational interval and the necessity of refinement--adjusted transport.
Differences in curvature residue force clocks at different potentials to
accumulate distinguishability at different rates, and the comparison of
their refinement counts produces the celebrated redshift.

\end{phenomenon}



\begin{coda}{The Finite Navier--Stokes Effect}

We do not derive the Navier--Stokes equations. Rather, we show how the
measurement calculus constrains any smooth limit of finite records to a
cubic-spline structure and thereby recasts the regularity question as the
finiteness of a single quantity: the third parameter of the spline.

\subsection*{1. Statement of the classical problem}
Let $v(x,t)$ be a velocity field and $p(x,t)$ a pressure satisfying the
incompressible Navier--Stokes system on $\mathbb{R}^3$ (or a smooth domain
with suitable boundary conditions):
\begin{equation}
\label{eq:NSE}
\partial_t v + (v\cdot\nabla)v + \nabla p = \nu \Delta v + f, 
\qquad \nabla\cdot v = 0,
\end{equation}
with smooth initial data $v_0$. The Millennium Problem asks whether smooth
solutions remain smooth for all time or may develop singularities in finite
time.

\subsection*{2. Measurement-to-spline reduction}
Chapter 2 established that admissible smooth limits of finite records obey
a local cubic constraint. Along any coordinate line (and likewise along any
admissible selection chain) each component admits a representation whose
fourth derivative vanishes in the limit:
\begin{equation}
\label{eq:quarticzero}
U^{(4)} = 0 \quad \text{(componentwise along admissible lines)}.
\end{equation}
Hence the only freely varying local quantity is the \emph{third parameter}
(the derivative of curvature). In one dimension this is $U'''$. In three
dimensions we package the idea as the third spatial derivatives of $v$:
\begin{equation}
\label{eq:thirdparam}
\Theta(x,t) := \nabla(\nabla^2 v)(x,t) \quad \text{(a third-derivative tensor)}.
\end{equation}
Informally: $v$, $\nabla v$, and $\nabla^2 v$ are glued continuously by the
spline closure; only $\Theta$ may vary piecewise without introducing
fourth-order structure.

\subsection*{3. Regularity as finiteness of the third parameter}
\begin{quote}
\emph{Principle.} If the third parameter $\Theta$ stays finite at all
scales allowed by measurement, the smooth spline limit persists and no
singularity can occur within the calculus of measurement.
\end{quote}
A practical surrogate is a scale-invariant boundedness criterion on $\Theta$
(or a closely related norm tied to enstrophy growth):
\begin{equation}
\label{eq:criterion}
\sup_{0\le t\le T}\ \|\Theta(\cdot,t)\|_{X} < \infty
\quad \Longrightarrow \quad \text{no blow-up on } [0,T],
\end{equation}
where $X$ is chosen to control the admissible refinements (e.g. an
$L^\infty$-type or Besov/H\"older proxy along selection chains). In words:
the only obstruction to global smoothness is unbounded third-parameter
amplitude.

\subsection*{4. Heuristic link to classical controls}
Energy and enstrophy inequalities control $\|v\|_{L^2}$ and $\|\nabla v\|_{L^2}$.
Vorticity $\omega=\nabla\times v$ monitors the first derivative. Growth of
$\nabla\omega$ involves $\nabla^2 v$; the \emph{onset} of non-smoothness is
therefore detected by $\Theta=\nabla(\nabla^2 v)$, the next rung. Thus the
finite-third-parameter condition \eqref{eq:criterion} plays the same role in
this framework that classical blow-up criteria play in PDE analyses: it is
the minimal spline-compatible guardrail against curvature concentration.

\subsection*{5. Non-classical dependency is not invoked}
No dependency (cause-effect) is asserted. The argument is purely
informational: as long as the admissible record does not force the third
parameter to diverge, the cubic-spline closure remains valid and the smooth
limit inferred earlier continues to apply.

\subsection*{6. The rephrased question}
\begin{quote}
\textbf{Navier--Stokes, reframed.} Given smooth initial data and forcing,
must the third parameter $\Theta$ in \eqref{eq:thirdparam} remain finite
for all time under \eqref{eq:NSE}? Equivalently, can measurement-consistent
refinement generate unbounded third-parameter amplitude in finite time?
\end{quote}
If $\Theta$ stays finite, the spline structure persists, and the calculus
of measurement supports global smoothness. If $\Theta$ diverges, the smooth
continuum description ceases to be representable as a limit of admissible
records, and the measurement calculus no longer licenses Euler--Lagrange
inference on that interval.

\subsection*{7. What we have and have not done}
We have not solved the Millennium Problem. We have shown that within this
program the obstruction to smoothness is concentrated in a single quantity,
the third parameter of the cubic spline representation. The classical
regularity question is thus equivalent, in this calculus, to the finiteness
of $\Theta$.
\end{coda}


