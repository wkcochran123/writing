\chapter{Introduction: Facts and Truths}

Scientific knowledge begins not with theory, but with tension.

A \emph{fact} is a record that resists trivial dismissal.  A \emph{truth} is a
structure that survives systematic attack.  The distance between them is the
entire labor of science. Sometimes, this distinction is lost.

The earliest failures of this distinction were philosophical, not
numerical. Berkeley’s objection to Newton was not that Newton’s mechanics
were wrong, but that infinitesimal and invisible constructions had been
smuggled into the argument without operational warrant~\cite{berkeley1734}. Newton’s fluxions
explained motion by appealing to quantities that could not be measured,
manipulated, or even coherently defined within the observational practices
of the time. Berkeley’s criticism was simple and devastating: a scientific
argument cannot depend on objects that cannot, even in principle, be tied
to an act of measurement. Should we trust a piece of mathematics if the
mathematics has no physically demonstrable meaning?

Later, the industrial sciences confronted the same problem from the
opposite direction: not the danger of infinite idealization, but the danger
of finite noisy data being mistaken for certainty. With the rise of
precision instrumentation in the nineteenth and twentieth centuries, it
became clear that raw measurements are never perfect; they are
approximations corrupted by noise, drift, bias, and calibration error. Yet
these imperfect measurements were often treated as exact, producing models
whose apparent precision exceeded the reliability of the data that
generated them. The overinterpretation of finite data can be as misleading
as the invocation of unobservable infinities.

Both failures of infinitessimals point to the same underlying issue: science requires a clear
distinction between the record of what has been observed and the structures
we infer from that record. The goal of this work is to formalize that
distinction and to construct the observed laws of the physical world from the 
admissible constraints imposed by the observational history itself.

\section{Observable and Inobservable}

In modern form, the criticism is that one cannot refine beyond what a
measurement can actually distinguish.  An argument that depends on
infinitesimal structure that no instrument could resolve is already
admitting information that the experimental record cannot contain.  

\begin{phenomenon}[The Bishop Berkeley Effect \cite{galileo1638}]
\label{ph:fact-effect}

The Bishop Berkeley Effect expresses a constraint on admissible mathematics:
structure may not be introduced faster than it can be operationally
recovered.  Berkeley’s complaint was not that Newton’s calculus failed, but
that it succeeded by appealing to entities that could neither be produced nor
distinguished by any finite observer.  Fluxions and infinitesimals appeared as
objects of calculation without being objects of measurement.

In this framework, a construction does not acquire the status of fact until
there exists a procedure by which its effects could be recorded in a causal
history.  Until such a procedure is available, the construction functions only
as inspiration---a useful guide for reasoning, but not a measured effect in
the experimental record.  Mathematics is therefore subordinate to distinguishability:
symbols are permitted only when they correspond to operations that could, in
principle, leave a finite trace.

Before Bishop Berkeley, Galileo noted the importance of 
empiracy (See Phenomenon~\ref{ph:galileo-effect}).  He was the
first to insist that a claim about nature is admissible only if an
instrument, a procedure, or a repeatable experiment could in principle
distinguish it~\cite{galileo1638}.  His program tied knowledge to operations 
that leave finite, recoverable traces.

Berkeley supplied the complementary warning: even when mathematics succeeds,
its constructions must still answer to that same discipline.  No symbolic
form, however persuasive, is entitled to represent a fact unless its effects
could be recovered by a finite observer.

Together they enforce a single principle: facts precede the symbols used to
describe them.  No mathematical object is admissible unless a
finite observer could, in principle, construct a record distinguishing it.

A fact is not a symbol.  It is a resistance: something that persists under
attempts to erase it.  Mathematics that outruns operational recovery loses its
connection to fact and becomes metaphysics.

This discipline---that facts precede structure---is the Bishop Berkeley Effect.
\end{phenomenon}

Phenomenon~\ref{ph:fact-effect} secures the boundary of admissible structure, but it
does not determine how claims survive contact with noise.  It tells us what
is forbidden to assert, but not how fragile assertions should be tested.

Once mathematics is disciplined by operational recoverability, a second
problem emerges immediately: admissible measurements are never exact.  Even
when structure is physically constructible, the record of observation is
finite, irregular, and contaminated by variation.  The universe does not
present crisp algebraic objects for observation, just clouds of outcomes.

At this point, failure changes character.  The danger is no longer the
introduction of metaphysical objects, but the premature declaration of truth
from insufficient evidence.  A new discipline is required: not one that
prevents imaginary structure, but one that makes genuine structure earn its
right to be believed.

The next phenomenon captures this inversion. It excludes every continuation
that is unsupported by the record, permitting a refinement only when all
other possibilities are ruled out as nonexistent or incompatible.

\begin{phenomenon}[The Hume Effect \cite{hume1748}]
\label{ph:hume-effect}
One operational mechansim of the Hume effect is explored in 
Phenomenon~\ref{ph:gosset-t-test}.

As Hume argued, no finite collection of observations can logically guarantee a
universal claim.  A history of recorded events, however extensive, cannot rule
out the possibility that some future refinement will produce a counterexample
that the present record is too coarse to reveal.  The obstacle is not merely
logical but operational: a finite observer cannot distinguish the indefinitely
many circumstances in which a purported law might fail.

Admissible knowledge therefore proceeds by attempted refutation rather
than confirmation.  A rule earns standing only by resisting opportunities to
break it.  Each refinement that fails to produce a contradiction strengthens
the rule's credibility, but none can elevate it to certainty.  Confirmation
adds no logical force; only the accumulation of failed refutations marks a
claim as durable.

In this framework, the reach of a universal statement is measured by the
growth of the history it continues to survive.  Its authority derives
from persistence under refinement, not from the number of times it has been
observed to hold.

This discipline---that universality rests on resistance rather than
accumulation---is the Hume Effect.
\end{phenomenon}

As such, the central claim of this monograph is that the universe can be described
as a pair of mutually defining operations: \emph{measurement} and \emph{distinction}.
The first gives rise to the calculus of variation; the second to the
ordering of events.  We introduce the \emph{Causal Universe Tensor} as the mathematical
structure that encodes measuring events.  The Causal Universe Tensor unites events by showing that every
measurement in the continuous domain corresponds to a finite operation in
the discrete domain, and that these two descriptions agree point-wise to
all orders in the limit of refinement of a finite gauge theory of information.  
The familiar objects of physics—wave equations, curvature,
energy, and stress—then emerge not as independent postulates but as
necessary conditions for maintaining consistency between the two sides of
this dual system.

From this perspective, the classical boundary between mathematics and
physics dissolves.  Calculus no longer describes how the universe evolves
in time; it expresses how consistent order is maintained across finite
domains of observation.  Its dual, the logic of event selection, guarantees
that these domains can be joined without contradiction.  Together they
form a closed pair: an algebra of relations and a calculus of measures,
each incomplete without the other.  The subsequent chapters formalize this
duality axiomatically, derive its tensor representation, and show that the
entire machinery of dynamics—motion, field, and geometry—arises as the
successive enforcement of consistency between the two.

We begin by describing the experimental record itself and, in doing so, set
down the axioms that govern the recording of measurement, the foundation upon
which understanding must rest. 

\section{Ledgers}
\label{sec:intro-ledger}
The experimental record is all the experiments and observations used in
the pursuit of science.  It starts in the single experiment whose results are
then merged into the common scientific understanding.  The single experiment
generates facts that are observed and noted.

A scientific observation is not a value of a continuous field, but a
distinguishable event produced at a definite moment in the observer’s
refinement of the world. To reason about such observations, we require a
structure that records them faithfully and constrains how they may evolve.
We call this structure a \emph{ledger}.

\begin{definition}[Ledger]
\label{def:intro-ledger}
A \textbf{ledger} is an ordered, finite or countable sequence of
distinguishable events,
\[
  L = \langle e_1 \prec e_2 \prec \cdots \prec e_n \prec \cdots \rangle,
\]
such that:
\begin{enumerate}
\item \textbf{Finiteness or countability} (See Axiom~\ref{ax:cantor}).  
      The ledger contains only finitely or countably many events.

\item \textbf{Irreversibility} (See Axiom~\ref{ax:planck}).  
      New events may be appended, but existing ones may not be erased or
      retroactively altered.

\item \textbf{Refinement structure}.  
      Each event $e_{k+1}$ is a refinement of the admissible outcomes
      remaining after $e_k$; that is, it restricts the set of configurations
      compatible with all earlier entries of the ledger.

\item \textbf{Distinguishability}.  
      Events must correspond to outcomes that the observer can tell apart.
      If two outcomes cannot be distinguished operationally, they represent
      the same event in the ledger.
\end{enumerate}
\end{definition}

A ledger is therefore not a passive list of observations, but an
\emph{active record of eliminations}. Each new event prunes the set of
admissible continuations, narrowing the universe of possibilities. The
ledger captures exactly what has survived this process of refinement and
nothing more.

In this sense, history is not an accumulation of information but the
systematic removal of incompatible configurations. The ledger is the
mathematical object that encodes this pruning, and it is the foundation on
which all later notions of compatibility, distance, and dynamics are built.

