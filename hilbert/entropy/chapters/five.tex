\chapter{Informational Stress}
\label{chap:kinematics-of-light}

The preceding chapters established that smooth motion appears as the
unique closure of causal order under refinement.  The Law of Spline
Sufficiency showed that any admissible continuous shadow must contain no
unrecorded structure and therefore satisfies the extremal condition
$\Psi^{(4)} = 0$.

In this chapter we examine the opposite extremum: the smallest admissible
refinement of the Causal Universe Tensor.  Such a refinement represents
the maximal rate at which distinguishability can propagate without
violating the Axiom of Planck.  We call this minimal, nonzero update an
\emph{informational quantum}.  It is not a physical particle or field;
it is the atomic refinement permitted by the axioms.

\section{Informtional Quantum}

Precision in this framework is not free. By the Law of Discrete Spline
Necessity, admissible interpolation cannot be performed with arbitrarily fine
resolution. Every smooth completion is the limit of a finite refinement
process, and every such refinement is bounded by the Axioms of Kolmogorov and
Planck. This forces a minimal unit of admissible distinction: an
\emph{informational quantum}. The spline may assign exact analytic values
between anchor points, but those values are only meaningful up to the smallest
refinement allowed by the causal ledger. Precision therefore emerges not as a
continuum ideal, but as a quantized requirement. The theory is compelled to be
precise only in discrete units, because no admissible history may resolve
structure smaller than the finite quantum of measurement demanded by coherent
spline closure.


\begin{phenomenon}[Precision]

The transition from discrete measurement to continuous description is not
introduced by assumption, but forced by the structure of admissible
interpolation. By the Law of Spline Sufficiency, any finite sequence of anchor
events admits a unique minimal--curvature completion. This completion assigns
values not only at the recorded events themselves, but at all admissible
points between them.

Precision and accuracy are distinct in this framework. \emph{Precision} refers
to the determinacy of the interpolated values supplied by the admissible
spline: once anchor points are fixed, the analytic completion assigns
well--defined values at every intermediate point. \emph{Accuracy}, by contrast,
refers only to agreement with future measurement events. A value may be
perfectly precise---uniquely determined by the axioms---and yet not be accurate if
a subsequent refinement records a different event. Precision is therefore a
property of admissible completion, while accuracy is a property of the
experimental record. The former is forced by coherence; the latter remains an
empirical constraint.
These intermediate values are not measurements. They are consequences.

The spline interpolant supplies a determinate analytic value at every point of
its domain, even though such points were never observed. This phenomenon is
the origin of \emph{precision} in the informational framework. The causal
ledger remains discrete, but the admissible completion is continuous. The
difference between what is recorded and what is implied is not a defect; it is
a structural requirement of coherence.

Precision does not arise from improved instruments or finer resolution. It
arises from necessity. Once anchor points are fixed, the axioms force a unique
analytic structure between them. The values supplied by the spline are not
guesses, and they are not stochastic. They are the only values consistent with
information minimality and global admissibility.

In this sense, precision is not an empirical achievement but a mathematical
obligation. The continuum is not observed; it is compelled.
\end{phenomenon}

The remainder of this chapter treats the consequences of this effect. When
analytic predictions are treated as real numbers rather than combinatorial
counts, new bookkeeping problems arise. These problems do not reflect physical
forces, but the informational cost of maintaining precision between discrete
events.

\section{The Informational Bound $\epsilon$}
\label{sec:epsilon}

\NB{The refinement bound $\epsilon$ is not a physical quantum, particle, or
energy unit.  It is the minimal nonzero increment of distinguishable structure
that survives every admissible refinement of the measurement record.  Its
origin is purely informational: $\epsilon$ is the continuous shadow of the
residual $\mathcal{C}^2$ freedom in spline closure.  No physical ontology is
implied.}

The refinement of an observational record proceeds through countable additions
of distinguishable events. As established in Chapter~\ref{chap:motion}, the weak form of the
discrete bending functional admits a single free $\mathcal{C}^2$ parameter,
corresponding to the third derivative of the spline interpolant. This degree
of freedom is not an artifact of approximation; it is a structural remnant of
finite measurement.

By the Law~\ref{law:finite-spline}, admissible completion cannot eliminate
this residual freedom except through the introduction of new anchor events.
When no additional measurements are recorded, the remaining degree of freedom
is irreducible. The continuous shadow is therefore forced to carry a minimal,
nonvanishing bound on curvature--level distinction. This bound is not imposed
by physics, but by the impossibility of selecting a unique refinement in the
absence of new information.

We denote this invariant residual by $\epsilon$.  Any admissible refinement of
the continuous shadow must preserve $\epsilon$; to refine below this threshold
would introduce unrecorded structure and contradict the finite measurement
sequence.  Conversely, any refinement that preserves $\epsilon$ remains
consistent with the discrete data.  Thus $\epsilon$ functions as the
kinematic limit of refinement and provides the foundation for the emergent
invariant interval $\tau$ and operators that may look familiar to some.

\begin{phenomenon}[The Richardson Effect~\cite{richardson1961}]
\label{ph:richardson-effect}
\NB{In a nut--shell, how long is Britain's coastline and why does the
answer depend on the length of the ruler~\cite{mandlbrot1967}?}

The measured length of a boundary increases without limit as the resolution of
measurement is refined, even though the underlying admissible structure
remains finite. This phenomenon is most clearly expressed by the classical
coastline mapping problem: the total measured length of a coastline depends
monotonically on the length of the measuring stick.

When a coastline is traced using a coarse measuring scale, long rulers bridge
over bays, inlets, and local irregularities. The resulting path is smooth at
that scale and the reported length is relatively short. As the measuring scale
is reduced, the ruler no longer spans these features. Previously ignored
curvature is now forced into the admissible path. The measured length
increases because the boundary is not smooth; it carries irreducible
roughness.

In the informational framework, this roughness is not accidental. By the Law
of Finite Spline Selection, a spline constrained only by finitely many anchor
points must retain a residual degree of curvature freedom. That freedom does
not vanish between measurements; it remains latent. As resolution improves,
the measurement process is compelled to resolve this latent curvature. The
coastline must appear rough, because a perfectly smooth boundary would require
infinite observational constraint.

The coastline does not acquire new structure under refinement. Rather, its
admissible minimal completion is forced to reveal structure that was always
present but previously collapsed by coarse measurement. The increase in
measured length is therefore not a property of the land, but a consequence of
how finite measurement interacts with unavoidable curvature residue.

In this sense, roughness is not an empirical irregularity. It is a structural
requirement of any boundary recorded by finitely many distinguishable events.

The Richardson Effect is not a property of space. It is a property of
measurement. A boundary is not an object with a fixed length; it is a ledger
of distinguishable anchor points together with their admissible minimal
completions. As refinements increase, the informational content of the
boundary increases, and the measured length grows accordingly.

There is no convergence to a true length. There is only an ever--refining
account of admissible curvature. See Phenomenon~\ref{ph:empiracy}.
\end{phenomenon}


\subsection{Residual Spline Freedom and the Minimal Refinement Bound}
\label{sec:residual-spline}

The weak formulation developed in Chapter~3 shows that a sequence of discrete
measurements determines a unique cubic spline interpolant, except for a single
free parameter associated with the third derivative.  This residual freedom is
a consequence of the information minimality constraint: in the absence of
additional recorded events, the interpolant must not introduce structure that
was not observed.  A fourth--order correction would imply an unrecorded change
in curvature and is therefore inadmissible.

Let $\Psi$ denote the continuous shadow of the discrete observational chain.
The freedom in $\Psi'''$ represents the smallest degree of curvature that can
be varied without conflicting with the discrete record.  Its magnitude cannot
be reduced by any admissible refinement unless a new event occurs.  The
minimal nonzero shadow of this residual is the refinement bound $\epsilon$.

Formally, $\epsilon$ is the infimum of all curvature--level refinements that do
not violate the existing event sequence.  Any attempt to impose a refinement
of order smaller than $\epsilon$ would create new curvature that must be
supported by new events, contradicting the record.  Thus $\epsilon$ is the
continuous representation of the irreducible $\mathcal{C}^2$ residue.

\subsection{Maximal Informational Propagation}
\label{sec:maximal-informational}

An admissible refinement of the observational record adds distinguishable
structure without contradicting previously recorded events.  A path that
\emph{saturates} the refinement bound $\epsilon$ propagates information at the
maximal admissible rate: it incorporates all allowable distinction while
introducing no unrecorded curvature.

Such paths form the extremal curves of the informational geometry.  They are
defined not by physical principles, but by the logical requirement that
refinement cannot fall below the $\epsilon$ threshold.  Any further reduction
would imply hidden structure and is therefore inadmissible.

In the continuous shadow, these maximally propagated paths serve as the
reference curves for defining the invariant interval $\tau$.  Two observers who
refine the same extremal path must agree on the number of informational units
required to describe it; this count determines the causal interval and anchors
the construction of the metric in Section~5.2.

\begin{phenomenon}[Compact Disc Encoding~\cite{sony1980,philips1980}]
\label{ph:cd-phenomenon}
\NB{The compact disc format is treated here not as an optical or physical
device but as a concrete implementation of an informational system.
Its behavior illustrates how distinguishability, admissible refinement,
finite alphabets, and boundary consistency determine the structure of a
real-world communication medium. No photonic or physical assumptions are
made; the CD is considered solely as a record of measurable distinctions.}
\NB{This thought experiment does \emph{not} describe photons as informational
quanta. It is a finite conceptual model illustrating how a gauge of separation
emerges from the logic of distinguishability alone. No physical ontology is
implied.}

The compact disc (CD) format developed jointly by Sony and Philips
implements a finite alphabet of distinguishable marks: pits and lands
arranged along a single spiral track. Each measurement by the reader
selects one symbol from this alphabet. The resulting word encodes audio
data through a sequence of refinements governed by cross–interleaved
Reed--Solomon coding (CIRC), an error-correcting structure patented in
the foundational work on digital optical media~\cite{sony1980,philips1980}.

A notable design constraint is the total record length. The original
Sony specification targeted a runtime of approximately 74 minutes
(often quoted as 72 minutes in early engineering drafts) so that a
single disc could contain a complete performance of Beethoven's Ninth
Symphony. Although historical details vary, the engineering requirement
is informational in nature: the spiral track must accommodate a finite
number of distinguishable symbols, each encoded with redundancy and
refinement structure sufficient to guarantee coherent recovery.

Thus the CD provides a physical instantiation of an informational
phenomenon: a medium whose structure, capacity, and correction rules are
determined entirely by the algebra of distinguishability and refinement.

A compact disc stores information as a finite, ordered chain of distinctions.
Each pit or land corresponds to a single admissible event, and the reader
detects a new event only when the reflected signal exceeds its threshold of
discernibility. Everything below this threshold is invisible; it cannot enter
the admissible record. Thus the sequence of detections,
\[
e_1 \prec e_2 \prec e_3 \prec \cdots,
\]
encodes not only what \emph{was} observed, but the binding constraint that no
additional distinguishable structure may be inserted between these events.

From the standpoint of information, the read head defines a \emph{gauge of
minimal separation}: two surface configurations are “far enough apart’’ exactly
when the detector must refine its admissible description to distinguish them.
The metric is not assumed; it is inferred from the rule that only resolvable
differences may appear as refinements in the causal chain.

Now imagine two readers, A and B, scanning the same disc. Reader~A has a coarser
threshold; reader~B resolves finer distinctions. Each produces its own ordered
sequence of admissible events. Where B records additional refinements, A
records none. Yet when their records are merged, global coherence requires a
single history that preserves all recorded distinctions. The finer record forces
a refinement on the coarser: A must treat certain portions of the disc as
informationally extended, for failure to accommodate B’s distinctions would
render the merged history inconsistent.

In the dense limit, this refinement rule induces a continuous connection: the
shadow of the logical requirement that adjacent descriptions remain compatible
under transport. What appears in the smooth theory as a \emph{metric} is
nothing more than this bookkeeping of distinguishability: the minimal rule that
certifies when two states differ in a way that must be reconciled.

In this model, “light’’ corresponds not to a substance but to the maximal rate
at which new distinctions can be admitted without contradiction. Any attempt to
introduce refinements faster than this rate would violate global coherence.
Thus the invariant causal interval of Chapter~5 reflects the same constraint: an
observer may not admit distinctions faster than a globally coherent merge can
support.

The compact disc reader therefore offers a finite, concrete metaphor for the
emergence of the gauge of light, the metric as a rule of separation, and the
transport laws that follow from informational consistency.
\end{phenomenon}


\section{The Metric as a Gauge of Informational Separation}
\label{sec:metric}

\NB{The metric is not a physical fabric, field, or medium.  It is the
continuous shadow of the rule by which refinements preserve the invariant
causal interval.  Its function is purely informational: the metric enforces
that all admissible observers agree on the amount of distinguishable structure
between neighboring events.  No geometric ontology is assumed.}

The refinement bound $\epsilon$ defined in Section~\ref{sec:epsilon} limits how
finely an admissible history may be resolved without contradicting the
measurement record.  When this bound is propagated along an extremal path, the
result is a conserved quantity---the informational interval $\tau$---which
represents the number of $\epsilon$-sized refinements required to traverse that
path.  Any two observers refining the same extremal sequence must therefore
agree on the value of $\tau$.  This invariance is the kinematic foundation of
the metric.

Let $dx^\mu$ denote the local labels an observer assigns to successive events
along an extremal refinement.  Another observer, using a different admissible
convention for distinguishing the same events, assigns labels
$dx'^\mu = \Lambda^\mu_{\ \nu} dx^\nu$, where the transformation
$\Lambda^\mu_{\ \nu}$ preserves causal order as required by the Axiom of
Selection.  Although the two observers differ in the coordinates they assign,
they must agree on the number of $\epsilon$-sized refinements separating the
events; otherwise their merged histories would violate global consistency.

This requirement implies the existence of a bilinear form $g_{\mu\nu}$ such
that the scalar quantity
\[
\tau^2 = g_{\mu\nu}\, dx^\mu dx^\nu
\]
is invariant under all admissible changes of local labeling conventions.  The
metric is therefore the gauge of informational separation: it encodes how
distinctions between events are preserved when observers describe them using
different coordinate choices.

Because $\tau$ counts informational increments rather than geometric lengths,
the invariance condition
\[
g'_{\mu\nu}\, dx'^\mu dx'^\nu = g_{\mu\nu}\, dx^\mu dx^\nu
\]
expresses the deeper statement that the number of distinguishable refinements
between neighboring events is independent of the observer.  The metric
formalizes this invariance.  It is the bilinear bookkeeping rule ensuring that
every observer's coordinate description yields the same value of $\tau$ for any
given extremal path.

In this interpretation, the components of $g_{\mu\nu}$ do not prescribe a
geometry; they record the local comparison rule by which distinguishability is
maintained across observational frames.  Section~\ref{sec:law-causal-transport}
elevates this requirement to a formal law, showing that preservation of the
causal interval uniquely determines the compatible affine connection used to
propagate distinguishability under refinement.


\subsection{The Law of Causal Transport}
\label{sec:law-causal-transport}

\NB{The Law of Causal Transport is a kinematic statement.  It asserts only that
informational refinements must preserve the invariant interval $\tau$ defined
in Section~\ref{sec:metric}.  No dynamical interpretation of curvature or
stress is assumed here.  The law specifies how distinguishability must be
propagated under admissible changes of frame; all higher structures of
connection and curvature follow in later sections.}

The refinement bound $\epsilon$ defines the smallest admissible increment of
distinguishable structure.  When propagated along an extremal path, $\epsilon$
induces the invariant interval $\tau$, representing the total number of such
increments required to describe that path.  Because every observer must refine
the same underlying event sequence, the value of $\tau$ must remain unchanged
under all admissible relabelings.

This requirement leads to the following principle.

\begin{law}[The Law of Causal Transport]
\label{law:causaltransport}
\emph{[Preservation of Distinguishability]}  
Any admissible refinement of an observational record must preserve the
informational interval $\tau$ between neighboring events.  In the continuous
shadow, this condition determines a unique bilinear form $g_{\mu\nu}$ and a
unique compatible rule of transport $\Gamma^{\lambda}_{\mu\nu}$ satisfying
\[
\nabla_{\lambda} g_{\mu\nu} = 0.
\]
The pair $(g_{\mu\nu}, \Gamma^{\lambda}_{\mu\nu})$ constitutes the metric gauge
of informational separation.
\end{law}

Because observers may assign different coordinates to the same infinitesimal
event displacement, we represent such a relabeling by
$dx'^{\mu} = \Lambda^{\mu}_{\ \nu}\, dx^{\nu}$, where $\Lambda^{\mu}_{\ \nu}$
preserves causal order.  The Law of Causal Transport requires that the
informational interval be invariant under this transformation:
\[
g'_{\mu\nu}\, dx'^{\mu} dx'^{\nu}
=
g_{\mu\nu}\, dx^{\mu} dx^{\nu}.
\]
This invariance elevates $g_{\mu\nu}$ from a mere bookkeeping device to a
constraint: it is the only bilinear form that guarantees all observers agree on
how many $\epsilon$-sized refinements separate neighboring events.

The law further implies that the comparison of nearby refinements must not
depend on the path taken in the space of coordinate labels.  This requirement
determines the connection coefficients $\Gamma^{\lambda}_{\mu\nu}$ as the
unique differential operators that preserve the metric gauge under change of
frame.

In this sense, the Law of Causal Transport encodes the most fundamental rule
of the kinematic structure: that distinguishability is preserved under motion.
The connection is not postulated, but forced by the need to maintain the
interval $\tau$ when an observer’s coordinate conventions vary from point to
point.  Section~\ref{sec:tau-invariance} elaborates the invariance of
$\tau$, and Section~\ref{sec:metric-bilinear} formalizes the role of
$g_{\mu\nu}$ as the bilinear form that preserves the $\epsilon$--refinement
count.


\subsection{Invariance of the Informational Interval $\tau$}
\label{sec:tau-invariance}

\NB{The interval $\tau$ is not a geometric length or a physical duration.  It is
the continuous shadow of an event count: the number of $\epsilon$-sized
refinements required to describe an extremal segment of an observational
record.  Its invariance expresses only that all admissible observers must agree
on the amount of distinguishable structure between neighboring events.}

The refinement bound $\epsilon$ defines the smallest admissible increment of
distinguishability.  When propagated along an extremal path---one that
saturates the refinement bound---each observer records the same number of
$\epsilon$-increments.  This count defines the informational interval $\tau$.
Because $\tau$ represents the number of admissible refinements rather than a
metric distance, its invariance follows from the requirement that no observer
may introduce or remove distinguishable structure that is not supported by the
measurement record.

Let $dx^{\mu}$ and $dx'^{\mu}$ denote the infinitesimal labels assigned by two
admissible observers to the same pair of neighboring events.  Their coordinate
labels differ by a transformation
\[
dx'^{\mu} = \Lambda^{\mu}_{\ \nu}\, dx^{\nu},
\]
where $\Lambda^{\mu}_{\ \nu}$ preserves causal order in the sense of the Axiom
of Selection.  Although the observers assign different coordinates, they must
agree on the number of $\epsilon$-increments between the events; otherwise
their merged histories would violate global consistency.

This agreement is enforced by a bilinear form $g_{\mu\nu}$ satisfying
\[
\tau^2 = g_{\mu\nu}\, dx^{\mu} dx^{\nu}.
\]
Under the coordinate transformation, the metric transforms as
\[
g'_{\mu\nu} = \Lambda^{\alpha}_{\ \mu}\, 
             \Lambda^{\beta}_{\ \nu}\, g_{\alpha\beta}.
\]
Substituting the transformed variables into the definition of $\tau$ yields
\[
\tau'^2 = g'_{\mu\nu}\, dx'^{\mu} dx'^{\nu}
        = g_{\alpha\beta}\, dx^{\alpha} dx^{\beta}
        = \tau^2.
\]

The invariance of $\tau$ thus expresses a simple but fundamental principle:
every admissible observer must assign the same number of distinguishable
increments to an extremal path.  Their coordinate descriptions may vary, but
the informational content of the path does not.

This invariance is the basis of the metric gauge introduced in
Section~\ref{sec:metric}.  It ensures that $\tau$ may serve as the universal
measure of informational separation, independent of the observer’s local
labeling conventions.  Section~\ref{sec:metric-bilinear} develops the metric
$g_{\mu\nu}$ as the bilinear form that enforces this invariance in the
continuous shadow.


\subsection{$g_{\mu\nu}$ as the Bilinear Form Preserving the $\epsilon$--Refinement Count}
\label{sec:metric-bilinear}

\NB{The metric $g_{\mu\nu}$ is not a geometric field on a manifold.  It is the
continuous shadow of the rule ensuring that all admissible observers preserve
the same count of $\epsilon$--sized refinements between neighboring events.
The components of $g_{\mu\nu}$ do not describe a physical medium or curvature;
they encode the invariant comparison rule required by informational
consistency.}

The interval $\tau$ defined in Section~\ref{sec:tau-invariance} expresses
the number of $\epsilon$--refinements along an extremal segment of the
measurement record.  Since this number must remain invariant under all
admissible relabelings of events, there must exist a bilinear form
$g_{\mu\nu}$ such that
\[
\tau^{2} = g_{\mu\nu}\, dx^{\mu} dx^{\nu}
\]
holds for every observer.  This expression is not a postulate but the unique
structure that enforces the preservation of $\tau$ under coordinate changes
that respect causal order.

To see this, consider two observers who assign infinitesimal labels
$dx^{\mu}$ and $dx'^{\mu} = \Lambda^{\mu}_{\ \nu}\, dx^{\nu}$ to the same pair of
neighboring events.  The Law of Causal Transport requires
\[
\tau'^2 = \tau^2,
\]
so we must have
\[
g'_{\mu\nu}\, dx'^{\mu} dx'^{\nu}
=
g_{\mu\nu}\, dx^{\mu} dx^{\nu}.
\]
Substituting $dx'^{\mu}$ and requiring equality for all admissible
transformations $\Lambda^{\mu}_{\ \nu}$ yields the transformation rule
\[
g'_{\mu\nu}
=
\Lambda^{\alpha}_{\ \mu}\,
\Lambda^{\beta}_{\ \nu}\,
g_{\alpha\beta}.
\]
Thus the metric is exactly the object that ensures agreement on the number of
$\epsilon$--sized refinements between neighboring events, regardless of the
coordinates used to describe them.

In this informational framework, $g_{\mu\nu}$ plays a role analogous to that of
a gauge potential: it specifies how infinitesimal refinements are compared
locally so that the global invariant $\tau$ remains unchanged.  The metric does
not specify “distance’’ in any geometric or physical sense; it enforces the
equivalence of all admissible measurement conventions.

Once $g_{\mu\nu}$ is introduced, the need to propagate these comparison rules
from point to point forces a unique notion of compatibility.  This requirement
determines the affine connection in Section~\ref{sec:connection} through the
condition
\[
\nabla_{\lambda} g_{\mu\nu} = 0,
\]
which expresses that the metric gauge is preserved under refinement and
transport.  The next section illustrates this invariance with a concrete
thought experiment.

\begin{phenomenon}[The Michelson--Morley Effect~\cite{michelson1887}]
\label{sec:michelson-morley}
\NB{This phenomenon is not interpreted as a physical test of ether
hypotheses, relativistic postulates, or the dynamics of light. It is
treated purely as an informational experiment: a demonstration that
distinguishable events may propagate through a region in which no medium
is observed. The null result is therefore a statement about the structure
of admissible refinements and boundary conditions, not about physical
substrates.}
\NB{This thought experiment does not appeal to optical physics, wave
interference, or the existence of a medium.  It is a finite informational model
illustrating that the metric gauge must assign the same refinement cost
$\epsilon$ to extremal paths in all admissible directions.  No physical claims
about light or propagation are implied.}

Consider an observer attempting to refine two extremal segments of equal
informational content, but aligned in different coordinate directions.  Let
$dx^{\mu}$ and $dy^{\mu}$ denote the local labels assigned to the two
segments.  Each segment is chosen such that its refinement requires the same
number of $\epsilon$--increments when described in the observer's own frame.

Now suppose the observer rotates their coordinate system.  After rotation, the
new labels are $dx'^{\mu} = \Lambda^{\mu}_{\ \nu} dx^{\nu}$ and
$dy'^{\mu} = \Lambda^{\mu}_{\ \nu} dy^{\nu}$.  The rotation
$\Lambda^{\mu}_{\ \nu}$ preserves causal order, so it is an admissible
transformation.  The question is whether the observer must still assign the
same informational interval $\tau$ to both segments after the rotation.

The Law of Causal Transport requires that the $\epsilon$--refinement counts for
both segments remain invariant:
\[
\tau_x^{2}
=
g_{\mu\nu}\, dx^{\mu} dx^{\nu},
\qquad
\tau_y^{2}
=
g_{\mu\nu}\, dy^{\mu} dy^{\nu}.
\]
After rotation, the transformed intervals are
\[
\tau_x'^{\,2}
=
g'_{\mu\nu}\, dx'^{\mu} dx'^{\nu},
\qquad
\tau_y'^{\,2}
=
g'_{\mu\nu}\, dy'^{\mu} dy'^{\nu}.
\]
Substituting the transformation rules for $dx'^{\mu}$, $dy'^{\mu}$, and
$g'_{\mu\nu}$ gives
\[
\tau_x'^{\,2}
=
g_{\alpha\beta}\, dx^{\alpha} dx^{\beta}
=
\tau_x^{2},
\qquad
\tau_y'^{\,2}
=
g_{\alpha\beta}\, dy^{\alpha} dy^{\beta}
=
\tau_y^{2}.
\]

Thus the observer must continue to assign the same informational interval to
the two extremal segments under any admissible rotation.  There is no freedom
to deform the refinement counts directionally: doing so would imply that
$\epsilon$--sized increments depend on orientation and would violate the
requirement that informational refinement be globally coherent.

This invariance is the informational analogue of isotropy.  It expresses that
the metric gauge $g_{\mu\nu}$ must refine extremal paths uniformly in all
directions: the number of $\epsilon$--increments needed to resolve a segment of
given informational content cannot depend on the coordinate orientation.

The Michelson--Morley experiment is therefore understood here not as a test of
a physical medium, but as a finite illustration of the isotropy of the metric
gauge.  The invariance of $\tau$ under rotations forces $g_{\mu\nu}$ to encode
a direction--independent refinement rule.  Section~\ref{sec:connection} develops
the compatible connection that propagates this rule under changes of frame.
\end{phenomenon}

\begin{phenomenon}[The Traffic Effect]
Light propagating through a region of elevated informational stress requires
additional refinement steps to preserve admissibility.  The resulting delay
is not a failure of transmission, but a bookkeeping cost.

The metric $g_{\mu\nu}$ acts as a gauge of informational separation.  In
stressed regions, the ledger must insert additional ticks in order to
transport a refinement across the same coordinate distance.  The observed
time delay is the accumulation of these additional admissible refinement
events.

The delay therefore measures not distance, but the increased cost of
maintaining consistency of the informational interval under transport.
\end{phenomenon}



\section{The Connection as Informational Bookkeeping}
\label{sec:connection}

\NB{The affine connection $\Gamma^{\lambda}_{\mu\nu}$ is not a force field or a
physical interaction.  It is the continuous shadow of an informational rule:
the minimal differential adjustment required to preserve the metric gauge
$g_{\mu\nu}$ as an observer moves from one event to its neighbor.  Its role is
purely kinematic.  The connection records how local measurement conventions
must tilt to maintain the invariant interval $\tau$; no dynamical content or
geometric ontology is assumed.}

The metric $g_{\mu\nu}$, introduced in Section~\ref{sec:metric}, guarantees that
all admissible observers assign the same informational interval $\tau$ to an
extremal displacement at a single event.  This invariance is enforced by the
bilinear form $g_{\mu\nu}\, dx^{\mu} dx^{\nu}$, which preserves the
$\epsilon$--refinement count under changes of coordinates.  However, the metric
by itself does not specify how these comparison rules extend from one event to
its neighbors.  To describe how distinguishability is maintained along a path,
we require a differential notion of consistency.

The connection $\Gamma^{\lambda}_{\mu\nu}$ provides this rule.  It specifies how
tensor components must be adjusted when an observer translates a local
measurement convention from one event to an infinitesimally adjacent one.  In
particular, the connection determines the covariant derivative, which measures
change in a way that respects the metric gauge.  Imposing that the metric
remain invariant under such differential updates leads to the condition
$\nabla_{\lambda} g_{\mu\nu} = 0$, known as covariant constancy of the metric.

In the informational picture, this condition is the statement that the act of
refinement may not create or destroy distinguishable structure as an observer
moves through the network of events.  The connection is the unique differential
bookkeeping device that satisfies this constraint.  When the metric is uniform,
the connection vanishes and no adjustment is needed: straight paths remain
informationally straight.  When the metric varies, a nonzero connection encodes
how local gauges must be rotated and rescaled so that scalar quantities built
from $g_{\mu\nu}$ remain unchanged.

The remainder of this section develops the connection as the compatibility
condition implied by covariant constancy of the metric and interprets parallel
transport as the differential expression of Martin consistency.  In this way,
the Law of Causal Transport acquires its full kinematic content: it is the rule
that propagates the gauge of separation through the continuous shadow of the
Causal Universe Tensor.

\begin{phenomenon}[The Sagnac Effect]
When refinement clocks are transported around a closed loop, global
synchronization fails.  The connection $\Gamma$ adjusts local refinement
rates to maintain admissibility, but the adjustments do not close under
cyclic transport.

Two refinement paths that traverse the same boundary in opposite directions
accumulate unequal refinement tallies.  This asymmetry is not a defect of
propagation, but the holonomy of the bookkeeping rule.

The observed time difference is the irreducible gap produced when a local
transport rule cannot be extended consistently around a closed causal
cycle.
\end{phenomenon}

\begin{phenomenon}[The Tail-Latency Effect]
\label{ph:tail-latency}

\textbf{Statement.}
Latency in an admissible region increases with both the number of active
causal connections and the surface measure of the region through which
refinements must be transported.

\textbf{Mechanism.}
Each admissible refinement must be reconciled across all attached causal
interfaces.  Let $N$ denote the number of active connections incident on a
region $\Omega$, and let $|\partial \Omega|$ denote the surface measure of its
boundary.  The cost of transport is not determined by the shortest path, but
by the slowest admissible reconciliation.

The tail of the latency distribution is therefore governed by
\[
\mathcal{L}_{\mathrm{tail}} \propto N \cdot |\partial \Omega|.
\]

\textbf{Interpretation.}
Transport in the causal ledger is not limited by average throughput but by
worst-case synchronization.  Each additional connection increases the number
of constraints that must be satisfied, and each increase in boundary area
expands the number of admissible reconciliation paths.

Latency therefore accumulates geometrically: wide interfaces and dense
connectivity do not accelerate refinement, they delay it.  The slowest
boundary dominates the admissible update rate.

This is not a property of signal speed.  It is a bookkeeping constraint: the
ledger cannot commit a refinement until every connected boundary can be
reconciled without contradiction.
\end{phenomenon}

\begin{phenomenon}[The Halt Effect.]
Not every admissible refinement admits a successor.  There exist boundary
configurations for which no further consistent update can be constructed.

If a partial ledger extension would require the separation of correlated
events without a permissible ordering, the update operator has no admissible
output.  The refinement process halts.

This is not a failure of computation but a structural limit of admissibility.
A halted ledger is not incomplete; it is complete in the only sense allowed by
the axioms.  No further event can be appended without violating global
consistency.

The halting of a causal sequence is therefore not destruction.  It is the
formal termination of admissible history.
\end{phenomenon}


\subsection{Covariant Constancy and the Compatibility Condition}
\label{ec:covariant-constancy}

\NB{Covariant constancy is not a physical conservation law.  It is the
informational requirement that the metric gauge $g_{\mu\nu}$, which preserves
the $\epsilon$--refinement count at a single event, must continue to preserve
that count as the observer moves to a neighboring event.  The affine connection
$\Gamma^{\lambda}_{\mu\nu}$ is therefore not introduced by assumption; it is
forced by the requirement that informational invariants remain invariant under
differential refinement.}

The metric $g_{\mu\nu}$ ensures that all admissible observers agree on the
informational interval $\tau$ at a point.  But as the observer moves from an
event $x$ to a nearby event $x + dx$, the local coordinate basis changes.
Under such a shift, the numerical components of $g_{\mu\nu}$ may appear to
change due to the alteration in basis, even if the underlying structure of
distinguishability remains the same.  To prevent this apparent change from
contaminating the informational interval, the transformation of $g_{\mu\nu}$
must be corrected by an additional adjustment term.

This correction is encoded by the covariant derivative.  The condition that the
metric gauge remain invariant under differential displacement is expressed as
\[
\nabla_{\lambda} g_{\mu\nu}
=
\partial_{\lambda} g_{\mu\nu}
-
\Gamma^{\sigma}_{\mu\lambda} g_{\sigma\nu}
-
\Gamma^{\sigma}_{\nu\lambda} g_{\mu\sigma}
=
0.
\]
The partial derivative $\partial_{\lambda} g_{\mu\nu}$ captures how the metric
components vary when written in the shifted coordinate system.  The remaining
terms subtract off this apparent variation by compensating for the tilt and
scale of the basis vectors themselves.  The equation $\nabla_{\lambda} g_{\mu\nu} = 0$
thus expresses the requirement that the informational interval $\tau$ remain
unchanged under any infinitesimal update of the observational coordinates.

This compatibility condition uniquely determines the connection when torsion is
absent.  As established in Chapter~3, the spline representation of admissible
histories carries no fourth--order freedom and is therefore torsion-free.  Under
this constraint, the metric compatibility condition fixes the connection to be
the Levi--Civita connection:
\[
\Gamma^{\lambda}_{\mu\nu}
=
\frac{1}{2}\,
g^{\lambda\sigma}
\left(
\partial_{\mu} g_{\nu\sigma}
+
\partial_{\nu} g_{\mu\sigma}
-
\partial_{\sigma} g_{\mu\nu}
\right).
\]
This expression is not a postulate; it is the only operator that ensures the
metric gauge remains intact under transport.  It is the continuous shadow of
the discrete requirement that refinement cannot introduce or eliminate
distinguishable structure beyond the $\epsilon$ bound.

With the connection now fixed by kinematic necessity, we may interpret its role
operationally.  The connection coefficients specify the adjustments required to
compare tensorial quantities at neighboring events, ensuring that the
informational interval $\tau$ and the refinement bound $\epsilon$ remain
consistent throughout the observer’s path.  The next subsection formalizes this
process as parallel transport.


\subsection{Parallel Transport as Differential Martin Consistency}
\label{sec:parallel-transport}

\NB{Parallel transport is not a physical motion of a vector through space.  It
is the informational requirement that the meaning of a direction---a rule for
distinguishing one infinitesimal refinement from another---remain consistent as
an observer updates coordinates from one event to the next.  In this framework,
a “vector’’ is an instruction for refinement, and parallel transport ensures
that such instructions are not distorted by changes in local labeling
conventions.}

The metric compatibility condition $\nabla_{\lambda} g_{\mu\nu} = 0$ determines
how the metric must be preserved under infinitesimal displacement.  Parallel
transport extends this requirement to all tensorial quantities, ensuring that
any object used to encode refinements of the observational record is carried
through the continuous shadow without introducing contradictions.

Let $V^{\mu}$ represent such a refinement direction.  When an observer moves
along a curve $x^{\mu}(s)$ in the event network, the numerical components of
$V^{\mu}$ change because the local coordinate basis changes.  The naive
derivative $d V^{\mu} / ds$ therefore incorporates both the intrinsic change in
the refinement direction and the apparent change induced by the shifting
coordinates.  To isolate the intrinsic change---the part that affects
distinguishability---we must subtract off the bookkeeping contribution provided
by the connection.

The covariant derivative along the path is thus defined as
\[
\frac{D V^{\mu}}{D s}
=
\frac{d V^{\mu}}{d s}
+
\Gamma^{\mu}_{\ \nu\lambda}\,
V^{\nu}\,
\frac{d x^{\lambda}}{d s}.
\]
Parallel transport requires that the intrinsic change vanish:
\[
\frac{D V^{\mu}}{D s}
=
0.
\]
This equation expresses the differential form of Martin consistency.  It states
that the instruction encoded by $V^{\mu}$ must retain its informational meaning
as the observer moves.  All coordinate-induced distortions of $V^{\mu}$ must be
canceled by the corresponding connection terms, ensuring that the refinement
direction does not acquire unrecorded structure.

The geometric interpretation of parallel transport as preserving “straightness’’
is replaced here by a purely informational one: parallel transport guarantees
that refinement instructions remain compatible with the metric gauge
$g_{\mu\nu}$ throughout the observer’s path.  Whenever the metric varies from
event to event, the connection coefficients encode the cost of adjusting the
observer’s basis to ensure that scalar comparisons built from $g_{\mu\nu}$ and
$V^{\mu}$ remain invariant.

In regions where $g_{\mu\nu}$ is uniform, the connection vanishes and the
informational meaning of $V^{\mu}$ is preserved without adjustment.  Where
$g_{\mu\nu}$ varies, nonzero connection coefficients encode the minimal
bookkeeping needed to keep the refinement count consistent.  This adjustment is
the kinematic origin of effects such as frequency shifts between observers in
different informational environments, which we examine in the following
subsection.

\section{Pendula}
\begin{phenomenon}[The Foucault Effect]
A refinement direction transported around a closed causal loop does not
generally return to its initial orientation.  The failure of closure is not a
mechanical torque, but the accumulated informational strain required to
preserve admissibility under transport.

The observed precession is the holonomy of an inconsistent connection: a
closed path in events produces an open path in orientation.
\end{phenomenon}


\section{Refinement--Adjusted Transport}
\label{sec:refinement-transport}

\NB{The frequency shift examined in this section is not a postulated effect.
It is the kinematic consequence of maintaining the invariant informational
interval $\tau$ across regions in which the metric gauge $g_{\mu\nu}$ varies.
No physical ontology is assumed.  The observable change in clock rates reflects
the differential bookkeeping enforced by the connection $\Gamma^{\lambda}_{\mu\nu}$.}

The previous sections established the chain of informational structure:
the refinement bound $\epsilon$ fixes the local increment of distinguishable
structure; the metric $g_{\mu\nu}$ expresses how these increments are compared
between observers; and the connection $\Gamma^{\lambda}_{\mu\nu}$ preserves this
comparison under differential displacement.  When the metric varies from one
location to another, this preservation requires that the local rate of event
counting---the clock frequency---adjusts so that the invariant interval remains
consistent across observers.

This section derives that adjustment and exhibits its observable consequence.

\subsection{The Invariant Causal Tally}
\label{sec:causal-tally}

\NB{An atomic clock does not measure a geometric length or a physical time.
It measures a count of distinguishable events.  The proper interval $\tau$ is
the continuous shadow of this count, expressed in units of the refinement bound
$\epsilon$.}

Consider an observer whose worldline is described by coordinates $(t, x^{i})$.
If the observer is at rest in their coordinate system ($dx^{i} = 0$), the
informational interval between neighboring events satisfies
\[
d\tau^{2} = g_{00}(x)\, dt^{2}.
\]
Thus the locally measured period of the clock is
\[
d\tau = \sqrt{g_{00}(x)}\, dt.
\]
Because $\tau$ counts $\epsilon$--sized refinements, the local clock frequency
$\nu(x)$ is inversely proportional to the size of this interval:
\[
\nu(x) = \frac{1}{d\tau}
        = \frac{1}{\sqrt{g_{00}(x)}}\,\frac{1}{dt}.
\]

Two observers at rest in different metric gauges therefore experience different
informational intervals for the same coordinate increment $dt$.  The
relationship between their locally recorded counts is fixed entirely by the
metric gauge.

\subsection{Derivation of Frequency Adjustment}
\label{sec:frequency-adjustment}

\NB{The global parameter $t$ is not a physical time.  It is the auxiliary
labeling parameter that all admissible observers must agree upon when their
records are merged.  Its increments must match across observers in order for
their $\epsilon$--counts to be reconciled.}

Let observers $A$ and $B$ be stationary in regions with metric components
$g_{00}(A)$ and $g_{00}(B)$.  Over a shared coordinate increment $\Delta t$,
their locally recorded proper intervals are
\[
\Delta \tau_{A} = \sqrt{g_{00}(A)}\, \Delta t,
\qquad
\Delta \tau_{B} = \sqrt{g_{00}(B)}\, \Delta t.
\]
Since a clock’s frequency is the inverse of the proper interval it records,
\[
\nu_{A}
=
\frac{1}{\Delta \tau_{A}}
=
\frac{1}{\sqrt{g_{00}(A)}}\,\frac{1}{\Delta t},
\qquad
\nu_{B}
=
\frac{1}{\sqrt{g_{00}(B)}}\,\frac{1}{\Delta t}.
\]

The ratio of their observed frequencies is therefore
\[
\frac{\nu_{A}}{\nu_{B}}
=
\frac{\sqrt{g_{00}(B)}}{\sqrt{g_{00}(A)}}.
\]
This expression is the kinematic consequence of the Law of Causal Transport.
When $g_{00}$ varies, the connection $\Gamma^{0}_{00}$ compensates by adjusting
the local rate of $\epsilon$--counting so that the merged observational record
remains consistent.  The observed frequency shift is thus the operational
signature of nonzero connection coefficients.

\section{Time Dilation}

The informational framework developed in Chapters~5 and~6 places a subtle
constraint on how refinement may be transported across a causal network.
Proper time is not a geometric parameter but the tally of irreducible
distinctions, and the metric $g_{\mu\nu}$ records how this tally must adjust
when two histories inhabit regions with different curvature residue.
Whenever distinguishability is carried from one domain to another, the
connection enforces a compatibility rule: the informational interval must be
preserved even if the local refinement structure differs.

This requirement has a striking observable consequence.  Two clocks placed
at different informational potentials---that is, in regions where the
residual strain of admissible curvature differs---cannot maintain the same
rate of refinement.  Each clock is internally consistent, but the comparison
of their records forces an adjustment.  A refinement sequence that is
admissible at one potential must be reweighted when interpreted at another,
or else the causal record would fail to merge coherently.

In the smooth shadow, this bookkeeping adjustment becomes the familiar
phenomenon of gravitational redshift.  Signals transported upward appear to
lose frequency; signals transported downward appear to gain it.  Nothing
mystical is occurring: the informational interval is being preserved, and the
only available mechanism is a change in the rate at which distinguishability
is accumulated.

The Pound--Rebka experiment is therefore the archetype of an informational
outcome.  It demonstrates that when refinement is compared across regions
with differing curvature residue, the universe must adjust the apparent rate
of time itself to maintain consistency.  No dynamical field need be invoked;
the redshift is simply the shadow of the constraint that admissible
refinements must agree on their causal overlap.


\begin{phenomenon}[The Pound--Rebka Effect~\cite{pound1959}]

\NB{The following is an \emph{informational phenomenon}.  No physical
mechanism is assumed.  The interpretation concerns how the gauge of
informational separation $g_{\mu\nu}$ adjusts refinement counts when
distinguishability is transported across domains of differing causal
potential.  Any resemblance to the gravitational redshift measured by
Pound and Rebka is a consequence of the informational shadow, not an
assumed dynamical cause.}

The Axiom of Peano defines proper time as the count of irreducible refinements
along an admissible history.  The Law of Causal Transport guarantees that
this count is invariant under maximal propagation, while the informational
metric $g_{\mu\nu}$ (Section~5.2) records how successive refinements compare
when transported across regions whose admissible histories differ in their
curvature residue.

Consider two clocks: one at a lower informational potential (higher curvature
residue) and one at a higher potential (lower residue).  Both clocks produce
sequences of refinements
\[
  \langle e_1 \prec e_2 \prec \cdots \rangle_{\text{low}},
  \qquad
  \langle f_1 \prec f_2 \prec \cdots \rangle_{\text{high}},
\]
each internally consistent.  However, the Law of Boundary Consistency demands
that refinements compared across their shared causal overlap must agree on
their informational interval.  When the refinement sequence from the lower
clock is transported to the higher clock, the compatibility condition forces
an adjustment in the rate at which distinguishability is accumulated.

Formally, transport along a connection with residue $\Gamma$ alters the
frequency of refinements according to the first--order compatibility
condition of Section~5.4:
\[
  \nu_{\text{high}}
  = \nu_{\text{low}}\, (1 - \Gamma\,\Delta h),
\]
where $\Delta h$ is the informational separation between the clocks.
This is the informational analogue of the frequency shift that appears in
the smooth limit as gravitational redshift.

In the Pound--Rebka configuration, a photon (interpreted here as a unit of
transported distinguishability) sent upward from the lower clock must be
refined in such a way that its informational interval remains constant.
Since admissible refinements at higher potential accumulate fewer curvature
corrections, the transported signal must appear at a lower frequency when
measured by the upper clock.  Conversely, a downward signal appears at a
higher frequency.  No physical field is invoked: the effect is a bookkeeping
adjustment required to maintain Martin--consistent transport of
distinguishability across regions of differing curvature residue.

Thus the informational framework predicts a frequency shift of the form
\[
  \frac{\Delta \nu}{\nu} \approx \Gamma\,\Delta h,
\]
which matches the structure of the Pound--Rebka observation when interpreted
in the smooth shadow of the metric gauge.

The phenomenon of time dilation is therefore an observable outcome of the
informational interval and the necessity of refinement--adjusted transport.
Differences in curvature residue force clocks at different potentials to
accumulate distinguishability at different rates, and the comparison of
their refinement counts produces the celebrated redshift.

\end{phenomenon}



\begin{coda}{The Kinematic Foundation of Geometry}
\label{sec:kinematic-closure}

\NB{This chapter derived the continuous kinematic structures---the metric
$g_{\mu\nu}$ and the connection $\Gamma^{\lambda}_{\mu\nu}$---from the
informational requirement that refinements remain globally consistent.  No
forces, fields, or dynamical assumptions were introduced.}

The development of this chapter followed the informational chain of emergence:
\[
\epsilon
\;\longrightarrow\;
\tau
\;\longrightarrow\;
g_{\mu\nu}
\;\longrightarrow\;
\Gamma^{\lambda}_{\mu\nu}.
\]
The refinement bound $\epsilon$ fixed the minimal increment of admissible
structure.  The interval $\tau$ encoded the invariant tally of such increments.
The metric $g_{\mu\nu}$ enforced this invariance across observers, and the
connection $\Gamma^{\lambda}_{\mu\nu}$ preserved it under differential
refinement.  The observable consequence of this structure is the redshift
effect, where nonuniformity of the metric gauge requires a corresponding
adjustment of the local $\epsilon$--counting rate.

\begin{phenomenon}[The Event Horizon Effect]
\label{ph:event-horizon}
\NB{In ordinary space, you approach the event horizon.  In an informational
black hole, the event horizon approaches you.}
A black hole is not a geometric singularity but an informational bottleneck.

The metric $g_{\mu\nu}$ functions as a gauge of informational separation, and
the connection $\Gamma$ is the bookkeeping rule that adjusts local refinement
rates in order to preserve the invariant informational interval $\tau$
(Sections~5.2--5.3) :contentReference[oaicite:0]{index=0}.  Transport is admissible only so long as this gauge can be
maintained at finite cost.

At an event horizon this cost diverges.  To export a single distinguishable
refinement from the interior requires an unbounded number of coordinate-time
updates.  The exchange rate of admissible refinements collapses to zero.

The classical singularity is therefore not a failure of physics but a failure
of mergeability: the causal universe tensor can no longer reconcile the
internal and external ledgers in finite informational time.

The interior record continues to refine, but its updates can no longer be
interleaved with the external history.  A black hole is thus not a hole in
space, but a latency horizon in the bookkeeping of causal order.
\end{phenomenon}

The event horizon represents a \emph{local} saturation of the transport
budget.  It is the point at which the informational cost of exporting a
refinement diverges with respect to an external region.  This divergence does
not require curvature to be extreme everywhere; it arises whenever the
connection can no longer preserve the invariant interval under admissible
exchange.

This observation admits a broader question.  If a finite region of the causal
ledger can exhaust its outward bandwidth, then a complete ledger --- the
entire admissible causal universe --- must also possess a maximal transport
capacity.  The issue is therefore not whether horizons exist, but whether a
global horizon is forced by the finiteness of refinement itself.

The local phenomenon thus points to a global constraint: if the universe is a
finite, admissible record, then there must exist a critical scale at which the
cost of exporting any further refinement diverges.  This is not a geometric
assumption, but a bookkeeping necessity.

The following phenomenon makes this limit explicit.

\begin{phenomenon}[The Schwarzschild Effect~\cite{schwarzschild1916}]
\label{ph:schwarzschild-limit}

\textbf{Statement.}
There exists a critical informational radius beyond which refinements cannot
be exported in finite time.  This radius is the Schwarzschild limit of the
causal ledger.

\textbf{Classical Shadow.}
In general relativity, the Schwarzschild radius
\[
r_s = \frac{2GM}{c^2}
\]
defines the surface at which the escape cost of light becomes infinite.  In
this framework, the same quantity appears not as a geometric boundary, but as
an informational one.

\textbf{Informational Interpretation.}
Let $M_U$ denote the total admissible mass--energy of the causal record and
define the corresponding informational radius
\[
r_U = \frac{2G M_U}{c^2}.
\]
This radius marks the point at which the cost of transporting a single
distinguishable refinement from the interior to any hypothetical exterior
diverges.

At this limit, the connection $\Gamma$ can no longer preserve the invariant
informational interval under transport.  The exchange rate of admissible
events collapses to zero.  The interior ledger may continue to refine, but its
updates cannot be merged with any external record within finite refinement
time.

\textbf{Consequence.}
The universe does not sit inside a black hole.  Rather, the universe
\emph{is} the maximal admissible ledger: a region whose informational content
is bounded by a horizon defined by its own total refinement budget.

The Schwarzschild limit therefore measures not the curvature of space, but the
maximum outward bandwidth of causally admissible history.

\end{phenomenon}


This completes the kinematic description of informational geometry.  The next
chapter introduces the dynamic concept of curvature, defined as the obstruction
to transporting refinement instructions consistently around a closed loop.  In
this way, the ``Curvature of Information'' becomes the natural extension of the
kinematic structures developed here.
\end{coda}



