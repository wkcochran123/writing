\chapter{Motion}

Motion requires a mechanism for comparing one record of distinction with another. In the
causal framework, that mechanism is the clock: a source of admissible refinements that
allow two observers to construct ordered sequences of events. A clock does not measure
duration, velocity, or geometry. It only asserts that one event occurred after another, and
that this ordering is invariant under refinement. Its utility is purely ordinal. Whether the
clock is an atomic transition, a quartz oscillator, a binary counter, or a list of threshold
crossings, its output is the same kind of object: a finite record of distinguishable ticks.

\begin{definition}[Clock (non-standard)]
\label{def:clock}
\NB{This is a \emph{formal} definition of a clock. No physical assumption is made that
clocks exist, or that any particular mechanism generates the ticks. In this framework, a
clock is only a logical instrument that emits distinguishable events, allowing the causal
order to be indexed. Its existence is a definition inside the mathematics of admissible
distinctions, not a physical postulate about quantum systems, atomic transitions, or
relativistic matter.}

A \emph{clock} is an instrument that emits a sequence of distinguishable events. Each emitted
event is admissible under Axiom~\ref{ax:planck}: it produces a finite refinement of the causal
record. A clock is therefore not a continuous variable or a dynamical law; it is a device
that guarantees the existence of a countable chain of ordered distinctions. The function
of a clock is to certify an ordering on the events of a measurement, nothing more.
\end{definition}

From this perspective, a clock is not a dynamical primitive. It is a logical instrument. The
act of ticking establishes a chain of events, and the absence of extra ticks is a data
constraint. If a clock recorded no intermediate events between two ticks, then no
admissible description may contain structure that would have produced one. In particular,
acceleration, oscillation, or curvature that would create additional ticks are ruled out by
informational minimality. Motion is therefore not inferred from a continuous trajectory,
but from the consistency of the tick record itself.

Because clocks produce ordered events, two observers may compare their records by
merging their tick sequences under global coherence. When the merge produces no
contradiction, a single coherent history exists, and the count of ordered refinements
defines the relative motion of their systems. In the smooth limit, the unique continuous
interpolant between ticked events is the cubic extremal with no unobserved structure. Thus,
classical kinematics is the shadow of a discrete bookkeeping process: a clock provides
order, informational minimality removes hidden curvature, and the continuum appears only
as the completion of finite refinements.

In what follows, motion will be defined as the reconciliation of two causal records produced
by clocks. Relative velocity, proper time, and inertial behavior arise not from geometry or
differential equations, but from the minimal continuous shadow consistent with their
countable tick sequences. Motion is what ordered distinction looks like when refinement
tends to the smooth limit.

\begin{example}[Laser Tracking and Informational Dilation]
\label{te:laser-tracking}
Two identical observers, A and B, begin co-located with synchronized clocks. Observer B
embarks on a journey involving periods of acceleration, while observer A remains at the
origin of an idealized inertial frame. We \emph{explicitly neglect} the gravitational and
relativistic influence of Earth, the Sun, Sagittarius~A*, and all other bodies; spacetime is
treated as Minkowski over the region of interest.

Rather than waiting for reunion, A continuously tracks B by emitting a stream of
monochromatic laser pulses. Each pulse is timestamped in A’s notebook when fired, and
timestamped again when the reflected pulse is received from B’s retroreflector.

Every fired pulse is a distinguishable event; every received pulse is another. If B follows a
complicated accelerative path, then the return times of the pulses form a more densely
refined sequence than the symmetric record A would observe if B were inertial. The point
is not energy or Doppler shift. The informational content of the record increases: each
round-trip establishes a new ordered pair of emission and reception, constraining B’s
admissible motion.

If B were inertial, the spacings of the returned timestamps would follow the unique minimal
interpolant that introduces no unobserved curvature. But acceleration forces extra
refinements: the return times become uneven in a way that cannot be reconciled with a
coasting trajectory. These “irregularities’’ are not interpreted through differential
equations; they are simply distinct events that must be merged into A’s causal record.

When B returns, both observers merge their sequences. A’s laser notebook contains a much
longer chain: every emission and every reflection has already placed constraints on B’s path.
B’s local clock, by contrast, has recorded only its own internal ticks and those refinements
forced by onboard events. The merge therefore requires A to reconcile a larger
informational workload, while B performs a smaller one. Consistent ordering assigns the
larger count of admissible distinctions to A, and the smaller to B. The result is that A’s
proper time is larger---she has the denser causal record.

In the smooth limit, the same count enforces the classical dilation formula of relativity. But
here the conclusion is purely informational: acceleration introduces refinements, refinements
create more events, and more events imply more work when histories are coherently merged.
Time dilation is the bookkeeping of laser-certified distinctions, not a geometric postulate.

This informational mechanism therefore recovers the ability to compute the Lorentz
contraction posed in Thought Experiment~\ref{te:boosting-velocity} through the update rule
\(E_k = \Psi\!\bigl(e_k \cap \Rhat(e_{k-1})\bigr)\), using only the observers’ laboratory notebooks.
\end{example}

\section{Relative Motion}
\label{se:relative-motion}

With a clock in hand, an observer constructs an ordered sequence of admissible events
\(\{e_k\}\). The Causal Universe Tensor encodes this growing record through the recursive
update
\[
E_k = \Psi\!\bigl(e_k \cap \Rhat(e_{k-1})\bigr),
\]
where \(\Rhat\) restricts all admissible continuations to those consistent with the most recent
distinguishable event. For a single inertial frame, this produces a monotone refinement of
information: each step eliminates histories that would contradict the record.

To describe motion, no new structure is required. Consider two inertial observers, A and B,
each with their own clock and their own causal universe tensors:
\[
E^A_k = \Psi\!\bigl(e^A_k \cap \Rhat(e^A_{k-1})\bigr),
\qquad
E^B_\ell = \Psi\!\bigl(e^B_\ell \cap \Rhat(e^B_{\ell-1})\bigr).
\]
Each observer possesses a complete and self-consistent history of admissible distinctions.
Relative motion is nothing more than the reconciliation of these two histories under the rule
of global coherence. When A and B compare notebooks, every admissible refinement recorded
by one must also be admissible to the other. In the absence of contradiction, a merge exists,
and the merged list induces a partial ordering on the pair \(\{E^A_k\} \cup \{E^B_\ell\}\).

If the merged record is strictly longer than either individual record, the observers infer that
their clocks have accumulated refinements at different rates. By Axiom~\ref{ax:cantor}, the
merged history cannot discard any admissible distinction: every recorded refinement must be
preserved. Consequently, the observer whose notebook produces the \emph{denser} merged
record corresponds to the \emph{longer} proper time. The informational work of reconciling
those refinements is the measure of duration. Thus,
\begin{equation}
\lvert\,e^A_k \cap \Rhat(e^A_{k-1})\,\rvert > \lvert\,e^B_\ell \cap \Rhat(e^B_{\ell-1})\,\rvert
\end{equation}
implies that A has the longer proper time. No metric, coordinate chart, or differential
equation is required; the relative motion is encoded entirely in the informational asymmetry
of their merged causal histories.


In the smooth limit, the unique continuous interpolant of the merged record is the cubic
extremal with no unobserved structure. Classical kinematics---relative velocity, time dilation,
and Lorentz contraction---appears as the shadow of this merge. The Causal Universe Tensor
does not simulate motion; it enforces consistency. Relative motion is what two coherent
universe tensors look like when compared under refinement.

\subsection{Merging a Single Event}
\label{se:merging-single-event}

Referring to Thought Experiment~\ref{te:laser-tracking}, consider the merging of a single
event: the moment a reflected photon is absorbed by A’s detector. This absorption is a
distinguishable refinement of A’s record and therefore constitutes an admissible event
\(e^A_{k+1}\). The photon has traveled to B, interacted with the retroreflector, and returned.
Whatever else the experimenter may imagine, this exchange contains one certified fact: the
causal distance between A and B has changed in a way detectable by A’s clock.

In the language of the causal universe tensor, the absorption is merged via
\[
E^A_{k+1} = \Psi\!\bigl(e^A_{k+1} \cap \Rhat(e^A_k)\bigr).
\]
Nothing more is required. The event contributes only the distinction that A received a
photon at that moment. The return time rules out any hypothetical motion of B that would
have prevented this arrival, and it rules out any curvature or oscillation that would have
produced additional admissible pulses. The refinement therefore narrows A’s admissible
histories to those consistent with both emission and reception.

When B later inspects A’s notebook, the same absorption event must be admissible within
B’s causal universe tensor:
\[
E^B_{\ell+1} = \Psi\!\bigl(e^A_{k+1} \cap \Rhat(e^B_\ell)\bigr).
\]
If a contradiction were forced---for example, if B’s notebook implied the photon could not
have returned at that time---global coherence would fail, and the combined record would be
inadmissible. But if the merge succeeds, the joint history becomes strictly more refined,
and the updated tensors\footnote{No physical model of a photon is required. The “photon’’
only represents a distinguishable event transmitted between observers.} encode a new
restriction on their relative motion.

A single merged photon event therefore eliminates an entire family of hypothetical motions.
It narrows the admissible set of configurations and extends the causal record without
introducing any continuous structure. In the smooth limit, repeated merges of this form
force the cubic extremal between emission and reception times---the unique interpolant with
no unobserved structure. Classical distance, velocity, and Lorentz contraction appear as the
continuous shadow of this discrete bookkeeping.

\subsection{Measurement of Acceleration as Counts of Events}
\label{se:acceleration-as-counts}

Acceleration does not require forces, masses, or differential equations. In the causal
framework, acceleration is nothing more than a second refinement: a change in the
distinguishable difference between successive admissible events. To detect such a change, a
single measurement is insufficient. At least two refined measurements are needed so that the
difference between them can itself be distinguished.

Suppose A emits two photons at events \(e^A_k\) and \(e^A_{k+1}\), and later receives their
reflections at \(e^A_{k+r}\) and \(e^A_{k+s}\). Each absorption is merged by
\[
E^A_{k+r} = \Psi\!\bigl(e^A_{k+r} \cap \Rhat(e^A_{k+r-1})\bigr),
\qquad
E^A_{k+s} = \Psi\!\bigl(e^A_{k+s} \cap \Rhat(e^A_{k+s-1})\bigr).
\]
If B is in uniform motion relative to A, the refinements contributed by these two events are
consistent with a unique minimal interpolant: the admissible histories require that the
difference in reception times is itself constant under refinement. Any hidden curvature or
oscillation would have produced additional admissible events---extra pulses, missed reflections,
or altered return order---and is therefore ruled out by the Axiom of Planck.

However, if the spacing between \(e^A_{k+r}\) and \(e^A_{k+s}\) cannot be reconciled by a
single coasting history, then the admissible set must be further restricted. The causal
universe tensor eliminates all hypothetical configurations in which B remained inertial. What
remains are those histories in which the separation of the events changes in a way that is
itself distinguishable. The second refinement is the signature of acceleration.

In this sense, acceleration is not a postulated quantity. It is the discovery that two
refinements cannot be merged into a single coasting interpolant without contradiction. A
sequence of such measurements produces a chain of eliminations: each return time excludes
admissible events that would require an invisible change in curvature. The remaining histories
are the ones in which acceleration has occurred.

In the smooth limit, repeated second refinements force a unique continuous extremal whose
second variation is nonzero. Classical acceleration appears as the shadow of a finite
bookkeeping process: acceleration is the count of distinguishable failures of coasting to
explain the merged record. No forces, masses, or trajectories are assumed. The event counts
alone enforce curvature in the admissible histories.

Thus, we have recovered the ability to verify Newton’s second law---again, with apologies to
Lord Berkeley. Acceleration is not a substance but the second variation of admissible
refinements in the merged event record.

\subsection{The Equations of Motion}
\label{se:equations-of-motion}

The remainder of this chapter examines the equations of motion that arise when finite
records of admissible distinctions are merged without contradiction. Nothing further is
assumed. Each equation appears as the continuous shadow of informational minimality: the
unique smooth extremal that contains no unrecorded structure.

We begin with heat transport. Although commonly divided into conduction, convection,
and radiation, all three arise here from distinct constraints on admissible refinements.
When refinements diffuse symmetrically through a medium with no hidden variations, the
smooth limit forces the diffusion equation. When refinements are transported coherently
through the medium, the extremal satisfies the advective transport law. When refinements
propagate at the maximal admissible speed, the continuous shadow is radiative transport
governed by the wave equation. No model of heat is assumed; each law is simply the
completion of a finite notebook of events.

Annealing appears when a ledger repeatedly reconciles its own coarse description. The
iterated application of the merge operator eliminates sharp distinctions that would predict
unobserved refinements. As the sequence of folds converges, the smooth limit is diffusion.
Annealing is therefore informational smoothing: the heat equation is its continuous shadow
when the coarse ledger is refined to closure.

Adiabatic transport arises when the ledger evolves without creating or destroying admissible
distinctions. In the smooth limit, this invariance forces the classical adiabatic law.
Nothing dynamical is postulated; an adiabatic process is simply a sequence of refinements
that preserves the global count of admissible configurations.

Even quantum phenomena admit the same treatment. The Casimir effect appears when the
merged record forbids a continuous family of admissible configurations between two
boundaries. The elimination of those histories produces an informational pressure, and the
smooth limit recovers the familiar expression for Casimir energy.

Alpha decay appears in its original form: the Mott problem~\cite{mott1929}. The ledger of the nucleus
contains two nearly indistinguishable configurations---one in which the alpha cluster remains
bound, and one in which it escapes. Over time, these dual descriptions drift out of
alignment. The moment of decay is not the passage of a particle through a barrier, but the
repair of a contradiction: the merged record eliminates all configurations in which the two
descriptions diverge. The resulting refinement is recorded as a distinct decay event. In the
smooth limit, this informational repair produces the exponential law of radioactive decay
without invoking forces, potentials, or tunneling particles.  No dice are rolled.

In each case, the classical equation of motion is not assumed. It is what consistency looks
like in the smooth limit of finite measurement. Motion is bookkeeping; the laws that follow
are shadows of refinement.

\subsection{Martin’s Condition and the Propagation of Order}
\label{se:martins-condition}

Up to this point, motion has been defined locally: two observers exchange admissible
events, merge their records, and eliminate any hypothetical history that would have
produced unrecorded refinements. This closure guarantees that each observer maintains a
coherent ledger. It does not yet guarantee that their ledgers are mutually compatible.

For observable physics, local coherence is not enough. Distinct observers must be able to
reconcile their refinements along their shared boundary without introducing new
distinguishabilities. The requirement that every locally finite patch of causal order extends
to a globally consistent history is Martin’s Condition.

\begin{definition}[Martin’s Condition (Non-standard)]
\label{def:martins-condition}
A causal network (Definition~\ref{def:causal-network}) satisfies Martin’s Condition if every locally finite subset of events can be
extended to a globally consistent ordering without introducing new admissible distinctions.
Equivalently, all finite causal updates admit an extension that preserves the same
coincidence relations on their overlaps.
\end{definition}

Intuitively, Martin’s Condition demands that information created in one region does not
contradict information measured in another. It forbids causal overcounting---the duplication
of distinctions that would destroy reversibility---by ensuring that overlapping observers
reconstruct identical splines of the causal universe tensor along their shared boundary. The
Axiom of Event Selection limits what may happen within a light cone; Martin’s Condition
governs how those choices propagate outward.

Once Martin’s Condition holds, the closure of finite refinements induces a global
propagation rule. Locally symmetric overlaps enforce a second variation and yield the
wave operator. Oriented overlaps enforce a first variation and yield advection. When
variations are eliminated by repeated projection, the smooth limit is diffusion. The familiar
equations of motion---waves, advection, diffusion, and later curvature---are therefore the
continuous shadows of global consistency under Martin’s Condition.

\begin{example}[Davisson--Germer and the Universality of Causal Waves~\cite{davisson1927}]
\label{te:davisson-germer}

\emph{[Waves are the continuous shadows of Martin–consistent propagation]}

Imagine an electron gun firing individual electrons toward a crystalline
nickel target. A distant screen records the arrival of scattered electrons as
distinguishable events. Between gun, crystal, and screen, no internal
distinctions are measured; the observers record only the emission, the
scattering plane, and the pattern of impacts. Each detection on the screen
is therefore an admissible refinement of the joint causal ledger of gun,
crystal, and detector.

Under Martin’s Condition, every locally finite segment of this ledger must
extend to a globally consistent history. The crystal introduces a periodic
partition: successive lattice planes represent indistinguishable choices,
except at angles where the merged ledger would predict additional or
missing refinements. Along these planes, reciprocal measurement enforces
translation invariance: if one segment of the ledger is shifted by a lattice
spacing, the count of admissible refinements must remain unchanged.

The only smooth extremals compatible with this translation invariance are
wave modes. Among these, the constructive modes are precisely those whose
wavelength $\lambda$ satisfies Bragg’s relation~\cite{bragg1913}
\[
2 d \sin \theta = m \lambda,
\qquad
\lambda = \frac{h}{p},
\]
where $d$ is the lattice spacing, $\theta$ the scattering angle, $m$ an
integer, and $h/p$ encodes the count of distinguishabilities preserved along
the oriented Martin bridges. At those angles, no hidden refinements are
predicted; outside them, the merged ledger would contain missing or extra
distinguishable events, contradicting Martin’s Condition.

Operationally, the bright peaks on the screen are fixed points of reciprocal
measurement under lattice translations. What physicists call “electron
diffraction’’ is simply the bookkeeping consequence of demanding that
indistinguishable causal neighborhoods propagate consistently across the
crystal. No wavefunction is assumed. The ``wave’’ is the unique smooth
extension of discrete, Martin–consistent event counts.

Thus, the Davisson--Germer experiment does not demonstrate that electrons
are waves or particles. It demonstrates that any causal history satisfying
Martin’s Condition must propagate its indistinguishabilities as waves. The
universality of wave behavior is a consequence of global consistency, not a
special property of matter.
\end{example}

\section{The Algebra of Interaction}
\label{se:algebra-interaction}

Each system $X$ carries an accumulated causal universe tensor as a left--fold of update
factors:
\[
\U^X_{1} = E^X_{1},\qquad
\U^X_{n+1} = E^X_{n+1}\,\U^X_{n},\qquad
E^X_{n+1} := \Psi\!\bigl(e^X_{n+1} \cap \Rhat(e^X_{n})\bigr).
\]


\begin{definition}[Interaction operator]
\label{def:interaction-operator}
Given two ledgers (tensors) $\U^A$ and $\U^B$, the \emph{interaction operator}
\[
f:\ (\U^A,\U^B)\ \longmapsto\ \U^{AB}
\]
returns the minimal accumulated state $\U^{AB}$ that extends both inputs and is Martin–consistent
on their overlap. Equivalently, $\U^{AB}$ is obtained by left–folding the common update
factors (the jointly admissible events) in observed order so that no unrecorded refinements
are invented (Axiom of Planck) and none already recorded are erased (Axiom of Cantor).
Let $E(\U)$ denote the underlying event set of $\U$ and define the newly contributed
distinctions by
\[
\J^{AB}\ :=\ E(\U^{AB})\setminus\bigl(E(\U^A)\cup E(\U^B)\bigr).
\]
\end{definition}

\begin{definition}[Length on the common boundary]
\label{def:length-boundary}
Let $\partial(\U^A,\U^B)$ denote the common boundary (overlap) of the ledgers $\U^A$ and $\U^B$.
The \emph{length on the boundary} is the number of folded factors from a ledger that lie on
this overlap:
\[
\len_{\partial}(\U^A,\U^B) \;:=\; \len\!\bigl(\U^A \upharpoonright_{\partial(\U^A,\U^B)}\bigr),
\qquad
\len_{\partial}(\U^B,\U^A) \;:=\; \len\!\bigl(\U^B \upharpoonright_{\partial(\U^A,\U^B)}\bigr).
\]
Equality $\len_{\partial}(\U^A,\U^B)=\len_{\partial}(\U^B,\U^A)$ expresses informational
equilibrium on the shared frontier.
\end{definition}


\begin{proposition}[The Anti–symmetry of Information Propogation]
\label{prop:antisym}
In general $f(\U^A,\U^B)\neq f(\U^B,\U^A)$. Symmetry holds iff the overlap carries equal
refinement counts:
\[
f(\U^A,\U^B)=f(\U^B,\U^A)\quad\Longleftrightarrow\quad
\len_{\partial}(\U^A,\U^B)=\len_{\partial}(\U^B,\U^A).
\]
\end{proposition}
\begin{proofsketch}{antisym}
The interaction operator $f(U_A,U_B)$ performs a left--fold of all jointly admissible
update factors on the overlap $\partial(U_A,U_B)$, in the unique order that is consistent
with the causal refinements already recorded in each ledger.  Anti--symmetry arises
because this fold depends on the observed order of refinements whenever the overlap
contains correlated (noncommuting) factors.

Suppose first that the refinement counts on the shared boundary are equal:
\[
  \len_{\partial}(U_A,U_B) = \len_{\partial}(U_B,U_A).
\]
Every factor lying on the overlap is therefore recorded with the same resolution by
both ledgers.  No ledger contributes a strictly finer refinement than the other on the
shared frontier.  In this case the overlap consists only of mutually uncorrelant update
factors: their order is not fixed by either ledger, and informational minimality forces
them to commute.  Because the only factors whose relative placement could differ lie in
this commuting set, the resulting left--fold is invariant under exchanging the inputs,
and
\[
  f(U_A,U_B) = f(U_B,U_A).
\]

Conversely, assume the refinement counts on the overlap are unequal.  Without loss of
generality, let $U_A$ record strictly more refinement on the boundary than $U_B$.
Then $\partial(U_A,U_B)$ contains at least one factor recorded by $A$ with higher
resolution than by $B$.  Such a factor cannot be uncorrelant: if it were, its finer
structure could not have been observed by only one ledger.  The overlap therefore
contains a correlated pair of update factors whose tensor representatives do not
commute.  The left--fold must place this pair in the local causal order recorded by the
corresponding ledger.  Because $U_A$ and $U_B$ record different boundary orders for
these noncommuting factors, the two possible folds produce distinct accumulated
tensors:
\[
  f(U_A,U_B) \neq f(U_B,U_A).
\]

Thus symmetry of the interaction operator occurs exactly when the two ledgers carry
equal refinement counts on their shared boundary, and fails precisely when one ledger
resolves strictly more distinguishable structure than the other.
\end{proofsketch}


\begin{proposition}[The Transitivity of Information Propogation]
\label{prop:transitive}
For any Martin–consistent triple $\U_n,\U_{n+1},\U_{n+2}$,
\[
f(\U^A,\U_{n+2})\ =\ f\!\bigl(\U^A,\ f(\U_n,\U_{n+1})\bigr).
\]
That is, folding via the intermediate ledger equals folding directly into $\U_{n+2}$.
\end{proposition}
\begin{proofsketch}{transitive}
Let $U_n, U_{n+1}, U_{n+2}$ be a Martin--consistent triple.  Each ledger is a left--fold
of its admissible update factors, and the interaction operator $f$ produces the minimal
ledger that extends its inputs without inventing or erasing recorded refinements.  The
transitivity property expresses the fact that the unique globally coherent ledger for the
triple does not depend on how the pairwise folds are grouped.

Consider the right--hand side,
\[
  f\!\bigl(U_A,\, f(U_n, U_{n+1})\bigr).
\]
The inner fold $f(U_n, U_{n+1})$ reconciles all jointly admissible refinements of
$U_n$ and $U_{n+1}$ on their shared boundary.  Because the pair is Martin--consistent,
this fold is unique: no alternative ordering of their overlapping factors survives the
consistency check.  The result is a ledger that contains exactly the refinements common
to both inputs together with their compatible unique factors.  Folding this ledger with
$U_A$ adds precisely the admissible refinements from $U_A$ that remain consistent with
the already merged pair.  No additional events may be inserted, and none already present
may be removed.

Now consider the left--hand side,
\[
  f(U_A, U_{n+2}).
\]
Since the triple is Martin--consistent, $U_{n+2}$ already encodes all refinements that
can appear after $U_{n+1}$ without violating the Axiom of Planck or the Axiom of
Cantor.  Any refinement compatible with $U_n$ and $U_{n+1}$ must also be compatible
with $U_{n+2}$.  Thus the direct fold of $U_A$ with $U_{n+2}$ produces a ledger that
contains exactly the jointly admissible refinements of all three inputs.  As before,
no additional distinctions may be introduced.

In both constructions, the surviving event factors are the same: the set of refinements
jointly admissible across the triple.  Martin's Condition ensures that this set admits a
unique causal ordering, so both sides fold precisely the same sequence of factors.  By
informational minimality and uniqueness of the admissible ordering, the resulting tensors
must coincide:
\[
  f(U_A, U_{n+2}) = f\!\bigl(U_A,\, f(U_n, U_{n+1})\bigr).
\]

Thus the interaction operator is transitive on any Martin--consistent triple: grouping
of intermediate folds does not affect the final accumulated ledger.
\end{proofsketch}


\begin{proposition}[The Commutativity of Uncorrelant Events]
\label{prop:simultaneity}
If
\[
f\!\bigl(f(\U^A,\U^B),\ f(\U^C,\U^D)\bigr)\;=\;
f\!\bigl(f(\U^C,\U^D),\ f(\U^A,\U^B)\bigr),
\]
then the pairs $(A,B)$ and $(C,D)$ are \emph{relativistically simultaneous}: exchanging block
order introduces no new admissible distinctions on the shared boundary; the merged tensor is
invariant under the swap.
\end{proposition}
\begin{proofsketch}{simultaneity}
Let $U^{AB} := f(U_A,U_B)$ and $U^{CD} := f(U_C,U_D)$.  The hypothesis is that the
two blocks commute under the interaction operator:
\[
  f\!\bigl(U^{AB}, U^{CD}\bigr) = f\!\bigl(U^{CD}, U^{AB}\bigr).
\]
By Proposition~\ref{prop:antisym}, such commutativity can occur only when the shared
boundary carries equal refinement counts.  In the present setting this means that every
update factor lying in the overlap $\partial(U^{AB},U^{CD})$ is recorded at the same
resolution by both blocks.  No factor is strictly more refined on one side than the
other.

Equal refinement counts force the overlapping factors to be uncorrelant: neither block
records a finer causal relation among these events, so informational minimality forbids
any ledger from resolving a precedence relation absent from the other.  In the tensor
algebra this uncorrelance appears as commutation of the corresponding update factors.
Because only these boundary factors can appear in different relative positions when the
blocks are folded, and because they commute, swapping the blocks yields the same
accumulated ledger.

To interpret this result, note that two events are uncorrelant precisely when neither
precedence $e<f$ nor $f<e$ is recorded in any admissible refinement.  Such events lie
outside each other's causal neighborhoods; exchanging their order introduces no new
distinguishable structure and preserves all scalar invariants of the universe tensor.
Thus, if the blocks $(A,B)$ and $(C,D)$ commute under $f$, every event in the first
block is uncorrelant with every event in the second.  No causal precedence can be
established across the blocks.

This is exactly the condition of relativistic simultaneity in the causal framework:
the two blocks occupy spacelike-separated regions of the observational record.  Their
fold order is unconstrained, and the merged ledger is invariant under the swap.  Hence
commutativity of the interaction operator implies relativistic simultaneity.
\end{proofsketch}
\NB{This is the point at which the usual notion of \emph{causality} is rejected.  
No geometric light cones, no differential structure, and no propagation law are 
assumed.  The only order in the development is the order of \emph{recorded} 
refinements.  What physicists call causal structure appears later only as the 
smooth shadow of informational bookkeeping: the continuum calculus that encodes 
cause--effect relations is not a primitive of the theory but an emergent 
completion of discrete refinements.  Nothing in this chapter assumes or relies 
on physical causation; all that is used is the partial order induced by the 
Axiom~\ref{ax:cantor}.}
\NB{Uncorrelant events play a central conceptual role in this framework.  
They are not ``independent random variables'' nor ``simultaneous in a 
reference frame'' nor artifacts of a chosen coordinate system.  They are the 
events for which the record contains \emph{no admissible refinement} that 
orders one before the other.  This absence of recorded precedence is an 
observable fact, not a geometric assumption.  All smooth notions of 
spacelike separation, relativistic simultaneity, and commuting update factors 
arise from this single idea.  When two events are uncorrelant, reordering 
their update factors creates no new distinguishable structure, and every 
algebraic invariant of the ledger is preserved.  The geometry of relativity 
is therefore not presupposed but recovered from the informational status of 
uncorrelance.}




\begin{remark}
\item \emph{Idempotence:} $f(\U^A,\U^A)=\U^A$.
\item \emph{Monotonicity:} $\U^{AB}$ is a monotone extension of both inputs; no recorded
      refinement is removed.
\item \emph{Locality:} Joint refinements lie in the common causal neighborhood; fold order is
      the observed order; reordering is forbidden unless the corresponding factors commute.
\item \emph{Operational link:} Bi–directional folds yield the wave operator; oriented folds
      yield advection; iterated projection yields diffusion. These are smooth shadows of the
      discrete left–fold $\U_{n+1}=E_{n+1}\U_n$ under $f$.
\end{remark}

\begin{example}[The Dantzig Pivot~\cite{dantzig1963} of Entanglement~\cite{einstein1935}]
\label{te:dantzig-pivot}
\NB{The Dantzig Pivot is not a physical process. Nothing travels, no signal is sent, and no
mechanism propagates. The pivot is bookkeeping: boundary consistency is enough to eliminate
incompatible histories without scanning the interior of the ledger.}


Two spacelike-separated laboratories, $A$ and $B$, each maintain their own causal universe
tensor. A single preparation event produces two admissible refinements, $e_i$ and $e_j$, that
are indistinguishable in causal order: both
\[
\langle e_i \prec e_j \rangle
\quad\text{and}\quad
\langle e_j \prec e_i \rangle
\]
generate the same accumulated state. No scalar invariant recorded in either ledger can tell
which ordering occurred. This is a state of \emph{causal degeneracy}: two distinct histories
produce the same observational content.

At time $n{+}1$, laboratory $A$ measures $e_i$. By the Axiom of Planck, this refinement must be
folded into the accumulated state. The interaction operator $f$ computes
\[
\U_{n+1} = f(\U_n, e_i),
\]
which is a strict update: $e_i$ now has a definite position in the record relative to all prior
events.

Because $e_i$ and $e_j$ were degenerate, this update triggers a global repair. The merged ledger
must eliminate every history in which $e_j$ is ordered incompatibly with $e_i$ under Martin’s
Condition. No signal is sent from $A$ to $B$; instead, the causal universe tensor performs a
\emph{pivot}: it selects the unique ordering of $(e_i,e_j)$ that avoids introducing new
distinguishabilities. The ambiguous pair collapses to a single admissible ordering.

\emph{Critically}, this repair is not a search over an entire volume of possible histories.
Martin’s Condition requires agreement only on the \emph{boundary} of the overlap: the parts of
$\U^A$ and $\U^B$ that already coincide. The pivot therefore acts on the smallest region where
a contradiction could occur. Only the boundary is inspected, and only the incompatible
orderings are removed. There is no need to re-evaluate the entire causal universe; the ledger
verifies consistency by checking the joint frontier. Interaction is thus \emph{computable}:
global coherence is enforced by local boundary repair, not by scanning an exponential set of
histories.

Thus, the “instantaneous” correlation is not a physical transmission. It is the bookkeeping
consequence of a non-degenerate refinement. Entanglement is the existence of causal
degeneracy; the apparent nonlocal update is the pivot that removes it by repairing the
boundary of the overlap.

The name “pivot’’ is not accidental. In Dantzig’s algorithm, a degenerate solution is resolved
by moving along the boundary of admissible configurations until a single vertex remains
consistent with all constraints. The search never explores the interior volume of the
feasible set; it advances only along the frontier where inconsistency can appear. The causal
pivot behaves the same way. When a non-degenerate refinement is recorded, the ledger
examines only the boundary of the overlap and removes incompatible orderings. The result is a
unique, globally coherent history selected by local boundary repair. In both settings, the
pivot is a boundary operation, not a volume search: global consistency is enforced without
scanning an exponential family of possibilities.
\end{example}

\begin{example}[Mach--Zehnder Interferometer as Causal Superposition]
\label{te:mach-zehnder-updated}
\NB{Interference is a bookkeeping result. The ledger compares two admissible histories and
keeps only the one that preserves Martin's Condition at the boundary. No wavefunction,
no probability amplitude, no transmission is assumed.}


A single photon enters a Mach--Zehnder interferometer. At the first beam splitter, a single
input event $e_0$ leads to two admissible refinements, $e_1$ (upper path) and $e_2$ (lower
path). Both produce valid causal chains: each path accumulates its own ordered list of
refinements---reflections, delays, and phase shifts---and each yields an accumulated tensor
$\U^{(1)}$ and $\U^{(2)}$ satisfying Martin's Condition. No experiment in either arm can
distinguish which refinement is ``real'': both histories are admissible and neither produces a
contradiction. The interferometer therefore carries two coexisting, consistent ledgers.

At the second beam splitter, the detection event $e_f$ must be recorded as a strict update.
By the Axiom of Planck, the refinement $e_f$ must fold into the accumulated state. The
interaction operator computes
\[
\U_{\mathrm{final}} = f(\U^{(1)},\U^{(2)}),
\]
the minimal accumulated tensor consistent with both paths. All hypothetical histories in
which the arrival at $e_f$ contradicts either ledger are removed.

Interference is the informational comparison of the two causal chains. If their accumulated
phase---a bookkept record of distinguishability---is equal modulo $2\pi$, the paths are
informationally indistinguishable at the boundary. The fold produces a single ledger: both
paths merge without creating new refinements. If the accumulated phase differs by $\pi$, the
asymmetric parts of the update factors cancel under the fold, and $e_f$ becomes inadmissible.
No destructive force is invoked; the cancellation expresses the fact that no consistent
ledger can be formed with that ordering.

Thus, ``superposition'' is the coexistence of multiple valid, Martin-consistent refinements
until detection forces a non-degenerate fold. The Mach--Zehnder interferometer does not show
a particle traveling two paths; it shows that causal histories can remain distinct and
simultaneously admissible until the interaction operator selects the unique ordering that
avoids contradiction at the boundary.
\end{example}

\begin{example}[Bell--Aspect Tests as Global Martin Consistency]
\label{te:bell-aspect}

Two spacelike-separated laboratories, $A$ and $B$, share a preparation event that produces
an entangled pair. Each maintains its own causal universe tensor. The preparation is such
that multiple ordered refinements remain admissible: different measurement settings at $A$
and $B$ produce distinct, yet individually consistent, ledgers. Before either measurement is
recorded, the global state is degenerate: many joint histories remain compatible with all
previous refinements, and no scalar invariant distinguishes among them.

A local hidden-variable model assumes that this degeneracy can be resolved purely by local
rules. In ledger language, it assumes that the update
\[
(\text{measure at }A,\text{ measure at }B)
\]
can be decomposed into separate, predetermined refinements in each ledger. That is,
the merged state could be written as a fold of two independent maps acting only on local
records, with no global repair.

The Bell--Aspect tests show this is impossible. When $A$ records a refinement corresponding
to setting $a$ and $B$ records one corresponding to $b$, the accumulated tensor must be
updated by the interaction operator,
\[
\U_{\mathrm{final}} = f(\U^A,\U^B).
\]
For many setting pairs $(a,b)$, the resulting ledger eliminates histories that would have
remained admissible under any local rule. The violation of Bell inequalities is the empirical
statement that no decomposition of $f$ into independent, local updates can preserve all
observed distinctions. The fold is intrinsically global.

Operationally, a new refinement at $A$ forces a pivot on the boundary shared with $B$,
eliminating joint histories that contradict the updated record. No signal travels between the
laboratories; no mechanism carries information. The ledger simply performs the minimal
boundary repair required by Martin's Condition. The observed ``nonlocal'' correlations are
the bookkeeping consequence of enforcing a single, globally consistent causal ordering.

Thus, the Bell--Aspect tests reveal that entanglement is not a hidden influence. It is the
fact that the causal universe must repair its boundary globally when a non-degenerate
refinement is recorded. Local hidden variables fail because they deny the existence of this
global pivot.
\end{example}

\begin{example}[The Qubit as a Causal Doublet]
\label{te:qubit-doublet}

Consider a system whose next admissible refinement may proceed in one of two distinct ways,
$e_0$ or $e_1$. Both are individually consistent with all recorded events. Neither produces a
new distinguishability. The ledger therefore admits two possible causal updates,
\[
\U_{n+1}=E_0\,\U_n
\qquad\text{or}\qquad
\U_{n+1}=E_1\,\U_n,
\]
and nothing in the accumulated tensor resolves which will occur. The pair
$S=\{e_0,e_1\}$ is a \emph{causal doublet}: two equally admissible refinements, each extending
the history without contradiction. The system is not undecided; it simply has more than one
globally consistent continuation.

This is the minimal unit of causal degeneracy. A \emph{qubit} is a causal doublet whose two
refinements remain admissible until a non-degenerate event forces a decision. When a
measurement is recorded, the new refinement must be folded into the ledger. The interaction
operator computes
\[
\U_{n+1}=f(\U_n,e_b)
\qquad\text{for a unique } b\in\{0,1\},
\]
and all hypothetical histories in which the other refinement contradicts the updated ledger
are removed. The doublet collapses to a single admissible ordering.

Nothing physical “chooses’’ $e_0$ or $e_1$. No hidden variable pre-selects the outcome. The
pivot is bookkeeping: Planck forbids invisible refinements, Cantor forbids erasure, and
Martin enforces global consistency. Until the pivot, $S$ remains a superposed causal state:
two admissible orderings, both equally consistent with all observations.
\end{example}

\begin{example}[Hawking Radiation Revisited]
\label{te:hawking-radiation-revisited}
\NB{No physical emission is assumed. Surrogate refinements are bookkeeping: the minimal
distinctions required to restore Martin consistency when the boundary saturates.}

An external laboratory maintains a causal universe tensor $\U^{\mathrm{out}}$ recording all
admissible events visible from outside a black hole. The horizon $H$ is the frontier of
distinguishability: an informational boundary beyond which no finite extension of
$\U^{\mathrm{out}}$ can include internal and external refinements in a single, Martin-consistent
ordering. Events remain locally finite, but the reconciliation problem saturates: the external
ledger cannot compute a consistent extension that includes both sides.

As an infalling system approaches $H$, its internal refinements accelerate. By the Axiom of
Cantor, $\U^{\mathrm{out}}$ may not erase distinctions it has already recorded; by the Axiom of
Planck, it may not invent invisible refinements. When the bridge of admissible overlap
collapses—when no joint ordering of internal and external updates remains feasible—the
external ledger must perform a repair. Martin’s Condition demands a globally consistent
ordering on the accessible side.

The repair introduces \emph{surrogate refinements} $e_{\mathrm{rad}}$:
\[
\U^{\mathrm{out}}_{n+1} = e_{\mathrm{rad}}\;\U^{\mathrm{out}}_{n},
\]
a compensatory update that restores coherence without referencing inaccessible events. These
surrogates are not particles escaping from behind the horizon; they are the unique refinements
that preserve global order when the boundary can no longer reconcile the missing interior.
The exponential spectrum attributed to Hawking radiation reflects the combinatorial
multiplicity of admissible surrogate updates once the informational channel saturates.

Thus, Hawking radiation is not a quantum field effect in curved spacetime. It is the minimal
bookkeeping required to maintain Martin consistency on the visible side of an informational
boundary. The horizon enforces a holographic constraint: global order must remain representable
on the surface that separates what can be reconciled from what cannot.
\end{example}

\section{The Law of Boundary Consistency}
\label{se:law-boundary}

Every example in this chapter has the same structure. When a new admissible refinement is
recorded, the ledger does not alter the interior of the accumulated state. Instead, it repairs
only the frontier where two descriptions overlap. The Causal Folding Operator updates the
boundary and leaves the interior fixed. This pattern is universal and admits a formal
statement.

\begin{law}[The Law of Boundary Consistency]
\label{law:boundary-consistency}
In any locally finite causal domain, every admissible update to the accumulated causal
universe tensor $\U$ arises from boundary refinement. The interior of $\U$ is fixed by
previously recorded distinctions: altering it would introduce an invisible refinement (Axiom
of Planck) or remove a recorded one (Axiom of Cantor), both of which are forbidden. When a
new admissible event is observed, the ledger repairs only the frontier where two descriptions
overlap, enforcing Martin’s Condition on the boundary of the accumulated state.

Therefore all dynamics---propagation, interaction, interference, and decay---are the shadows
of boundary reconciliation. Nothing propagates through the interior; motion is the smooth limit
of reconciling admissible distinctions at the frontier of $\U$.
\end{law}

\begin{remark}
\item \emph{No interior modification.} Once folded, the interior of $\U$ contains no
      unobserved structure. Any change to it would imply either an invisible refinement or the
      erasure of a recorded one, violating Planck or Cantor.
\item \emph{Minimal repair.} When ledgers overlap, the operator updates only the smallest
      region where a contradiction could occur. This is a boundary operation, not a volume
      operation.
\item \emph{Computability.} Martin’s Condition is enforced by checking only the joint frontier:
      the causal surface where two descriptions must agree. No global search or
      re-evaluation of the interior is required.
\item \emph{Operational meaning.} Waves, interference, scattering, advection, and diffusion
      appear in the smooth limit of boundary reconciliation. The equations of motion arise
      from the unique completion that preserves the folded boundary without altering the
      interior.
\end{remark}

This law closes the algebra of interaction. The Causal Folding Operator enforces global
consistency by repairing only the frontier of the accumulated state. Every dynamic phenomenon
considered in this chapter---the Dantzig pivot of entanglement, the Mach--Zehnder interference
fold, the Bell--Aspect repair, and the surrogate refinements of a causal horizon---is an
instance of the same rule: the ledger changes only at the boundary.

This statement is the discrete analogue of Gauss’s Theorem. In the continuum, specifying the
value of a field on a closed boundary determines its interior uniquely. The Law of Boundary
Consistency asserts the same principle for causal ledgers: every admissible refinement enters
through the frontier where two descriptions overlap, and the interior is fixed by previously
recorded distinctions. Nothing propagates through the volume of $\U$; every update is a
boundary repair.

All examples in this chapter—velocity boosts, interference, entanglement, and surrogate
events near a causal horizon—share this structure. A new admissible event forces only the
minimal reconciliation on the overlap. The interior never changes. Motion is the continuum
shadow of this purely discrete principle.

At this point nothing further is required. Once every admissible update is confined to the
boundary, the smooth limit follows automatically: the interior is fixed, and all variation
arises from finite differences on the frontier. The familiar equations of motion are just the
continuum shadow of these discrete boundary repairs. Writing them down is a matter of
expressing the boundary updates in finite–difference form and passing to the smooth limit.

\section{Classical Transport}
\label{sec:classical-transport}
\NB{Nothing in this construction asserts that a differential equation
\emph{must} govern the data.  We show only that if the ledger admits a smooth
completion consistent with the axioms, then the corresponding differential
equation appears as its unique smooth shadow.  The calculus is a consequence
of measurement consistency, not an independent postulate.}

Classical transport is the process by which refinement differences reconcile across
space.  In the discrete ledger, this appears as iterated boundary smoothing:
sharp discontinuities trigger local folds until no admissible repair remains.
In the smooth limit, these reconciliation rules generate the transport
equations of classical thermodynamics.  The organizing principle is the
variational order of the correction.

\subsection{First Variation: Slope-Level Ledger Corrections}
\label{sec:heat-first-variation}

First-variation updates alter only the slope of the admissible spline
representation.  Informational minimality forbids the creation of new turning
points between event anchors: any correction that introduced a fresh extremum
would constitute an unrecorded event.  All admissible first-order updates are
therefore monotone.  Their smooth limit yields irreversible transport.

\subsubsection{Annealing and Conduction (Symmetric Reconciliation)}
\label{sec:heat-conduction}

Conduction appears when a ledger repeatedly reconciles a coarse description of
itself.  A sharp difference in refinement counts across a boundary triggers a
sequence of local folds, each of which reduces the discrepancy without
altering the interior.  This iterative process is \emph{annealing}:
informational tension is monotonically released until no further repair is
admissible.

Under the Law of Spline Sufficiency, symmetric reconciliation introduces no
oscillation and no hidden curvature.  The discrete flux is governed by the
centered jump between neighboring cells, and the update rule is a symmetric
projection back into the admissible class.  In the smooth limit, these finite
differences converge to the classical diffusion equation.

\paragraph{Discrete Ledger Update and the Flux Form.}

Let $u_i^k$ denote the normalized refinement count recorded on cell $i$ at
discrete time $t_k$, with spatial spacing $\Delta x$ and time step $\Delta t$.
The update must obey informational conservation in a conservative flux form:
\begin{equation}
  u_i^{k+1}
  \;=\;
  u_i^{k}
  \;-\;
  \frac{\Delta t}{\Delta x}
  \bigl(F_{i+\frac12}^k - F_{i-\frac12}^k\bigr).
  \label{eq:conduction-conservative}
\end{equation}

\emph{Symmetric reconciliation} uses the centered jump as the flux.  If
$\kappa$ is the informational diffusion coefficient,
\begin{equation}
  F_{i+\frac12}^k
  \;=\;
  -\,\kappa\,
  \frac{u_{i+1}^k - u_i^k}{\Delta x}.
  \label{eq:conduction-flux}
\end{equation}
Substituting \eqref{eq:conduction-flux} into
\eqref{eq:conduction-conservative} yields the standard symmetric smoothing
rule:
\begin{equation}
  u_i^{k+1}
  \;=\;
  u_i^k
  \;+\;
  \frac{\kappa\,\Delta t}{\Delta x^2}
  \bigl(u_{i+1}^k - 2u_i^k + u_{i-1}^k\bigr).
  \label{eq:conduction-update}
\end{equation}

\begin{proof}[Proof Sketch: Convergence to $u_t = D u_{xx}$]
Approximate the temporal derivative using a forward difference:
\[
  u_t(x_i,t_k)
  \;\approx\;
  \frac{u_i^{k+1} - u_i^k}{\Delta t}.
\]
Substituting \eqref{eq:conduction-update} and rearranging,
\[
  \frac{u_i^{k+1} - u_i^k}{\Delta t}
  \;=\;
  \frac{\kappa}{\Delta x^2}\,
  \bigl(u_{i+1}^k - 2u_i^k + u_{i-1}^k\bigr).
\]
The spatial term on the right is the standard centered approximation of the
second derivative,
\[
  u_{xx}(x_i,t_k)
  \;\approx\;
  \frac{u_{i+1}^k - 2u_i^k + u_{i-1}^k}{\Delta x^2}.
\]
Thus
\[
  u_t(x_i,t_k)
  \;=\;
  \kappa\,u_{xx}(x_i,t_k).
\]
Taking the continuous limit $\Delta x,\Delta t\to 0$ and letting
$\kappa \to D$ yields the diffusion equation
\[
  u_t = D\,u_{xx}.
\]
\end{proof}

The convergence is admissible because the Law of Spline Sufficiency guarantees
that the solution remains $\mathcal{C}^2$ and introduces no hidden curvature.
The symmetric finite-difference update is therefore a monotone, stable
smoothing process: the smooth shadow of informational annealing.


\subsubsection{Convection and Oriented Transport (Boundary Consistency)}
\label{sec:heat-convection}

Convection models the directed transport of distinctions, where the orientation
of the flow is realized as a preferred direction in the causal refinement
process.  When a boundary carries an orientation, reconciliation must respect
that direction: smoothing from the downstream side would create unrecorded
structure on the wrong side of the interface.

\paragraph{Oriented Boundary Reconciliation.}

Let $u_i^k$ be the normalized refinement count on cell $i$ at time $t_k$.
When the interface $(i,i{+}1)$ has a known inflow direction, the Law of
Boundary Consistency requires that the ledger flux across that interface be
determined solely by the state on the inflow side:
\begin{equation}
  F_{i+\frac12}^k = c\,u_i^k,
  \label{eq:convection-flux}
\end{equation}
where $c$ is the order speed.  Substituting \eqref{eq:convection-flux} into
the conservative update
\begin{equation}
  u_i^{k+1}
  \;=\;
  u_i^{k}
  \;-\;
  \frac{\Delta t}{\Delta x}
  \bigl(F_{i+\frac12}^k - F_{i-\frac12}^k\bigr)
  \label{eq:conservative-update-again}
\end{equation}
yields the upwind rule
\begin{equation}
  u_i^{k+1}
  \;=\;
  u_i^k
  \;-\;
  \frac{c\,\Delta t}{\Delta x}\,
  \bigl(u_i^k - u_{i-1}^k\bigr).
  \label{eq:convection-update}
\end{equation}

\begin{proof}[Proof Sketch: Convergence to $u_t + c\,u_x = 0$]
Divide \eqref{eq:convection-update} by $\Delta t$ to obtain
\[
  \frac{u_i^{k+1} - u_i^k}{\Delta t}
  \;=\;
  -\,c\,
  \frac{u_i^k - u_{i-1}^k}{\Delta x}.
\]
As $\Delta t,\Delta x \to 0$, the left side is the forward difference
approximation of the time derivative $\partial_t u$, and the right side is the
backward difference approximation of the space derivative $\partial_x u$.
Taking the smooth limit yields the advection equation
\[
  u_t + c\,u_x = 0.
\]
\end{proof}

The update \eqref{eq:convection-update} is admissible only when it remains
monotone, which is guaranteed by the CFL condition
$0 \le c\,\Delta t/\Delta x \le 1$.  Under this constraint no new turning
points are introduced, so the Law of Spline Sufficiency is respected: the
directed transport is a projection back into the admissible spline class.

\NB{Boundary Consistency selects the upwind flux, and Spline Sufficiency
forbids oscillatory corrections; the advection equation is the smooth shadow
of oriented ledger reconciliation.}



\subsubsection{Advection--Diffusion (Mixed Closure)}
\label{sec:heat-advection-diffusion}

In many settings, admissible reconciliation requires both symmetric
homogenization and directed transport.  The ledger must smooth local
inconsistencies while simultaneously respecting boundary orientation.  The
resulting update combines the symmetric and upwind fluxes.

\paragraph{Combined Flux.}

Let the oriented flux be given by
\[
  F^{\text{adv}}_{i+\frac12} = c\,u_i^k,
\]
and the symmetric flux by
\[
  F^{\text{diff}}_{i+\frac12}
  = -\,\kappa\,\frac{u_{i+1}^k - u_i^k}{\Delta x}.
\]
The total flux across the interface is their sum:
\begin{equation}
  F_{i+\frac12}^k
  = c\,u_i^k
  - \kappa\,\frac{u_{i+1}^k - u_i^k}{\Delta x}.
  \label{eq:ad-mixed-flux}
\end{equation}
Substituting \eqref{eq:ad-mixed-flux} into the conservative update
\begin{equation}
  u_i^{k+1}
  \;=\;
  u_i^{k}
  - \frac{\Delta t}{\Delta x}
    \bigl(F_{i+\frac12}^k - F_{i-\frac12}^k\bigr)
  \label{eq:ad-mixed-conservative}
\end{equation}
yields the discrete advection--diffusion rule
\begin{equation}
  u_i^{k+1}
  =
  u_i^k
  - \frac{c\,\Delta t}{\Delta x}\,(u_i^k - u_{i-1}^k)
  + \frac{\kappa\,\Delta t}{\Delta x^2}
     (u_{i+1}^k - 2u_i^k + u_{i-1}^k).
  \label{eq:ad-mixed-update}
\end{equation}

\begin{proof}[Proof Sketch: Convergence to $u_t + c\,u_x = D\,u_{xx}$]
Divide \eqref{eq:ad-mixed-update} by $\Delta t$ to obtain
\[
  \frac{u_i^{k+1} - u_i^k}{\Delta t}
  =
  -\,c\,\frac{u_i^k - u_{i-1}^k}{\Delta x}
  + \kappa\,\frac{u_{i+1}^k - 2u_i^k + u_{i-1}^k}{\Delta x^2}.
\]
In the limit $\Delta x,\Delta t \to 0$, the left side becomes
$\partial_t u$, the first term becomes $-c\,\partial_x u$, and the second
becomes $\kappa\,\partial_{xx} u$.  Setting $D=\kappa$ gives
\[
  u_t + c\,u_x = D\,u_{xx},
\]
the advection--diffusion equation.
\end{proof}

The mixed closure is the most general first-order reconciliation of the
refinement record.  Information spreads down gradients (diffusion) while
coherent packets of distinction are carried along oriented interfaces
(advection).  The process is irreversible in either mode, and no additional
structure is assumed beyond the slope-level correction forced by the axioms.

\NB{In every case, first-variation closure is a projection back into the
admissible spline class: no new extrema are introduced, and no hidden
structure appears under refinement.  The differential equations are the smooth
shadows of monotone reconciliation.}


\subsection{Second Variation: Curvature-Level Ledger Corrections}
\label{sec:heat-second-variation}

Second-variation updates alter curvature while preserving slope and anchor
values.  These corrections are reversible: they propagate distinctions without
loss and produce no additional smoothing.  Their smooth limit yields wave
transport.

\subsubsection{Radiation (Symmetric Curvature Smoothing)}
\label{sec:heat-radiation}

Radiation represents the propagation of distinction at the maximal admissible
speed.  Unlike the first--order corrections of
Sections~\ref{sec:heat-conduction}--\ref{sec:heat-advection-diffusion}, radiation is
reversible: once the ledger has reconciled curvature symmetrically, no net
informational gain or loss remains.  The process is the smooth shadow of
\emph{symmetric curvature smoothing}.

\paragraph{Vanishing Second Variation.}

Let $\mathcal{A}$ denote the amplitude of distinction recorded over a finite
causal neighborhood.  The second variation $\delta^2\mathcal{A}$ measures the
change in $\mathcal{A}$ under two sequential, infinitesimal perturbations of
the record.  Radiation occurs when these perturbations commute exactly:
\begin{equation}
  \delta^2\mathcal{A} = 0.
  \label{eq:radiation-second-variation}
\end{equation}
No net expansion or contraction of distinguishability can remain; curvature
differences are repaired symmetrically and without directional bias.  This is
the reversible complement of annealing: where first--order correction removes
slope--level inconsistencies, second--order correction removes curvature--level
tension.

\paragraph{Discrete Curvature Laplacian.}

In the discrete domain, the sum of all pairwise second variations over
neighboring events defines the discrete Laplacian on event sets:
\[
  \nabla_E^2 \mathcal{A}
  \;=\;
  \sum_{f \in \mathrm{Nbr}(e)} \bigl(\mathcal{A}(f) - \mathcal{A}(e)\bigr).
\]
Martin's Condition enforces that this curvature vanishes:
\begin{equation}
  \nabla_E^2 \mathcal{A} = 0,
  \label{eq:radiation-discrete-laplace}
\end{equation}
so that symmetric curvature smoothing is locally maximal and globally neutral.

\paragraph{Smooth Shadow.}

In the continuum limit, the second--order symmetric closure converges to the
homogeneous wave equation.  If $u(x,t)$ is the smooth completion of the
refinement record, then
\begin{equation}
  u_{tt} = c^2\,u_{xx},
  \label{eq:wave-equation}
\end{equation}
where $c$ is the order speed---the combinatorial rate at which causal
constraints traverse the event network.  Equation~\eqref{eq:wave-equation}
expresses reversible propagation: local expansions and contractions of
distinguishability cancel globally, so that information moves without net
amplification or dissipation.

\NB{Second--variation closure enforces symmetric curvature repair and forbids
net informational gain or loss.  The wave equation is therefore the unique
smooth shadow of reversible curvature smoothing, derived solely from the
axioms of causal refinement.}


\subsubsection{Adiabatic Transport (Curvature Invariance)}
\label{sec:heat-adiabatic}

Adiabatic transport is the ideal limit of reversible motion in the causal
record.  Distinctions are neither created nor destroyed: informational entropy
remains constant, and the curvature of the smooth completion is preserved.
This process is the logical dual of annealing, establishing the boundary
condition for zero informational work.

\paragraph{Invariance of Distinguishability.}

Let $\lambda$ parameterize a smooth evolution of an admissible history
$\Psi(\lambda)$.  The history undergoes adiabatic transport when the
informational entropy is invariant:
\begin{equation}
  \frac{d}{d\lambda}\,\mathcal{S}(\Psi) = 0.
  \label{eq:adiabatic-entropy}
\end{equation}
Equivalently, the update operator satisfies
\[
  U_{\lambda+\delta\lambda}
  =
  U_{\lambda} + \mathcal{O}(\delta\lambda^2),
\]
so the leading--order change in the refinement record vanishes.  The motion is
norm--preserving and informationally reversible: the ledger drifts without loss
of distinction.

\paragraph{Curvature Invariance.}

Because $\mathcal{S}$ counts admissible configurations, the condition
\eqref{eq:adiabatic-entropy} forces the evolution to proceed along a path of
constant informational curvature.  Locally,
\begin{equation}
  \frac{d}{d\lambda}\,\Psi'' = 0,
  \label{eq:adiabatic-curvature}
\end{equation}
so that no curvature--level tension is released or accumulated.  This is the
reversible complement to the symmetric curvature smoothing of
Section~\ref{sec:heat-radiation}.

\paragraph{Smooth Shadow.}

Under the Law of Spline Sufficiency ($\Psi^{(4)}=0$), curvature invariance
selects the unique extremal that transports distinctions without dissipation:
the geodesic or undamped wave.  Informational entropy remains constant,
and the ledger evolves along the smooth completion $\Psi$ without net repair or
decay.  Nothing dynamical is postulated; the law is a theorem of informational
conservation.

\NB{Adiabatic transport is the limit of causal motion that preserves
informational order.  It connects reversible evolution ($d\mathcal{S}=0$) with
the requirement that distinguishability cannot decrease.  The geodesic
structure is therefore a consequence of informational invariance, not an
independent physical postulate.}


\section{Quantum Transport}
\label{se:quantum-transport}

Some transport phenomena do not appear as flows of a substance, but as discrete repairs of
nearly degenerate descriptions. When two ledgers support multiple admissible extensions,
the Causal Folding Operator must select the unique completion that preserves all recorded
distinctions. The familiar quantum effects arise as the smooth shadows of this repair.

\subsection{The Casimir Effect: Boundary-Limited Distinction}
\label{sec:heat-casimir}

The Casimir effect is the boundary expression of informational pressure.  When
admissible refinements are restricted by geometry, the ledger must perform a
compensatory update to preserve global distinguishability.  In the smooth
limit, this boundary repair appears as a physical force.

\paragraph{Boundary--Induced Asymmetry.}

Consider two parallel constraints that restrict the admissible causal updates
in the interior region.  Each admissible field mode corresponds to a
distinguishable refinement of the causal record.  The plates suppress many
of these modes, so the interior ledger records fewer admissible distinctions
than the exterior.  Outside the plates, no such suppression occurs; the ledger
remains unrestricted.  This produces an imbalance in refinement counts across
the boundary: the exterior supports strictly more admissible updates than the
interior.

\paragraph{Compensatory Boundary Update.}

The Second Law of Causal Order requires that global distinguishability must
not decrease.  The imbalance therefore creates informational tension.  Because
no additional interior modes are admissible, the only possible repair is a
boundary update that restores global consistency without altering the
restricted interior.  The unique correction is an outward curvature of the
boundary ledger: refinements accumulate on the exterior frontier, pushing the
constraints toward one another.

In the smooth limit, this boundary curvature appears as the Casimir pressure.
No mechanical postulate is introduced; the force is the smooth shadow of a
compensatory update that restores consistency between the restricted interior
and unrestricted exterior ledgers.

\NB{In this interpretation, the Casimir effect is a holographic phenomenon:
the minimal boundary correction enforced by global distinguishability.  The
pressure is not a hypothesis about zero--point energy, but the unique repair
consistent with the axioms of causal refinement.}


\subsection{Alpha Decay: Repair of a Causal Contradiction}
\label{sec:quantum-alpha}

Alpha decay is the irreversible repair of a causal contradiction on the
boundary of the nuclear ledger.  The nucleus admits two nearly indistinguishable
continuations of its refinement record:
\[
  \Psi_{\mathrm{bound}}
  \qquad\text{and}\qquad
  \Psi_{\mathrm{unbound}}.
\]
Both are initially admissible: each agrees with all external anchors and
differs only within a bounded interior neighborhood.

\paragraph{Causal Degeneracy and Symmetry Drift.}

Over informational time, unresolved curvature accumulates and the two ledgers
drift out of alignment.  Their boundary descriptions become incompatible with
Martin Consistency: the overlap cannot be reconciled without introducing
unrecorded structure.  A repair is required to preserve the global order of
the causal record.

\paragraph{Removal of an Inconsistent Branch.}

The Causal Folding Operator $f$ performs the minimal corrective update by
removing the inconsistent branch:
\[
  f:\;
  \Psi_{\mathrm{bound}}
  \longrightarrow
  \Psi_{\mathrm{unbound}}
  \;+\;
  \alpha.
\]
The emitted alpha particle is the recorded trace of this boundary repair.  The
interior ledger returns to an admissible configuration, and the causal record
evolves on the remaining branch.

\paragraph{Smooth Shadow.}

In the continuum limit, the finite differences of this irreversible repair
produce the exponential law of radioactive decay.  No hidden forces or
tunneling mechanism is assumed: alpha decay is the unique boundary update that
eliminates a causal contradiction while preserving global distinguishability.

\NB{Alpha decay is the irreversible removal of an inconsistent branch from the
refinement record.  The emitted particle is the holographic trace of the
boundary correction, not a postulated tunneling object.}


\subsection{Gamma Decay (Restoration of Causal Symmetry)}
\label{sec:quantum-gamma}

Gamma decay is a reversible repair of internal causal symmetry.  An excited
nuclear state corresponds to an admissible configuration whose internal
refinement record is nearly, but not exactly, consistent with the minimal
ground state.  Over time, unresolved curvature accumulates, producing a small
informational asymmetry in the internal ledger.

\paragraph{Informational Synchronization.}

Let $\Psi^{*}$ denote the smooth completion of the excited state and $\Psi$
that of the ground state.  Both are admissible: they agree on all external
anchors and differ only in a bounded internal neighborhood.  The difference is
a phase drift in the internal causal partition---a small curvature that
violates informational minimality.  The nucleus must perform a repair that
restores the unique, globally consistent ground state.

The minimal symmetric repair is the emission of a gamma photon:
\[
  \Psi^{*} \;\longrightarrow\; \Psi \;+\; \gamma.
\]
The photon is the propagated correction: a reversible wave of order that
carries the excess curvature away from the nucleus while leaving the internal
ledger in its minimal configuration.

\paragraph{Zero--Mass Boundary Repair.}

Unlike alpha decay (Section~\ref{sec:quantum-alpha}), which removes an entire
inconsistent branch from the record, gamma decay preserves the identity of the
nucleus.  It is informationally reversible: no new branches are created, and
no admissible distinctions are destroyed.  The process is the smooth shadow of
symmetric curvature repair:
\[
  \delta^{2}\mathcal{A} = 0
  \quad\Longrightarrow\quad
  \text{emission of }\gamma\text{ with } E=h\nu.
\]

The energy of the photon measures the amount of curvature removed from the
internal ledger.  No mechanical postulate is required; gamma decay is the
unique boundary update that restores global distinguishability without
altering the underlying causal identity of the system.

\NB{In this interpretation, gamma decay is not a force--mediated transition,
but a minimal holographic correction: a reversible synchronization event that
propagates excess curvature as a photon and restores Martin Consistency in the
internal ledger without altering the causal identity of the nucleus.}



\subsection{Brownian Motion: Diffusive Boundary Uncertainty}
\subsubsection{Brownian Motion as Quantized Uncertainty}
\label{sec:brownian-quantum}

Brownian motion can be interpreted as a quantum informational phenomenon in
the present framework.  The source of randomness is not mechanical noise but
\emph{finite causal resolution}: each refinement step leaves a family of
equally admissible micro--orderings that the ledger cannot distinguish.  The
coarse record therefore evolves stochastically.

\paragraph{Stochastic Reconciliation at Finite Resolution.}

Let $u_i^k$ be the normalized refinement count on cell $i$ at time $t_k$.
When the observer cannot resolve all admissible distinctions at scale
$\Delta x$, the symmetric smoothing update acquires an irreducible stochastic
term:
\begin{equation}
  u_i^{k+1}
  =
  u_i^k
  +
  \frac{\kappa\,\Delta t}{\Delta x^2}
  (u_{i+1}^k - 2u_i^k + u_{i-1}^k)
  +
  \sqrt{2D\,\Delta t}\;\xi_i^k,
  \qquad \mathbb{E}[\xi_i^k]=0,\;\mathbb{E}[(\xi_i^k)^2]=1.
  \label{eq:brownian-update}
\end{equation}
The deterministic part is the symmetric reconciliation enforced by the Law of
Spline Sufficiency; the random term is the ledger's irreducible uncertainty at
the observation scale.

\paragraph{Smooth Shadow: Diffusion as Quantum Measure.}

Under refinement $\Delta x,\Delta t\to 0$ with $D$ fixed, the central limit
theorem implies convergence of \eqref{eq:brownian-update} to the diffusion
equation for the coarse density $u(x,t)$:
\begin{equation}
  u_t = D\,u_{xx}.
  \label{eq:brownian-diffusion}
\end{equation}
Here $D$ is the \emph{informational diffusion coefficient}: the effective
bandwidth of unresolved distinctions per unit time.

\paragraph{Bridge to Schr\"odinger via Analytic Continuation.}

The free Schr\"odinger equation is related to diffusion by analytic
continuation of time.  Setting
\(
  D = \tfrac{\hbar}{2m}
\)
and $t \mapsto -\,i t$ maps \eqref{eq:brownian-diffusion} to
\begin{equation}
  i\,\hbar\,\partial_t \Psi = -\,\frac{\hbar^2}{2m}\,\partial_{xx}\Psi,
  \label{eq:schrodinger-free}
\end{equation}
i.e., the smooth shadow of unresolved, symmetric refinement at fixed
informational bandwidth equals the quantum free evolution with Planck scale
$\hbar$.  In this sense, Brownian motion is \emph{quantized uncertainty}:
$\hbar$ calibrates the minimal unresolved action, while $D$ measures the rate
at which that unresolved structure propagates statistically.

\paragraph{Consistency with the Two Laws.}

- \emph{Spline Sufficiency} ensures no spurious extrema: the stochastic update
  remains a projection into the admissible class almost surely.
- \emph{Boundary Consistency} fixes oriented interfaces; adding an upwind
  drift $c$ to \eqref{eq:brownian-update} yields the standard
  advection--diffusion (Fokker--Planck) limit.

\NB{This construction shows \emph{how} quantum evolution can arise from
measurement limits: if the ledger's unresolved bandwidth $D$ is fixed by a
Planck scale, diffusion analytically continues to Schr\"odinger dynamics.  It
does not assert that nature must realize this identification in every regime.}



\section{First Quantization as an Application of the Two Laws}
\label{sec:first-quantization}

The classical picture of quantization treats the wavefunction, Hilbert space,
and operator algebra as new physical axioms.  In the present framework they
arise automatically from the two kinematic consistency laws:

\begin{itemize}
\item \textbf{Law of Spline Sufficiency:} no admissible refinement may
      introduce unrecorded structure; smooth closure is $\mathcal{C}^{2}$
      and satisfies $\Psi^{(4)} = 0$,
\item \textbf{Law of Boundary Consistency:} oriented boundaries must be
      reconciled from the inflow side; no correction may propagate across a
      boundary in the wrong direction.
\end{itemize}

Together, these laws force the structure known in physics as \emph{first
quantization}.  Nothing new is added: the quantized theory is the smooth
shadow of informational bookkeeping.

\subsection{Hilbert Structure from Spline Closure}
\label{sec:hilbert-from-splines}

Under Spline Sufficiency, every admissible history has a unique smooth
representative $\Psi$ that is cubic between anchors and $\mathcal{C}^{2}$
globally.  Any two admissible histories $\Psi$ and $\Phi$ differ only in their
recorded curvature.  Their overlap is therefore measured by the curvature
functional
\[
  \langle \Psi,\,\Phi \rangle
  = \int \Psi''(x)\,\Phi''(x)\,dx.
\]
This inner product is positive definite on the admissible class and yields a
complete inner-product space: the Hilbert space of admissible closures.  The
``wavefunction'' is nothing more than $\Psi$ viewed as an element of this
space.

\subsection{Canonical Structure from Boundary Consistency}
\label{sec:canonical-structure}

The curvature functional determines a unique conjugate operator.  Integration
by parts yields
\[
  \langle \Psi,\,x\,\Phi \rangle
  - \langle x\,\Psi,\,\Phi \rangle
  = \int \Psi(x)\,\Phi'(x)\,dx,
\]
where the boundary term is fixed in sign by the inflow rule of Boundary
Consistency.  The operator that realizes this antisymmetry is
\[
  \hat{p} = -\,i\,\partial_x,
\]
the momentum operator of canonical quantization.  No new axiom is required:
the oriented boundary rule uniquely determines the self-adjoint generator of
translations.

\subsection{Energy Levels from Informational Minimality}

Consider an admissible history constrained by a restoring boundary (a fold
that always returns toward the anchor).  Under Spline Sufficiency the closure
is cubic between anchors and $\Psi^{(4)}=0$; under Boundary Consistency the
inflow rule forces the curvature to alternate monotonically between turning
points.  The Galerkin limit of this curvature balance is the harmonic
oscillator:
\[
  -\,\Psi''(x) + x^{2}\Psi(x) = \lambda \Psi(x),
\]
whose eigenvalues are discrete because no new turning points may be added
between anchors.  The spectrum is the familiar
\[
  \lambda_n = (2n+1), \qquad n=0,1,2,\dots
\]
Quantization is therefore a \emph{restriction of admissible curvature}, not a
postulate about nature.

\subsection{Summary}

\begin{itemize}
\item Spline Sufficiency $\Rightarrow$ Hilbert space of smooth closures,
\item Boundary Consistency $\Rightarrow$ canonical commutators,
\item Discrete curvature balance $\Rightarrow$ quantized energy levels.
\end{itemize}

\begin{equation*}
  \text{finite ledger}
  \;\xrightarrow{\text{spline closure}}\;
  \Psi
  \;\xrightarrow{\text{boundary consistency}}\;
  \hat{x},\,\hat{p}
  \;\xrightarrow{\text{curvature balance}}\;
  \text{quantized energies}.
\end{equation*}

Thus the apparatus of ``first quantization'' is not a new physics.  It is the
smooth bookkeeping of the two kinematic laws applied to finite informational
records.

\NB{In this sense, quantization is not an independent hypothesis.  It is the
minimal correction rule forced by informational sufficiency and boundary
orientation.}


\section*{Coda: The Informational Harmonic Oscillator~\cite{planck1901}}
\addcontentsline{toc}{section}{Coda: The Informational Harmonic Oscillator}

\begin{example}[The Informational Harmonic Oscillator]
\textbf{N.B.} This coda introduces the simplest reversible dynamics permitted by the Second Law of Causal Order.  
It shows that oscillation---in its most abstract sense---arises whenever information alternates between two complementary forms: record and prediction.  
No physical mass, force, or energy is implied.  
The oscillator here is entirely informational: a minimal closed loop of distinguishability \cite{planck1901,landauer1961,wheeler1983}.
\NB{The refinement argument developed in this section parallels the mathematical
structure of Planck's resolution of the ultraviolet catastrophe.  The similarity
is formal only.  No physical quantization assumptions are introduced here; the
discrete refinement spectrum arises solely from informational consistency and the
minimal distinction bound $\epsilon$.}


\emph{Setup.}  
Let $(x,p)$ denote a conjugate pair of informational coordinates on a two–dimensional causal phase space.  
$x$ represents the observer’s recorded distinctions (the state of knowledge);  
$p$ represents the predictive momentum (the rate at which distinctions are changing).  
Define the informational Hamiltonian
\[
\Entropy(x,p) = \tfrac{1}{2}\,(\alpha x^{2} + \beta p^{2}),
\]
where $\alpha$ and $\beta$ are positive constants measuring informational stiffness and inertia.  
The reversible evolution equations are
\[
\dot{x} =  \phantom{-}\frac{\partial \Entropy}{\partial p} = \beta p,
\qquad
\dot{p} = -\frac{\partial \Entropy}{\partial x} = -\alpha x.
\]
Eliminating $p$ yields
\[
\ddot{x} + \omega^{2} x = 0, \qquad \omega^{2} = \alpha\beta.
\]
Thus the observer’s state executes harmonic motion in informational phase space with constant total measure $\Entropy(x,p)$.

\emph{Interpretation.}  
At each turning point of the oscillation, information is maximally localized: the record $x$ is fixed, the prediction $p$ vanishes.  
At each midpoint, prediction dominates and the record is momentarily indeterminate.  
The system alternately \emph{stores} and \emph{transmits} distinguishability, maintaining constant total informational entropy.  
The cycle expresses the complementarity of knowledge and expectation: every complete measurement must eventually swing back toward uncertainty to preserve $\Delta S \ge 0$.

\emph{Relation to Transport.}  
The four informational transport regimes appear as limiting cases of this oscillator:
\begin{itemize}
  \item \textbf{Adiabatic transport:} small, continuous oscillations with $\dot S = 0$ (reversible exchange of information).
  \item \textbf{Annealing:} inclusion of damping $\gamma > 0$, giving $\ddot{x} + \gamma\dot{x} + \omega^{2}x = 0$, a monotonic relaxation to equilibrium.
  \item \textbf{Brownian motion:} addition of stochastic forcing $\xi(t)$, $\ddot{x} + \omega^{2}x = \xi(t)$, producing diffusive variance growth.
  \item \textbf{Wave/particle duality:} interpretation of $(x,p)$ as the conjugate pair of localization and amplitude---dual views of the same informational invariant.
\end{itemize}
Each regime preserves causal consistency and satisfies $\Delta S \ge 0$; only the mode of information exchange changes.

\emph{Scope.}  
This oscillator is the canonical closed system of the informational universe:  
a bounded transformation in which every increase in record precision is offset by a proportional loss of predictive capacity, and vice versa.  
It represents the minimal rhythm of causal order---the reversible heartbeat of information itself.
\end{example}


\begin{center}
\textit{Motion, in this theory, is not caused by energy.  
It is the preservation of order under Martin’s Condition.}
\end{center}




