\chapter{The Continuum of Experiment}
\label{ch:calculus-dynamics}

Every measurement advances the admissible history by adding a new
distinction to an observer’s record. In prior chapters, this record was shown
to accumulate as a finite vector of distinguishable counts: a count of counts.
Such a structure evolves step by discrete step, through refinement operators
that preserve what is already known while introducing new, minimally
necessary distinctions.

When a single observer refines the record, the evolution appears
deterministic: each update follows uniquely from what has already been
distinguished. The unfolding of admissible events resembles a gearwork,
advancing one tooth at a time. From this perspective, the universe appears
to operate like a perfect automaton.

\begin{phenomenon}[The Laplace Effect~\cite{laplace1814}]
\label{ph:clockwork}
In the special case of a single observer following a single causal thread,
the axioms of measurement select a unique admissible refinement at each
ordinal step. Once an initial experimental record is fixed, every subsequent
update is determined by the requirement of minimal refinement. From this
perspective the evolution of the record appears completely constrained: the
future is a necessary continuation of what has already been distinguished.
This is the informational analogue of the classical picture in which the
universe behaves like a perfectly wound mechanism, evolving according to a
single deterministic rule in the sense articulated by Laplace~\cite{laplace1814}.

Consider a single observer whose experimental record evolves under the
axioms of measurement. Suppose that at each ordinal step $n$ there exists a
unique admissible refinement operator $\mathbf{E}_n$ consistent with the
current record. Then the state vector $\boldsymbol{\psi}_n$ evolves according
to the discrete update rule
\begin{equation}
  \boldsymbol{\psi}_{n+1} = \mathbf{E}_n \, \boldsymbol{\psi}_n,
\end{equation}
with cumulative evolution
\begin{equation}
  \boldsymbol{\psi}_n = \mathbf{U}_n \, \boldsymbol{\psi}_0,
  \qquad
  \mathbf{U}_n = \mathbf{E}_{n-1} \cdots \mathbf{E}_0.
\end{equation}
In this regime the admissible history advances as a uniquely determined
sequence of refinements. The observed universe behaves as a clockwork:
a discrete mechanism in which each new distinction is fixed by those that
precede it.
\end{phenomenon}

In the discrete domain, \emph{anchor points} are the only places where the universe has
committed to a specific value---specific events yielding specific observations.  Between 
anchors the record is silent: the
data permit many possible continuations, but most would introduce
unobserved structure.  Any admissible configuration must therefore agree
at the anchor points and remain free of additional distinguishable
features in between.  The role of the anchors is not geometric; it is
informational.  They fix the admissible boundary data against which all
variations are tested.  A candidate variation that disagrees at an anchor
is rejected immediately, because it contradicts an established event.  A
variation that agrees at the anchors but inserts additional oscillation,
curvature, or ``hidden motion'' is rejected by
Axiom~\ref{ax:boltzmann}, because those features would have produced
additional anchors that do not appear in the record.

\begin{definition}[Anchor Points~\cite{deboor1978}]
\label{def:anchors}
A finite set of \emph{anchor points} is the collection of measured
events at which admissible configurations must agree.  Two candidate
histories $\psi$ and $\phi$ are said to share the same anchors if they
record identical distinguishable values at those events. Axiom~\ref{ax:ockham}
requires that any refinement of a history preserve agreement
on the anchors: no admissible configuration may contradict an observed
event.
\end{definition}

But no observer measures the universe alone. When multiple observers refine
their records independently, their accumulated distinctions need not align.
Disagreement becomes observable. To maintain global admissibility, these
differences must be reconciled on any overlapping portion of their records.
They must be \emph{compatible}.

Compatibility introduces a new observable: the minimal number of refinements
required to reconcile two distinct records on their overlap. This operational
count defines an informational distance between observers. The necessity of
preserving compatibility across refinements thereby introduces the first
traces of geometry. It is here that motion emerges: not as a postulated law,
but as the consequence of observers counting differently.

Thus, what appears as a clockwork when viewed along a single thread becomes
dynamic when viewed across many. The evolution of the universe is not a
predetermined mechanism but a continuous reconciliation of discrete
distinctions. Dynamics arise from the requirement that all admissible records
agree wherever they meet. Relative motion is the process by which compatibility is
maintained.

\section{The Experimental Record as a Count of Counts}
\label{sec:count-of-counts}

We begin by discarding the assumption that the state of a system is represented
by a continuous field or a point in a smooth manifold. Instead, we adhere
strictly to the axioms of finite distinguishability: the only admissible data
are finite sets of events, labeled and ordered by their recording.

The fundamental object of dynamics is not position $x(t)$, but the
\emph{experimental record vector} $\boldsymbol{\psi}_n$. This vector does not
describe where the system \emph{is}; it describes what the system has
\emph{done}.  The experimental record is built from anchor points, it isn't the
anchor points themselves.

\subsection{Construction of an Experimental Record Vector}
\label{subsec:construction-ER}

Let $\Sigma = \{c_1, c_2, \dots, c_M\}$ be the finite alphabet of
distinguishable outcomes available to an observer. At ordinal rank $n$ (the
``proper time'' of the observer), the experimental record is the tally of all
outcomes observed thus far.

We define the experimental record vector $\boldsymbol{\psi}_n \in \mathbb{N}^M$ by
\[
  \boldsymbol{\psi}_n
  =
  \begin{pmatrix}
    k_1 \\
    k_2 \\
    \vdots \\
    k_M
  \end{pmatrix},
\]
where $k_i$ denotes the number of times outcome $c_i$ has been recorded in the
history $H_n = \langle e_1 \prec \cdots \prec e_n \rangle$.

This vector is an ``accumulator'' or histogram of the history. It satisfies:

\begin{itemize}
\item
\textbf{Integer normality.}
Entries are nonnegative integers: $k_i \in \mathbb{N}$.
\item
\textbf{Conservation of count.}
The $L_1$ norm corresponds to the ordinal rank:
\[
\|\boldsymbol{\psi}_n\|_1 = \sum_{i=1}^M k_i = n.
\]
\item
\textbf{Irreversibility.}
Entries can only increase: $k_i(n+1) \ge k_i(n)$.
Erasing a recorded distinction is forbidden.
\end{itemize}

\subsection{Minimal Refinement Operators}
\label{subsec:minimal-U}

Dynamics is the rule governing the update $\boldsymbol{\psi}_n \to
\boldsymbol{\psi}_{n+1}$. Because the record grows one event at a time, the
update must be sparse.

A \emph{minimal refinement} is an operator $\hat{R}_j$ that increments the
count of outcome $c_j$ by exactly one unit, leaving all other components
invariant:
\[
  \hat{R}_j \boldsymbol{\psi}_n = \boldsymbol{\psi}_n + \mathbf{e}_j,
\]
where $\mathbf{e}_j$ is the $j$th standard basis vector in $\mathbb{N}^M$.

Two principles characterize these operators:

\begin{enumerate}
\item
\textbf{Unit step.}
The operator adds one, never fractions: information is discretized.
\item
\textbf{Non-triviality.}
The zero operator is inadmissible: a measurement that records nothing is
indistinguishable from the absence of measurement.
\end{enumerate}

Thus, the evolution of the system is a path on the integer lattice
$\mathbb{N}^M$, driven by the sequential application of minimal refinement
operators.

\subsection{Variation as a Change in Count Structure}
\label{subsec:variation-counts}

In classical calculus, variation is defined as a perturbation of a continuous
variable. Here, variation is defined as a \emph{change in count distribution}
between two admissible histories of the same length.

Consider two histories $A$ and $B$ that have reached the same ordinal rank
$n$. Their records $\boldsymbol{\psi}_n^A$ and $\boldsymbol{\psi}_n^B$ may
differ. The \emph{variation} is
\[
  \delta\boldsymbol{\psi}
  =
  \boldsymbol{\psi}_n^A
  -
  \boldsymbol{\psi}_n^B.
\]
Because both vectors sum to $n$, the components of
$\delta\boldsymbol{\psi}$ must sum to zero. Variation is therefore a
\emph{trade-off}: increasing one count requires decreasing another relative to
a baseline.

\begin{phenomenon}[The Heisenberg Effect as Trade-off]
\label{ph:heisenberg-tradeoff}
A ledger with fixed capacity $n$ cannot refine all outcome classes
simultaneously. Allocating refinements to one group of outcomes consumes the
budget available to resolve the remainder. The variation
$\delta\boldsymbol{\psi}$ expresses this pivot between mutually exclusive
informational descriptions.
\end{phenomenon}

Variation is therefore not a derivative; it is a \emph{reallocation of
counts}. The calculus of dynamics that follows is the study of which
reallocations are admissible while preserving the coherence of the global
ledger.

\section{Exhaustion of Distinguishability}
\label{sec:exhaustion}

The experimental record advances only when a new distinguishable event is
successfully observed. Distance, defined internally as the tally of repeated
outcomes, is therefore meaningful only while new increments are possible.
If a proposed refinement yields no observable event, the history cannot
continue.

Let $\{c_1,\dots,c_M\}$ denote the current outcome labels available to the
observer. At ordinal step $n+1$, the observer attempts to refine the
experimental record. If no admissible outcome occurs, then
\[
  \boldsymbol{\psi}_{n+1}
  \text{ is undefined},
\]
and the admissible history terminates at step $n$.

\begin{phenomenon}[The Malus Effect~\cite{malus1810}]
\label{ph:polarization-termination}
Consider a beam of light that has passed through a linear polarizing filter.
All subsequent photons are aligned to that axis. If the observer introduces a
second polarizer oriented at $90^\circ$ to the first, no photon passes. There
is no new distinguishable event. The count of counts cannot increase, and
the experimental record ends. The light can no longer be observed.
\end{phenomenon}

In this setting, the attempt to extend the record fails. The observer has
exhausted the available structure. Without a new admissible event, no component of
$\boldsymbol{\psi}_n$ can grow, and no distance can be defined beyond this
point.

\begin{center}
\emph{If a refinement produces no observable outcome, the history stops.}
\end{center}

The failure to propagate the experimental record marks a fundamental limit:
progress requires either additional distinguishable outcomes or an expanded
basis of measurement. As long as only one observer is present, such limits
are absolute. A new source of distinction is needed for the universe to
continue unfolding.


\section{Multiple Observers and Compatibility}
\label{sec:multiple-observers}

A single observer records a sequence of events, generating a private history
vector $\boldsymbol{\psi}_A$. A second observer generates a separate history
$\boldsymbol{\psi}_B$. The central problem of physics is not describing these
histories in isolation, but determining when they can be merged into a single,
compatible global history.

The universe does not ``average'' incompatible observations. If Observer~A
records an event that contradicts the causal structure recorded by Observer~B,
no joint history exists. The system does not find a compromise; it halts.

\subsection{Compatibility of Experimental Records}
\label{subsec:compatibility}

Let $E_A$ and $E_B$ be the ordered sets of events for two observers. We say
that $E_A$ and $E_B$ are \emph{compatible} if there exists a third event set
$E_{\mathrm{univ}}$ such that:

\begin{enumerate}
\item \textbf{Embeddability.}
Both $E_A$ and $E_B$ admit order-preserving injections into
$E_{\mathrm{univ}}$.
\item \textbf{No contradiction.}
If an event $e$ appears in both event sets (identified by shared causal
ancestry), its ordering relationships with all shared prior events must be
identical in both.
\item \textbf{Global admissibility.}
$E_{\mathrm{univ}}$ must satisfy all axioms of measurement: finiteness,
distinctness, and non-decreasing recorded information.
\end{enumerate}

If these conditions are met, the observers inhabit the same admissible
universe. If they are not met, the observers are causally disjoint or the
proposed merge represents a physical impossibility.

\begin{phenomenon}[The Halt Effect]
\label{ph:halt-effect}
Consider two processors attempting to write different values to the same
memory address at the same logical clock tick. The system cannot resolve
this by superposition or averaging; the operation is undefined. The system
halts.

Likewise, if two observers record mutually exclusive refinements of what
they assert is the \emph{same} event (for instance, simultaneous assignments
of ``spin up'' and ``spin down'' in the same basis), then no compatible
$E_{\mathrm{univ}}$ exists. The joint evolution is forbidden. The admissible
timeline terminates.
\end{phenomenon}

\subsection{Overlap and Shared Distinguishability}
\label{subsec:overlap}

Compatibility is determined entirely by the \emph{overlap} of the event sets.
The overlap $\mathcal{O} = E_A \cap E_B$ consists of all events causally
ancestral to both observers.

For the experimental records to be compatible, they must agree on the counts
and ordering of events in $\mathcal{O}$. This induces a synchronization
condition on the history vectors:
\[
  \boldsymbol{\psi}_A \big|_{\mathcal{O}}
  =
  \boldsymbol{\psi}_B \big|_{\mathcal{O}},
\]
where the restriction $|_{\mathcal{O}}$ projects each history vector onto the
shared outcome classes.

This equality is the \emph{Martin Consistency Condition} in vector form: the
shared past has only one admissible representation. Any discrepancy implies
either that one observer has recorded a distinction the other has not, or
that the observers inhabit different universes.

If $\mathcal{O} = \varnothing$, the observers are uncorrelant (spacelike
separated). If $\mathcal{O} \neq \varnothing$ and the consistency condition is
satisfied, the observers are causally connected. If
$\mathcal{O} \neq \varnothing$ but the condition fails, the configuration is
physically forbidden.



\section{Informational Distance}
\label{sec:informational-distance}

Once compatibility is secured (Section~\ref{sec:multiple-observers}), a
quantitative relationship between observers becomes meaningful. If two
records agree on a shared past, we may ask: how many further distinctions
separate their present observations?

Distance measures refinement. It counts how many distinguishable
updates one history has recorded beyond another.

\subsection{Compatibility as the Basis of Distance}
\label{subsec:distance-definition}

A distance measure is only meaningful when the underlying comparison
procedure is repeatable. Before defining a metric, we must establish the
instrument that performs the counting.

\begin{definition}[Ruler]
\label{def:ruler}
A \textbf{ruler} is a fixed repeatable procedure that assigns a nonnegative
integer to any ordered pair of compatible events. Formally, a ruler is a map
\[
  R : E \times E \to \mathbb{N}
\]
such that $R(e_i, e_j)$ equals the number of admissible refinement steps
required to distinguish $e_j$ from $e_i$.
\end{definition}

The ruler does not rely on geometry or coordinates. It operates purely by
tracking how refinements accumulate. If repeated trials yield inconsistent
counts, no distance is defined.

Given a shared event set $E_{\mathrm{univ}}$, the \emph{informational
distance} between two compatible events $e_A \prec e_B$ is the largest number
of refinements that any admissible observer can record between them:
\[
  d(e_A, e_B)
  =
  \max
  \bigl\{
    R(e_i, e_{i+1}) :
    e_A = e_0 \prec \cdots \prec e_k = e_B
  \bigr\}.
\]

\subsection{Additivity on Overlaps}
\label{subsec:additivity}

Distance is defined by maximizing over refinement chains. If an intermediate
event lies along a maximal chain, then separating the journey into parts
preserves the count.

\begin{proposition}[Additivity of Maximal Refinement]
\label{prop:additivity}
Let $e_A \prec e_B \prec e_C$ be events such that $e_B$ is on a maximal
chain from $e_A$ to $e_C$. Then
\[
  d(e_A, e_C)
  =
  d(e_A, e_B)
  +
  d(e_B, e_C).
\]
\end{proposition}

\begin{proof}
A maximal chain from $e_A$ to $e_C$ passing through $e_B$ consists of a
maximal chain from $e_A$ to $e_B$ followed by a maximal chain from $e_B$ to
$e_C$. If either subchain were not maximal, replacing it with a longer one
would contradict maximality of the full chain. Thus, the lengths add.
\end{proof}

Additivity ensures there are no ``hidden'' refinements. All observable
distinguishability is accounted for in the record.

\subsection{Operational Properties}
\label{subsec:operational-distance}

Informational distance is a nonnegative integer determined entirely by
causal structure. For compatible events $e_A, e_B, e_C$:

\begin{enumerate}
\item
\textbf{Non-negativity.}
$d(e_A, e_B) \ge 0$. Refinements cannot decrement.
\item
\textbf{Identity of indiscernibles.}
$d(e_A, e_B) = 0$ if and only if $e_A$ and $e_B$ represent the same record
of distinctions.
\item
\textbf{Maximality.}
If $e_A \prec e_B \prec e_C$, then
\[
d(e_A, e_C) \ge d(e_A, e_B) + d(e_B, e_C),
\]
with equality if and only if $e_B$ lies on a maximal refinement chain.
\end{enumerate}

In this sense, distance is the measure of how many admissible distinctions
separate one informational state from another. It is not imposed from
outside. It emerges directly from the count structure observed within the
history.

\section{Minimality and Reciprocity}
\label{sec:minimality-reciprocity}

The Axiom of Ockham imposes a strict constraint: a physical trajectory must
not introduce any distinguishable structure that does not appear in the
experimental record. To enforce this, the history must remain stable not
only in its original description, but under any admissible relabeling of
the outcomes.

\subsection{Weak Form as Minimal Compatibility Correction}
\label{subsec:weak-form}

Let $\boldsymbol{\psi}$ be a candidate history connecting two anchor events.
Let $\boldsymbol{\phi}$ be a \emph{test variation}: a hypothetical refinement
that vanishes at the anchors but introduces count changes in the interior.

The Weak Form is the requirement that no such variation reveals any
distinguishable deviation from $\boldsymbol{\psi}$:
\begin{equation}
  \langle \boldsymbol{\psi}, \boldsymbol{\phi} \rangle_L = 0
  \qquad
  \forall \boldsymbol{\phi} \in V_{\mathrm{test}},
  \label{eq:weak-form-min}
\end{equation}
where $\langle \cdot , \cdot \rangle_L$ is a pairing that detects
distinguishable increments. If this quantity were nonzero, the variation
would represent unrecorded structure, contradicting the record and halting
the history.

Thus, the Weak Form does not minimize anything. It eliminates all histories
that would produce additional observable distinctions.

\subsection{Reciprocity and the Transpose Map}
\label{subsec:adjoint}

Two observers may describe the same history using different outcome
alphabets. Let $L$ be the linear operator that translates the record of
Observer~A into the alphabet of Observer~B:
\[
  \boldsymbol{\psi}_B = L \boldsymbol{\psi}_A.
\]

If $\boldsymbol{\phi}$ is a test variation in Observer~B’s frame, the total
count of distinguishable increments must remain invariant when expressed in
either description:
\[
  \langle \boldsymbol{\psi}_B, \boldsymbol{\phi} \rangle
  =
  \langle \boldsymbol{\psi}_A, \tilde{\boldsymbol{\phi}} \rangle.
\]

Using the standard inner-product representation for integer counts,
\[
  (L \boldsymbol{\psi}_A)^T \boldsymbol{\phi}
  =
  \boldsymbol{\psi}_A^T (L^T \boldsymbol{\phi}),
\]
we identify the transformed test
\[
  \tilde{\boldsymbol{\phi}} = L^T \boldsymbol{\phi}.
\]

The transpose $L^T$ is therefore defined operationally as the \emph{reciprocal
change of variables}: it translates tests backward exactly as $L$ translates
histories forward. This ensures that the count of refinements does not
depend on the observer’s chosen description.

Reciprocity is the condition:
\begin{equation}
  \langle L \boldsymbol{\psi}, \boldsymbol{\phi} \rangle
  =
  \langle \boldsymbol{\psi}, L^T \boldsymbol{\phi} \rangle
  \qquad \forall \boldsymbol{\psi}, \boldsymbol{\phi},
  \label{eq:reciprocity}
\end{equation}
guaranteeing that a change of variables cannot create or erase information.
The adjoint arises not from geometry, but from the informational requirement
that two observers must agree on the tally of distinctions they jointly
record.


\section{Galerkin Closure}
\label{sec:galerkin-closure}

We now address the problem of reconstruction: given the finite experimental
record $\boldsymbol{\psi}$, which admissible history extends it without
predicting refinements that do not appear?

In numerical analysis, the Galerkin method selects a solution by enforcing
orthogonality of the residual. In our setting, this principle is elevated to
a fundamental law: the physical history is the one that introduces no
distinguishable structure beyond what the observer records.

\subsection{Trial Functions as Hypothetical Refinements}
\label{subsec:trial-refinements}

Let $\mathcal{H}$ denote the space of conceivable histories connecting two
anchors. A \emph{trial history} $\mathbf{u} \in \mathcal{H}$ interpolates the
recorded events but may contain additional unobserved refinements. These
represent hypothetical structure without operational support.

The space $\mathcal{H}$ is far larger than what measurement can constrain.

\subsection{Test Functions as Observer Constraints}
\label{subsec:test-constraints}

A \emph{test function} $\mathbf{v}$ represents a distinguishability
constraint: a refinement the observer \emph{could} resolve.

The admissible tests are determined by a change-of-variables operator $L$:
the mapping that relates one observer’s outcome alphabet to another’s. The
rows of $L$ span the test space
\[
  V_{\mathrm{test}} = \mathrm{rowspan}(L).
\]
A trial refinement that lies outside this span cannot be detected by
measurement.

\subsection{Minimal Refinement as $L$-Orthogonality}
\label{subsec:minimal-weak-limit}

Let $\mathbf{R}(\mathbf{u})$ denote the informational residue: the portion of
$\mathbf{u}$ that is not required by the anchors. The physical history
$\mathbf{u}_h$ is determined by the condition
\begin{equation}
  \langle \mathbf{R}(\mathbf{u}_h), \mathbf{v} \rangle_L = 0
  \qquad
  \forall\, \mathbf{v} \in V_{\mathrm{test}},
  \label{eq:Lorthogonality}
\end{equation}
where $\langle \cdot, \cdot \rangle_L$ is the observational pairing induced
by $L$.

This condition ensures:

\begin{enumerate}
\item
\textbf{Fidelity.}
$\mathbf{u}_h$ agrees with the recorded counts: it matches the anchors.
\item
\textbf{Minimality.}
Any excess structure in $\mathbf{u}_h$ is undetectable by the observer and
contributes no distinguishability.
\end{enumerate}

In algebraic terms,
\[
  \mathbf{R}(\mathbf{u}_h) \in \ker(L^T).
\]
All admissible residue is confined to the kernel of the dual measurement
operator. Anything else would be detectable and therefore forbidden.

\begin{center}
\emph{Reality is the projection of conceivable histories onto shared,
operationally accessible distinctions.}
\end{center}

As the test space densifies and spans the full dual,
$\ker(L^T) \to \{0\}$, forcing $\mathbf{R}(\mathbf{u}_h) = 0$. In this limit,
the physical history becomes uniquely determined by the experimental record.
Smooth dynamics appears not as a primitive feature, but as the residual-free
closure of discrete refinement data.


% 3.6
\section{Dense Limit and Classical Calculus}
\label{sec:dense-limit}
  \subsection{Differentiable Distance in the Smooth Shadow}
  \label{subsec:smooth-distance}
  \subsection{Euler--Lagrange Form as Informational Minimality}
  \label{subsec:euler-lagrange}

% 3.7
\section{Consequences of Compatible Refinement}
\label{sec:consequences}
  \subsection{Relative Velocity as Change in Distance}
  \label{subsec:relative-velocity}
  \subsection{Time Dilation as Unequal Refinement Density}
  \label{subsec:time-dilation}
  \subsection{Finite Navier--Stokes Closure}
  \label{subsec:finite-NS}

